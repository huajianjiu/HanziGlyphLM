{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renderer\n",
    "from PIL import ImageFont\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "char_size = 24\n",
    "# char render\n",
    "def render(text, font=None):\n",
    "    if font is None:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\", char_size)\n",
    "    mask = font.getmask(text)\n",
    "    size = mask.size[::-1]\n",
    "    a = np.asarray(mask).reshape(size) / 255\n",
    "    res = cv2.resize(a, dsize=(char_size, char_size), interpolation=cv2.INTER_CUBIC)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model/data_utils.py\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, max_size=None):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 1\n",
    "        self.word2idx['⸘'] = 0 # as unk\n",
    "        self.idx2word[0] = '⸘'\n",
    "        self.max_size = max_size + 1\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx and self.idx < self.max_size:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, max_size=None):\n",
    "        self.dictionary = Dictionary(max_size=max_size)\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "#         with open(path, 'r') as f:\n",
    "#             tokens = 0\n",
    "#             for line in f:\n",
    "#                 words = line.split() + ['<eos>']\n",
    "#                 tokens += len(words)\n",
    "#                 for word in words: \n",
    "#                     self.dictionary.add_word(word)  \n",
    "\n",
    "        # split words to char and add to dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                tokens += len(chars)\n",
    "                for char in chars:\n",
    "                    self.dictionary.add_word(char)\n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                for char in chars:\n",
    "                    if char in self.dictionary.word2idx:\n",
    "                        ids[token] = self.dictionary.word2idx[char]\n",
    "                        token += 1\n",
    "                    else:\n",
    "                        ids[token] = self.dictionary.word2idx['⸘']\n",
    "                        token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, char_cnn_o, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = char_cnn_o\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, h = self.gru(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, h\n",
    "\"\"\"\n",
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)\n",
    "\"\"\"\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    if type(stride) is not tuple:\n",
    "        stride = (stride, stride)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride[0]) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride[1]) + 1)\n",
    "    return h, w\n",
    "\n",
    "# Dai et al. 's CNN glyph encoder\n",
    "class Dai_CNN(nn.Module):\n",
    "    def __init__(self, embed_size, input_size=(24, 24)):\n",
    "        super(Dai_CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, (7, 7), stride=(2,2))\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.zeros_(self.conv1.bias)\n",
    "        h, w = conv_output_shape(input_size, (7, 7), (2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 16, (5, 5), stride=(2,2))\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        torch.nn.init.zeros_(self.conv2.bias)\n",
    "        h, w = conv_output_shape((h, w), (5, 5), (2, 2))\n",
    "                \n",
    "        self.fc = nn.Linear(16*h*w, embed_size)\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        torch.nn.init.zeros_(self.fc.bias)\n",
    "        \n",
    "        self.h, self.w = h, w\n",
    "        \n",
    "    def forward(self, char_img):\n",
    "        b = char_img.size(0)\n",
    "        x = self.conv1(char_img)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = x.view(-1, 16*self.h*self.w)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "embed_size = 300\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "seq_length = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Load dataset\n",
    "corpus = Corpus(max_size=4000)\n",
    "ids = corpus.get_data('icwb2-data/training/msr_training.utf8', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "cnn_encoder = Dai_CNN(embed_size, input_size=(char_size, char_size)).to(device)\n",
    "model.train()\n",
    "cnn_encoder.train()\n",
    "params = list(model.parameters())+list(cnn_encoder.parameters())\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# Truncated backpropagation\n",
    "def detach(state):\n",
    "    return state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-25 17:09:00.825180 Epoch [1/50], Step[0/8080], Loss: 8.2959, Perplexity: 4007.45\n",
      "2019-11-25 17:15:20.302716 Epoch [1/50], Step[1000/8080], Loss: 5.9313, Perplexity: 376.66\n",
      "2019-11-25 17:21:37.509303 Epoch [1/50], Step[2000/8080], Loss: 5.3139, Perplexity: 203.14\n",
      "2019-11-25 17:27:59.752441 Epoch [1/50], Step[3000/8080], Loss: 4.9082, Perplexity: 135.40\n",
      "2019-11-25 17:34:25.121433 Epoch [1/50], Step[4000/8080], Loss: 4.8073, Perplexity: 122.40\n",
      "2019-11-25 17:41:48.158266 Epoch [1/50], Step[5000/8080], Loss: 5.1813, Perplexity: 177.91\n",
      "2019-11-25 17:48:54.614945 Epoch [1/50], Step[6000/8080], Loss: 5.0580, Perplexity: 157.28\n",
      "2019-11-25 17:55:51.318983 Epoch [1/50], Step[7000/8080], Loss: 4.6831, Perplexity: 108.10\n",
      "2019-11-25 18:02:56.155131 Epoch [1/50], Step[8000/8080], Loss: 4.6707, Perplexity: 106.77\n",
      "2019-11-25 18:03:31.046830 Epoch [2/50], Step[0/8080], Loss: 4.6619, Perplexity: 105.84\n",
      "2019-11-25 18:10:34.486079 Epoch [2/50], Step[1000/8080], Loss: 4.6841, Perplexity: 108.21\n",
      "2019-11-25 18:17:35.195182 Epoch [2/50], Step[2000/8080], Loss: 4.5575, Perplexity: 95.34\n",
      "2019-11-25 18:24:43.369610 Epoch [2/50], Step[3000/8080], Loss: 4.4856, Perplexity: 88.73\n",
      "2019-11-25 18:31:51.204191 Epoch [2/50], Step[4000/8080], Loss: 4.4188, Perplexity: 83.00\n",
      "2019-11-25 18:38:08.952770 Epoch [2/50], Step[5000/8080], Loss: 4.8518, Perplexity: 127.97\n",
      "2019-11-25 18:44:24.953798 Epoch [2/50], Step[6000/8080], Loss: 4.8144, Perplexity: 123.28\n",
      "2019-11-25 18:50:30.498703 Epoch [2/50], Step[7000/8080], Loss: 4.4281, Perplexity: 83.77\n",
      "2019-11-25 18:56:42.774081 Epoch [2/50], Step[8000/8080], Loss: 4.5095, Perplexity: 90.88\n",
      "2019-11-25 18:57:13.213479 Epoch [3/50], Step[0/8080], Loss: 4.4838, Perplexity: 88.57\n",
      "2019-11-25 19:03:29.294644 Epoch [3/50], Step[1000/8080], Loss: 4.4487, Perplexity: 85.52\n",
      "2019-11-25 19:10:22.096965 Epoch [3/50], Step[2000/8080], Loss: 4.4269, Perplexity: 83.67\n",
      "2019-11-25 19:17:14.845153 Epoch [3/50], Step[3000/8080], Loss: 4.3825, Perplexity: 80.04\n",
      "2019-11-25 19:23:54.098573 Epoch [3/50], Step[4000/8080], Loss: 4.2931, Perplexity: 73.19\n",
      "2019-11-25 19:30:14.613906 Epoch [3/50], Step[5000/8080], Loss: 4.7693, Perplexity: 117.83\n",
      "2019-11-25 19:36:26.443795 Epoch [3/50], Step[6000/8080], Loss: 4.6448, Perplexity: 104.04\n",
      "2019-11-25 19:42:44.311377 Epoch [3/50], Step[7000/8080], Loss: 4.2962, Perplexity: 73.42\n",
      "2019-11-25 19:49:01.191730 Epoch [3/50], Step[8000/8080], Loss: 4.4354, Perplexity: 84.39\n",
      "2019-11-25 19:49:31.417069 Epoch [4/50], Step[0/8080], Loss: 4.4422, Perplexity: 84.96\n",
      "2019-11-25 19:55:39.780887 Epoch [4/50], Step[1000/8080], Loss: 4.2887, Perplexity: 72.87\n",
      "2019-11-25 20:02:05.274147 Epoch [4/50], Step[2000/8080], Loss: 4.3167, Perplexity: 74.94\n",
      "2019-11-25 20:08:24.501001 Epoch [4/50], Step[3000/8080], Loss: 4.3212, Perplexity: 75.28\n",
      "2019-11-25 20:14:46.614422 Epoch [4/50], Step[4000/8080], Loss: 4.2445, Perplexity: 69.72\n",
      "2019-11-25 20:21:05.209852 Epoch [4/50], Step[5000/8080], Loss: 4.6964, Perplexity: 109.55\n",
      "2019-11-25 20:27:18.573649 Epoch [4/50], Step[6000/8080], Loss: 4.5614, Perplexity: 95.72\n",
      "2019-11-25 20:33:33.065412 Epoch [4/50], Step[7000/8080], Loss: 4.2237, Perplexity: 68.29\n",
      "2019-11-25 20:39:49.630574 Epoch [4/50], Step[8000/8080], Loss: 4.3416, Perplexity: 76.83\n",
      "2019-11-25 20:40:19.298277 Epoch [5/50], Step[0/8080], Loss: 4.4121, Perplexity: 82.44\n",
      "2019-11-25 20:46:37.223206 Epoch [5/50], Step[1000/8080], Loss: 4.2218, Perplexity: 68.16\n",
      "2019-11-25 20:52:54.649375 Epoch [5/50], Step[2000/8080], Loss: 4.3004, Perplexity: 73.73\n",
      "2019-11-25 20:59:12.440730 Epoch [5/50], Step[3000/8080], Loss: 4.2855, Perplexity: 72.64\n",
      "2019-11-25 21:05:21.924682 Epoch [5/50], Step[4000/8080], Loss: 4.2115, Perplexity: 67.46\n",
      "2019-11-25 21:11:34.706214 Epoch [5/50], Step[5000/8080], Loss: 4.6344, Perplexity: 102.97\n",
      "2019-11-25 21:17:52.871557 Epoch [5/50], Step[6000/8080], Loss: 4.5048, Perplexity: 90.45\n",
      "2019-11-25 21:24:12.225467 Epoch [5/50], Step[7000/8080], Loss: 4.1161, Perplexity: 61.32\n",
      "2019-11-25 21:30:37.385382 Epoch [5/50], Step[8000/8080], Loss: 4.3232, Perplexity: 75.43\n",
      "2019-11-25 21:31:07.695023 Epoch [6/50], Step[0/8080], Loss: 4.3744, Perplexity: 79.39\n",
      "2019-11-25 21:37:38.869243 Epoch [6/50], Step[1000/8080], Loss: 4.1991, Perplexity: 66.63\n",
      "2019-11-25 21:44:24.035987 Epoch [6/50], Step[2000/8080], Loss: 4.3026, Perplexity: 73.89\n",
      "2019-11-25 21:51:00.577493 Epoch [6/50], Step[3000/8080], Loss: 4.2342, Perplexity: 69.01\n",
      "2019-11-25 21:57:27.425467 Epoch [6/50], Step[4000/8080], Loss: 4.1884, Perplexity: 65.91\n",
      "2019-11-25 22:03:44.369554 Epoch [6/50], Step[5000/8080], Loss: 4.6228, Perplexity: 101.78\n",
      "2019-11-25 22:10:00.803258 Epoch [6/50], Step[6000/8080], Loss: 4.4711, Perplexity: 87.45\n",
      "2019-11-25 22:16:19.070578 Epoch [6/50], Step[7000/8080], Loss: 4.1008, Perplexity: 60.39\n",
      "2019-11-25 22:22:37.252521 Epoch [6/50], Step[8000/8080], Loss: 4.2939, Perplexity: 73.25\n",
      "2019-11-25 22:23:07.698395 Epoch [7/50], Step[0/8080], Loss: 4.3741, Perplexity: 79.37\n",
      "2019-11-25 22:29:23.331060 Epoch [7/50], Step[1000/8080], Loss: 4.1652, Perplexity: 64.40\n",
      "2019-11-25 22:35:40.410649 Epoch [7/50], Step[2000/8080], Loss: 4.2718, Perplexity: 71.65\n",
      "2019-11-25 22:41:57.822223 Epoch [7/50], Step[3000/8080], Loss: 4.2133, Perplexity: 67.58\n",
      "2019-11-25 22:48:35.647419 Epoch [7/50], Step[4000/8080], Loss: 4.1621, Perplexity: 64.21\n",
      "2019-11-25 22:55:36.527445 Epoch [7/50], Step[5000/8080], Loss: 4.5986, Perplexity: 99.34\n",
      "2019-11-25 23:02:39.255026 Epoch [7/50], Step[6000/8080], Loss: 4.4522, Perplexity: 85.82\n",
      "2019-11-25 23:09:30.835087 Epoch [7/50], Step[7000/8080], Loss: 4.0811, Perplexity: 59.21\n",
      "2019-11-25 23:16:24.510184 Epoch [7/50], Step[8000/8080], Loss: 4.3014, Perplexity: 73.80\n",
      "2019-11-25 23:16:58.163883 Epoch [8/50], Step[0/8080], Loss: 4.3428, Perplexity: 76.92\n",
      "2019-11-25 23:23:57.305884 Epoch [8/50], Step[1000/8080], Loss: 4.1488, Perplexity: 63.36\n",
      "2019-11-25 23:30:49.546563 Epoch [8/50], Step[2000/8080], Loss: 4.2916, Perplexity: 73.08\n",
      "2019-11-25 23:37:46.885062 Epoch [8/50], Step[3000/8080], Loss: 4.1922, Perplexity: 66.17\n",
      "2019-11-25 23:44:40.226084 Epoch [8/50], Step[4000/8080], Loss: 4.1282, Perplexity: 62.07\n",
      "2019-11-25 23:51:28.064673 Epoch [8/50], Step[5000/8080], Loss: 4.5704, Perplexity: 96.59\n",
      "2019-11-25 23:58:11.533137 Epoch [8/50], Step[6000/8080], Loss: 4.4412, Perplexity: 84.88\n",
      "2019-11-26 00:04:57.415381 Epoch [8/50], Step[7000/8080], Loss: 4.0395, Perplexity: 56.80\n",
      "2019-11-26 00:11:38.177408 Epoch [8/50], Step[8000/8080], Loss: 4.2503, Perplexity: 70.13\n",
      "2019-11-26 00:12:10.376511 Epoch [9/50], Step[0/8080], Loss: 4.3403, Perplexity: 76.73\n",
      "2019-11-26 00:18:52.496178 Epoch [9/50], Step[1000/8080], Loss: 4.1357, Perplexity: 62.54\n",
      "2019-11-26 00:25:36.223654 Epoch [9/50], Step[2000/8080], Loss: 4.2431, Perplexity: 69.62\n",
      "2019-11-26 00:32:23.777477 Epoch [9/50], Step[3000/8080], Loss: 4.1577, Perplexity: 63.93\n",
      "2019-11-26 00:39:09.181027 Epoch [9/50], Step[4000/8080], Loss: 4.0935, Perplexity: 59.95\n",
      "2019-11-26 00:45:48.189606 Epoch [9/50], Step[5000/8080], Loss: 4.5766, Perplexity: 97.18\n",
      "2019-11-26 00:52:32.765172 Epoch [9/50], Step[6000/8080], Loss: 4.4084, Perplexity: 82.14\n",
      "2019-11-26 00:59:18.204284 Epoch [9/50], Step[7000/8080], Loss: 4.0112, Perplexity: 55.21\n",
      "2019-11-26 01:05:57.854540 Epoch [9/50], Step[8000/8080], Loss: 4.2382, Perplexity: 69.28\n",
      "2019-11-26 01:06:30.753453 Epoch [10/50], Step[0/8080], Loss: 4.3213, Perplexity: 75.28\n",
      "2019-11-26 01:13:12.329990 Epoch [10/50], Step[1000/8080], Loss: 4.1055, Perplexity: 60.67\n",
      "2019-11-26 01:19:56.098227 Epoch [10/50], Step[2000/8080], Loss: 4.2158, Perplexity: 67.75\n",
      "2019-11-26 01:26:38.767651 Epoch [10/50], Step[3000/8080], Loss: 4.1412, Perplexity: 62.88\n",
      "2019-11-26 01:33:22.366279 Epoch [10/50], Step[4000/8080], Loss: 4.0906, Perplexity: 59.77\n",
      "2019-11-26 01:40:07.749342 Epoch [10/50], Step[5000/8080], Loss: 4.5690, Perplexity: 96.45\n",
      "2019-11-26 01:46:55.438325 Epoch [10/50], Step[6000/8080], Loss: 4.4101, Perplexity: 82.28\n",
      "2019-11-26 01:53:39.292643 Epoch [10/50], Step[7000/8080], Loss: 3.9889, Perplexity: 54.00\n",
      "2019-11-26 02:00:22.250714 Epoch [10/50], Step[8000/8080], Loss: 4.2092, Perplexity: 67.30\n",
      "2019-11-26 02:00:55.014112 Epoch [11/50], Step[0/8080], Loss: 4.2998, Perplexity: 73.68\n",
      "2019-11-26 02:07:37.677416 Epoch [11/50], Step[1000/8080], Loss: 4.0888, Perplexity: 59.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 02:14:21.784382 Epoch [11/50], Step[2000/8080], Loss: 4.2067, Perplexity: 67.13\n",
      "2019-11-26 02:21:03.811082 Epoch [11/50], Step[3000/8080], Loss: 4.1389, Perplexity: 62.73\n",
      "2019-11-26 02:27:49.800121 Epoch [11/50], Step[4000/8080], Loss: 4.0466, Perplexity: 57.20\n",
      "2019-11-26 02:34:32.614196 Epoch [11/50], Step[5000/8080], Loss: 4.5631, Perplexity: 95.88\n",
      "2019-11-26 02:41:12.030792 Epoch [11/50], Step[6000/8080], Loss: 4.3595, Perplexity: 78.22\n",
      "2019-11-26 02:47:57.500543 Epoch [11/50], Step[7000/8080], Loss: 3.9911, Perplexity: 54.11\n",
      "2019-11-26 02:54:41.872821 Epoch [11/50], Step[8000/8080], Loss: 4.2026, Perplexity: 66.86\n",
      "2019-11-26 02:55:14.460793 Epoch [12/50], Step[0/8080], Loss: 4.2834, Perplexity: 72.48\n",
      "2019-11-26 03:02:00.700610 Epoch [12/50], Step[1000/8080], Loss: 4.1152, Perplexity: 61.26\n",
      "2019-11-26 03:08:44.149891 Epoch [12/50], Step[2000/8080], Loss: 4.1998, Perplexity: 66.67\n",
      "2019-11-26 03:15:27.755647 Epoch [12/50], Step[3000/8080], Loss: 4.1336, Perplexity: 62.40\n",
      "2019-11-26 03:22:09.728434 Epoch [12/50], Step[4000/8080], Loss: 4.0498, Perplexity: 57.39\n",
      "2019-11-26 03:28:50.757315 Epoch [12/50], Step[5000/8080], Loss: 4.5924, Perplexity: 98.73\n",
      "2019-11-26 03:35:37.336344 Epoch [12/50], Step[6000/8080], Loss: 4.3929, Perplexity: 80.87\n",
      "2019-11-26 03:42:25.728575 Epoch [12/50], Step[7000/8080], Loss: 3.9818, Perplexity: 53.61\n",
      "2019-11-26 03:49:08.497259 Epoch [12/50], Step[8000/8080], Loss: 4.2358, Perplexity: 69.11\n",
      "2019-11-26 03:49:40.793661 Epoch [13/50], Step[0/8080], Loss: 4.2535, Perplexity: 70.35\n",
      "2019-11-26 03:56:19.659669 Epoch [13/50], Step[1000/8080], Loss: 4.0872, Perplexity: 59.57\n",
      "2019-11-26 04:02:59.880505 Epoch [13/50], Step[2000/8080], Loss: 4.2127, Perplexity: 67.54\n",
      "2019-11-26 04:09:45.573531 Epoch [13/50], Step[3000/8080], Loss: 4.1147, Perplexity: 61.23\n",
      "2019-11-26 04:16:27.154237 Epoch [13/50], Step[4000/8080], Loss: 4.0621, Perplexity: 58.10\n",
      "2019-11-26 04:23:08.901078 Epoch [13/50], Step[5000/8080], Loss: 4.5623, Perplexity: 95.80\n",
      "2019-11-26 04:29:50.605593 Epoch [13/50], Step[6000/8080], Loss: 4.3682, Perplexity: 78.90\n",
      "2019-11-26 04:36:32.052355 Epoch [13/50], Step[7000/8080], Loss: 3.9850, Perplexity: 53.79\n",
      "2019-11-26 04:43:14.114530 Epoch [13/50], Step[8000/8080], Loss: 4.2365, Perplexity: 69.17\n",
      "2019-11-26 04:43:46.694549 Epoch [14/50], Step[0/8080], Loss: 4.2627, Perplexity: 71.00\n",
      "2019-11-26 04:50:30.415475 Epoch [14/50], Step[1000/8080], Loss: 4.0833, Perplexity: 59.34\n",
      "2019-11-26 04:57:13.324158 Epoch [14/50], Step[2000/8080], Loss: 4.2277, Perplexity: 68.56\n",
      "2019-11-26 05:03:57.193925 Epoch [14/50], Step[3000/8080], Loss: 4.0828, Perplexity: 59.31\n",
      "2019-11-26 05:10:37.960938 Epoch [14/50], Step[4000/8080], Loss: 4.0533, Perplexity: 57.59\n",
      "2019-11-26 05:17:22.375436 Epoch [14/50], Step[5000/8080], Loss: 4.5331, Perplexity: 93.04\n",
      "2019-11-26 05:24:04.029106 Epoch [14/50], Step[6000/8080], Loss: 4.3616, Perplexity: 78.38\n",
      "2019-11-26 05:30:44.194579 Epoch [14/50], Step[7000/8080], Loss: 3.9744, Perplexity: 53.22\n",
      "2019-11-26 05:37:28.722517 Epoch [14/50], Step[8000/8080], Loss: 4.2236, Perplexity: 68.28\n",
      "2019-11-26 05:38:00.604893 Epoch [15/50], Step[0/8080], Loss: 4.2681, Perplexity: 71.39\n",
      "2019-11-26 05:44:43.082005 Epoch [15/50], Step[1000/8080], Loss: 4.1037, Perplexity: 60.56\n",
      "2019-11-26 05:51:26.128445 Epoch [15/50], Step[2000/8080], Loss: 4.2059, Perplexity: 67.08\n",
      "2019-11-26 05:58:10.294054 Epoch [15/50], Step[3000/8080], Loss: 4.0631, Perplexity: 58.15\n",
      "2019-11-26 06:04:50.951182 Epoch [15/50], Step[4000/8080], Loss: 4.0624, Perplexity: 58.11\n",
      "2019-11-26 06:11:35.850161 Epoch [15/50], Step[5000/8080], Loss: 4.5536, Perplexity: 94.97\n",
      "2019-11-26 06:18:21.218170 Epoch [15/50], Step[6000/8080], Loss: 4.3310, Perplexity: 76.02\n",
      "2019-11-26 06:25:03.855693 Epoch [15/50], Step[7000/8080], Loss: 3.9686, Perplexity: 52.91\n",
      "2019-11-26 06:31:42.435982 Epoch [15/50], Step[8000/8080], Loss: 4.1813, Perplexity: 65.45\n",
      "2019-11-26 06:32:14.707691 Epoch [16/50], Step[0/8080], Loss: 4.2335, Perplexity: 68.96\n",
      "2019-11-26 06:38:58.942399 Epoch [16/50], Step[1000/8080], Loss: 4.0770, Perplexity: 58.97\n",
      "2019-11-26 06:45:39.869437 Epoch [16/50], Step[2000/8080], Loss: 4.1894, Perplexity: 65.98\n",
      "2019-11-26 06:52:22.830890 Epoch [16/50], Step[3000/8080], Loss: 4.0809, Perplexity: 59.20\n",
      "2019-11-26 06:59:01.867361 Epoch [16/50], Step[4000/8080], Loss: 4.0418, Perplexity: 56.93\n",
      "2019-11-26 07:05:43.254767 Epoch [16/50], Step[5000/8080], Loss: 4.5308, Perplexity: 92.83\n",
      "2019-11-26 07:12:20.805624 Epoch [16/50], Step[6000/8080], Loss: 4.3265, Perplexity: 75.68\n",
      "2019-11-26 07:19:01.811777 Epoch [16/50], Step[7000/8080], Loss: 3.9495, Perplexity: 51.91\n",
      "2019-11-26 07:25:43.248290 Epoch [16/50], Step[8000/8080], Loss: 4.2088, Perplexity: 67.28\n",
      "2019-11-26 07:26:15.817202 Epoch [17/50], Step[0/8080], Loss: 4.2388, Perplexity: 69.32\n",
      "2019-11-26 07:32:56.700934 Epoch [17/50], Step[1000/8080], Loss: 4.0675, Perplexity: 58.41\n",
      "2019-11-26 07:39:36.597502 Epoch [17/50], Step[2000/8080], Loss: 4.1959, Perplexity: 66.41\n",
      "2019-11-26 07:46:25.348567 Epoch [17/50], Step[3000/8080], Loss: 4.0699, Perplexity: 58.55\n",
      "2019-11-26 07:53:11.194785 Epoch [17/50], Step[4000/8080], Loss: 4.0579, Perplexity: 57.85\n",
      "2019-11-26 07:59:57.061069 Epoch [17/50], Step[5000/8080], Loss: 4.5306, Perplexity: 92.81\n",
      "2019-11-26 08:06:34.181847 Epoch [17/50], Step[6000/8080], Loss: 4.3257, Perplexity: 75.62\n",
      "2019-11-26 08:13:20.416144 Epoch [17/50], Step[7000/8080], Loss: 3.9571, Perplexity: 52.31\n",
      "2019-11-26 08:20:02.468990 Epoch [17/50], Step[8000/8080], Loss: 4.2088, Perplexity: 67.28\n",
      "2019-11-26 08:20:35.091288 Epoch [18/50], Step[0/8080], Loss: 4.2977, Perplexity: 73.53\n",
      "2019-11-26 08:27:17.039367 Epoch [18/50], Step[1000/8080], Loss: 4.0874, Perplexity: 59.58\n",
      "2019-11-26 08:34:00.100553 Epoch [18/50], Step[2000/8080], Loss: 4.2010, Perplexity: 66.75\n",
      "2019-11-26 08:40:42.956168 Epoch [18/50], Step[3000/8080], Loss: 4.0387, Perplexity: 56.75\n",
      "2019-11-26 08:47:25.222121 Epoch [18/50], Step[4000/8080], Loss: 4.0645, Perplexity: 58.23\n",
      "2019-11-26 08:54:06.701212 Epoch [18/50], Step[5000/8080], Loss: 4.5349, Perplexity: 93.22\n",
      "2019-11-26 09:00:47.498807 Epoch [18/50], Step[6000/8080], Loss: 4.2826, Perplexity: 72.43\n",
      "2019-11-26 09:07:29.097786 Epoch [18/50], Step[7000/8080], Loss: 3.9430, Perplexity: 51.57\n",
      "2019-11-26 09:14:13.441701 Epoch [18/50], Step[8000/8080], Loss: 4.1965, Perplexity: 66.45\n",
      "2019-11-26 09:14:46.050419 Epoch [19/50], Step[0/8080], Loss: 4.2447, Perplexity: 69.73\n",
      "2019-11-26 09:21:30.385457 Epoch [19/50], Step[1000/8080], Loss: 4.0431, Perplexity: 57.00\n",
      "2019-11-26 09:28:12.291599 Epoch [19/50], Step[2000/8080], Loss: 4.1883, Perplexity: 65.91\n",
      "2019-11-26 09:34:55.773017 Epoch [19/50], Step[3000/8080], Loss: 4.0706, Perplexity: 58.59\n",
      "2019-11-26 09:41:42.220342 Epoch [19/50], Step[4000/8080], Loss: 4.0777, Perplexity: 59.01\n",
      "2019-11-26 09:48:28.712850 Epoch [19/50], Step[5000/8080], Loss: 4.5420, Perplexity: 93.88\n",
      "2019-11-26 09:55:12.735201 Epoch [19/50], Step[6000/8080], Loss: 4.2865, Perplexity: 72.71\n",
      "2019-11-26 10:01:59.104433 Epoch [19/50], Step[7000/8080], Loss: 3.9627, Perplexity: 52.60\n",
      "2019-11-26 10:08:42.285784 Epoch [19/50], Step[8000/8080], Loss: 4.1820, Perplexity: 65.50\n",
      "2019-11-26 10:09:14.866673 Epoch [20/50], Step[0/8080], Loss: 4.2101, Perplexity: 67.36\n",
      "2019-11-26 10:15:58.461142 Epoch [20/50], Step[1000/8080], Loss: 4.0206, Perplexity: 55.74\n",
      "2019-11-26 10:22:41.554294 Epoch [20/50], Step[2000/8080], Loss: 4.1800, Perplexity: 65.37\n",
      "2019-11-26 10:29:25.928786 Epoch [20/50], Step[3000/8080], Loss: 4.0061, Perplexity: 54.93\n",
      "2019-11-26 10:36:09.570784 Epoch [20/50], Step[4000/8080], Loss: 4.0804, Perplexity: 59.17\n",
      "2019-11-26 10:42:50.172976 Epoch [20/50], Step[5000/8080], Loss: 4.5329, Perplexity: 93.03\n",
      "2019-11-26 10:49:33.927835 Epoch [20/50], Step[6000/8080], Loss: 4.3106, Perplexity: 74.49\n",
      "2019-11-26 10:56:18.046599 Epoch [20/50], Step[7000/8080], Loss: 3.9510, Perplexity: 51.99\n",
      "2019-11-26 11:02:56.528490 Epoch [20/50], Step[8000/8080], Loss: 4.2185, Perplexity: 67.93\n",
      "2019-11-26 11:03:29.664579 Epoch [21/50], Step[0/8080], Loss: 4.2671, Perplexity: 71.32\n",
      "2019-11-26 11:10:13.323311 Epoch [21/50], Step[1000/8080], Loss: 4.0550, Perplexity: 57.69\n",
      "2019-11-26 11:16:56.637455 Epoch [21/50], Step[2000/8080], Loss: 4.1610, Perplexity: 64.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 11:23:35.953235 Epoch [21/50], Step[3000/8080], Loss: 4.0283, Perplexity: 56.17\n",
      "2019-11-26 11:30:20.166043 Epoch [21/50], Step[4000/8080], Loss: 4.0650, Perplexity: 58.26\n",
      "2019-11-26 11:37:04.211133 Epoch [21/50], Step[5000/8080], Loss: 4.5362, Perplexity: 93.33\n",
      "2019-11-26 11:43:46.354274 Epoch [21/50], Step[6000/8080], Loss: 4.3013, Perplexity: 73.80\n",
      "2019-11-26 11:50:29.287913 Epoch [21/50], Step[7000/8080], Loss: 3.9532, Perplexity: 52.10\n",
      "2019-11-26 11:57:17.300156 Epoch [21/50], Step[8000/8080], Loss: 4.1866, Perplexity: 65.80\n",
      "2019-11-26 11:57:49.604476 Epoch [22/50], Step[0/8080], Loss: 4.2463, Perplexity: 69.84\n",
      "2019-11-26 12:04:33.883542 Epoch [22/50], Step[1000/8080], Loss: 4.0545, Perplexity: 57.66\n",
      "2019-11-26 12:11:15.182698 Epoch [22/50], Step[2000/8080], Loss: 4.1821, Perplexity: 65.50\n",
      "2019-11-26 12:17:55.466937 Epoch [22/50], Step[3000/8080], Loss: 4.0097, Perplexity: 55.13\n",
      "2019-11-26 12:24:41.053405 Epoch [22/50], Step[4000/8080], Loss: 4.0620, Perplexity: 58.09\n",
      "2019-11-26 12:31:24.387246 Epoch [22/50], Step[5000/8080], Loss: 4.5448, Perplexity: 94.15\n",
      "2019-11-26 12:38:05.634716 Epoch [22/50], Step[6000/8080], Loss: 4.2907, Perplexity: 73.02\n",
      "2019-11-26 12:44:49.253607 Epoch [22/50], Step[7000/8080], Loss: 3.9544, Perplexity: 52.16\n",
      "2019-11-26 12:51:35.015212 Epoch [22/50], Step[8000/8080], Loss: 4.1735, Perplexity: 64.94\n",
      "2019-11-26 12:52:07.369220 Epoch [23/50], Step[0/8080], Loss: 4.2262, Perplexity: 68.46\n",
      "2019-11-26 12:58:51.976970 Epoch [23/50], Step[1000/8080], Loss: 4.0419, Perplexity: 56.94\n",
      "2019-11-26 13:05:27.167088 Epoch [23/50], Step[2000/8080], Loss: 4.1973, Perplexity: 66.50\n",
      "2019-11-26 13:12:11.355460 Epoch [23/50], Step[3000/8080], Loss: 3.9979, Perplexity: 54.48\n",
      "2019-11-26 13:18:56.471678 Epoch [23/50], Step[4000/8080], Loss: 4.0743, Perplexity: 58.81\n",
      "2019-11-26 13:25:40.846199 Epoch [23/50], Step[5000/8080], Loss: 4.5808, Perplexity: 97.59\n",
      "2019-11-26 13:32:24.791425 Epoch [23/50], Step[6000/8080], Loss: 4.3023, Perplexity: 73.87\n",
      "2019-11-26 13:39:11.178454 Epoch [23/50], Step[7000/8080], Loss: 3.9442, Perplexity: 51.64\n",
      "2019-11-26 13:45:55.613730 Epoch [23/50], Step[8000/8080], Loss: 4.1763, Perplexity: 65.12\n",
      "2019-11-26 13:46:27.580696 Epoch [24/50], Step[0/8080], Loss: 4.2138, Perplexity: 67.61\n",
      "2019-11-26 13:53:10.403539 Epoch [24/50], Step[1000/8080], Loss: 4.0081, Perplexity: 55.04\n",
      "2019-11-26 13:59:55.697896 Epoch [24/50], Step[2000/8080], Loss: 4.1998, Perplexity: 66.68\n",
      "2019-11-26 14:06:38.387114 Epoch [24/50], Step[3000/8080], Loss: 4.0093, Perplexity: 55.11\n",
      "2019-11-26 14:13:23.034717 Epoch [24/50], Step[4000/8080], Loss: 4.0608, Perplexity: 58.02\n",
      "2019-11-26 14:20:04.794791 Epoch [24/50], Step[5000/8080], Loss: 4.5165, Perplexity: 91.51\n",
      "2019-11-26 14:26:48.731257 Epoch [24/50], Step[6000/8080], Loss: 4.2856, Perplexity: 72.65\n",
      "2019-11-26 14:33:29.531879 Epoch [24/50], Step[7000/8080], Loss: 3.9179, Perplexity: 50.29\n",
      "2019-11-26 14:40:14.846930 Epoch [24/50], Step[8000/8080], Loss: 4.1689, Perplexity: 64.64\n",
      "2019-11-26 14:40:46.689543 Epoch [25/50], Step[0/8080], Loss: 4.2079, Perplexity: 67.21\n",
      "2019-11-26 14:47:30.046206 Epoch [25/50], Step[1000/8080], Loss: 4.0482, Perplexity: 57.29\n",
      "2019-11-26 14:54:10.773210 Epoch [25/50], Step[2000/8080], Loss: 4.1839, Perplexity: 65.62\n",
      "2019-11-26 15:00:51.042835 Epoch [25/50], Step[3000/8080], Loss: 4.0136, Perplexity: 55.35\n",
      "2019-11-26 15:07:34.477780 Epoch [25/50], Step[4000/8080], Loss: 4.0681, Perplexity: 58.44\n",
      "2019-11-26 15:14:14.323368 Epoch [25/50], Step[5000/8080], Loss: 4.5178, Perplexity: 91.63\n",
      "2019-11-26 15:20:52.106236 Epoch [25/50], Step[6000/8080], Loss: 4.3011, Perplexity: 73.78\n",
      "2019-11-26 15:27:05.456719 Epoch [25/50], Step[7000/8080], Loss: 3.9475, Perplexity: 51.81\n",
      "2019-11-26 15:33:13.775880 Epoch [25/50], Step[8000/8080], Loss: 4.1329, Perplexity: 62.36\n",
      "2019-11-26 15:33:42.583145 Epoch [26/50], Step[0/8080], Loss: 4.2466, Perplexity: 69.87\n",
      "2019-11-26 15:39:54.131572 Epoch [26/50], Step[1000/8080], Loss: 3.9869, Perplexity: 53.89\n",
      "2019-11-26 15:46:06.835189 Epoch [26/50], Step[2000/8080], Loss: 4.1693, Perplexity: 64.67\n",
      "2019-11-26 15:52:17.490476 Epoch [26/50], Step[3000/8080], Loss: 4.0135, Perplexity: 55.34\n",
      "2019-11-26 15:58:28.817661 Epoch [26/50], Step[4000/8080], Loss: 4.0618, Perplexity: 58.08\n",
      "2019-11-26 16:04:41.093613 Epoch [26/50], Step[5000/8080], Loss: 4.5131, Perplexity: 91.20\n",
      "2019-11-26 16:10:54.752363 Epoch [26/50], Step[6000/8080], Loss: 4.2864, Perplexity: 72.71\n",
      "2019-11-26 16:17:09.009067 Epoch [26/50], Step[7000/8080], Loss: 3.9551, Perplexity: 52.20\n",
      "2019-11-26 16:23:21.696842 Epoch [26/50], Step[8000/8080], Loss: 4.1387, Perplexity: 62.72\n",
      "2019-11-26 16:23:50.934036 Epoch [27/50], Step[0/8080], Loss: 4.2055, Perplexity: 67.05\n",
      "2019-11-26 16:30:06.330666 Epoch [27/50], Step[1000/8080], Loss: 3.9700, Perplexity: 52.98\n",
      "2019-11-26 16:36:51.369747 Epoch [27/50], Step[2000/8080], Loss: 4.1533, Perplexity: 63.65\n",
      "2019-11-26 16:43:42.194222 Epoch [27/50], Step[3000/8080], Loss: 4.0000, Perplexity: 54.60\n",
      "2019-11-26 16:50:35.179343 Epoch [27/50], Step[4000/8080], Loss: 4.0503, Perplexity: 57.42\n",
      "2019-11-26 16:57:26.301560 Epoch [27/50], Step[5000/8080], Loss: 4.5356, Perplexity: 93.28\n",
      "2019-11-26 17:04:26.752936 Epoch [27/50], Step[6000/8080], Loss: 4.2707, Perplexity: 71.57\n",
      "2019-11-26 17:11:49.988599 Epoch [27/50], Step[7000/8080], Loss: 3.9355, Perplexity: 51.19\n",
      "2019-11-26 17:19:06.243280 Epoch [27/50], Step[8000/8080], Loss: 4.1249, Perplexity: 61.86\n",
      "2019-11-26 17:19:41.542161 Epoch [28/50], Step[0/8080], Loss: 4.2009, Perplexity: 66.75\n",
      "2019-11-26 17:27:01.351693 Epoch [28/50], Step[1000/8080], Loss: 3.9985, Perplexity: 54.51\n",
      "2019-11-26 17:34:18.225922 Epoch [28/50], Step[2000/8080], Loss: 4.1657, Perplexity: 64.44\n",
      "2019-11-26 17:41:32.365002 Epoch [28/50], Step[3000/8080], Loss: 4.0099, Perplexity: 55.14\n",
      "2019-11-26 17:48:22.227327 Epoch [28/50], Step[4000/8080], Loss: 4.0444, Perplexity: 57.08\n",
      "2019-11-26 17:55:23.713382 Epoch [28/50], Step[5000/8080], Loss: 4.5250, Perplexity: 92.29\n",
      "2019-11-26 18:02:39.815344 Epoch [28/50], Step[6000/8080], Loss: 4.2798, Perplexity: 72.23\n",
      "2019-11-26 18:09:38.858290 Epoch [28/50], Step[7000/8080], Loss: 3.9164, Perplexity: 50.22\n",
      "2019-11-26 18:16:39.732018 Epoch [28/50], Step[8000/8080], Loss: 4.1791, Perplexity: 65.30\n",
      "2019-11-26 18:17:14.640649 Epoch [29/50], Step[0/8080], Loss: 4.2185, Perplexity: 67.93\n",
      "2019-11-26 18:24:29.305136 Epoch [29/50], Step[1000/8080], Loss: 3.9725, Perplexity: 53.12\n",
      "2019-11-26 18:31:53.323972 Epoch [29/50], Step[2000/8080], Loss: 4.1882, Perplexity: 65.91\n",
      "2019-11-26 18:39:16.251484 Epoch [29/50], Step[3000/8080], Loss: 4.0171, Perplexity: 55.54\n",
      "2019-11-26 18:46:39.684854 Epoch [29/50], Step[4000/8080], Loss: 4.0611, Perplexity: 58.04\n",
      "2019-11-26 18:54:03.736561 Epoch [29/50], Step[5000/8080], Loss: 4.4894, Perplexity: 89.07\n",
      "2019-11-26 19:01:26.720945 Epoch [29/50], Step[6000/8080], Loss: 4.2823, Perplexity: 72.41\n",
      "2019-11-26 19:08:52.725660 Epoch [29/50], Step[7000/8080], Loss: 3.9264, Perplexity: 50.73\n",
      "2019-11-26 19:15:33.047932 Epoch [29/50], Step[8000/8080], Loss: 4.1367, Perplexity: 62.59\n",
      "2019-11-26 19:16:05.922072 Epoch [30/50], Step[0/8080], Loss: 4.2136, Perplexity: 67.60\n",
      "2019-11-26 19:22:49.325682 Epoch [30/50], Step[1000/8080], Loss: 4.0161, Perplexity: 55.49\n",
      "2019-11-26 19:29:41.333017 Epoch [30/50], Step[2000/8080], Loss: 4.1774, Perplexity: 65.20\n",
      "2019-11-26 19:36:33.443266 Epoch [30/50], Step[3000/8080], Loss: 4.0107, Perplexity: 55.19\n",
      "2019-11-26 19:43:24.328449 Epoch [30/50], Step[4000/8080], Loss: 4.0849, Perplexity: 59.43\n",
      "2019-11-26 19:50:14.054089 Epoch [30/50], Step[5000/8080], Loss: 4.4808, Perplexity: 88.30\n",
      "2019-11-26 19:57:03.997706 Epoch [30/50], Step[6000/8080], Loss: 4.2769, Perplexity: 72.02\n",
      "2019-11-26 20:03:52.363679 Epoch [30/50], Step[7000/8080], Loss: 3.9276, Perplexity: 50.79\n",
      "2019-11-26 20:10:44.242763 Epoch [30/50], Step[8000/8080], Loss: 4.1582, Perplexity: 63.95\n",
      "2019-11-26 20:11:16.448162 Epoch [31/50], Step[0/8080], Loss: 4.2175, Perplexity: 67.86\n",
      "2019-11-26 20:18:06.046700 Epoch [31/50], Step[1000/8080], Loss: 4.0114, Perplexity: 55.22\n",
      "2019-11-26 20:24:56.166283 Epoch [31/50], Step[2000/8080], Loss: 4.1757, Perplexity: 65.08\n",
      "2019-11-26 20:31:47.039240 Epoch [31/50], Step[3000/8080], Loss: 4.0133, Perplexity: 55.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 20:38:43.573941 Epoch [31/50], Step[4000/8080], Loss: 4.0750, Perplexity: 58.85\n",
      "2019-11-26 20:45:33.588903 Epoch [31/50], Step[5000/8080], Loss: 4.5274, Perplexity: 92.52\n",
      "2019-11-26 20:52:21.471447 Epoch [31/50], Step[6000/8080], Loss: 4.2727, Perplexity: 71.72\n",
      "2019-11-26 20:59:09.389796 Epoch [31/50], Step[7000/8080], Loss: 3.9272, Perplexity: 50.77\n",
      "2019-11-26 21:06:00.595902 Epoch [31/50], Step[8000/8080], Loss: 4.1320, Perplexity: 62.30\n",
      "2019-11-26 21:06:34.746504 Epoch [32/50], Step[0/8080], Loss: 4.2026, Perplexity: 66.86\n",
      "2019-11-26 21:13:23.622938 Epoch [32/50], Step[1000/8080], Loss: 4.0166, Perplexity: 55.51\n",
      "2019-11-26 21:20:13.487050 Epoch [32/50], Step[2000/8080], Loss: 4.1691, Perplexity: 64.66\n",
      "2019-11-26 21:27:01.936793 Epoch [32/50], Step[3000/8080], Loss: 3.9979, Perplexity: 54.48\n",
      "2019-11-26 21:33:52.961344 Epoch [32/50], Step[4000/8080], Loss: 4.0505, Perplexity: 57.43\n",
      "2019-11-26 21:40:45.689071 Epoch [32/50], Step[5000/8080], Loss: 4.5245, Perplexity: 92.25\n",
      "2019-11-26 21:47:32.679401 Epoch [32/50], Step[6000/8080], Loss: 4.2471, Perplexity: 69.90\n",
      "2019-11-26 21:54:19.305040 Epoch [32/50], Step[7000/8080], Loss: 3.9635, Perplexity: 52.64\n",
      "2019-11-26 22:01:13.374517 Epoch [32/50], Step[8000/8080], Loss: 4.1196, Perplexity: 61.53\n",
      "2019-11-26 22:01:47.668672 Epoch [33/50], Step[0/8080], Loss: 4.2165, Perplexity: 67.79\n",
      "2019-11-26 22:08:35.966077 Epoch [33/50], Step[1000/8080], Loss: 4.0152, Perplexity: 55.43\n",
      "2019-11-26 22:15:29.790172 Epoch [33/50], Step[2000/8080], Loss: 4.1631, Perplexity: 64.27\n",
      "2019-11-26 22:22:18.435761 Epoch [33/50], Step[3000/8080], Loss: 4.0016, Perplexity: 54.68\n",
      "2019-11-26 22:29:08.089248 Epoch [33/50], Step[4000/8080], Loss: 4.0446, Perplexity: 57.09\n",
      "2019-11-26 22:35:58.818329 Epoch [33/50], Step[5000/8080], Loss: 4.5348, Perplexity: 93.21\n",
      "2019-11-26 22:42:50.812874 Epoch [33/50], Step[6000/8080], Loss: 4.2530, Perplexity: 70.31\n",
      "2019-11-26 22:49:43.923424 Epoch [33/50], Step[7000/8080], Loss: 3.9456, Perplexity: 51.71\n",
      "2019-11-26 22:56:36.097804 Epoch [33/50], Step[8000/8080], Loss: 4.1124, Perplexity: 61.09\n",
      "2019-11-26 22:57:09.740313 Epoch [34/50], Step[0/8080], Loss: 4.1936, Perplexity: 66.26\n",
      "2019-11-26 23:03:56.706707 Epoch [34/50], Step[1000/8080], Loss: 4.0122, Perplexity: 55.27\n",
      "2019-11-26 23:10:50.607855 Epoch [34/50], Step[2000/8080], Loss: 4.1447, Perplexity: 63.10\n",
      "2019-11-26 23:17:45.152861 Epoch [34/50], Step[3000/8080], Loss: 3.9758, Perplexity: 53.29\n",
      "2019-11-26 23:24:39.730196 Epoch [34/50], Step[4000/8080], Loss: 4.0213, Perplexity: 55.77\n",
      "2019-11-26 23:31:27.897134 Epoch [34/50], Step[5000/8080], Loss: 4.5000, Perplexity: 90.01\n",
      "2019-11-26 23:38:14.692486 Epoch [34/50], Step[6000/8080], Loss: 4.2693, Perplexity: 71.47\n",
      "2019-11-26 23:45:08.257161 Epoch [34/50], Step[7000/8080], Loss: 3.9354, Perplexity: 51.18\n",
      "2019-11-26 23:51:58.850998 Epoch [34/50], Step[8000/8080], Loss: 4.0882, Perplexity: 59.63\n",
      "2019-11-26 23:52:31.696751 Epoch [35/50], Step[0/8080], Loss: 4.1980, Perplexity: 66.56\n",
      "2019-11-26 23:59:22.734092 Epoch [35/50], Step[1000/8080], Loss: 3.9803, Perplexity: 53.53\n",
      "2019-11-27 00:06:11.743740 Epoch [35/50], Step[2000/8080], Loss: 4.1373, Perplexity: 62.64\n",
      "2019-11-27 00:12:58.648284 Epoch [35/50], Step[3000/8080], Loss: 3.9961, Perplexity: 54.38\n",
      "2019-11-27 00:19:52.769234 Epoch [35/50], Step[4000/8080], Loss: 4.0532, Perplexity: 57.58\n",
      "2019-11-27 00:26:43.236822 Epoch [35/50], Step[5000/8080], Loss: 4.5279, Perplexity: 92.56\n",
      "2019-11-27 00:33:31.017213 Epoch [35/50], Step[6000/8080], Loss: 4.2581, Perplexity: 70.67\n",
      "2019-11-27 00:40:24.273646 Epoch [35/50], Step[7000/8080], Loss: 3.9457, Perplexity: 51.71\n",
      "2019-11-27 00:47:18.203194 Epoch [35/50], Step[8000/8080], Loss: 4.1158, Perplexity: 61.30\n",
      "2019-11-27 00:47:50.437380 Epoch [36/50], Step[0/8080], Loss: 4.1767, Perplexity: 65.15\n",
      "2019-11-27 00:54:35.895876 Epoch [36/50], Step[1000/8080], Loss: 4.0240, Perplexity: 55.92\n",
      "2019-11-27 01:01:27.493560 Epoch [36/50], Step[2000/8080], Loss: 4.1442, Perplexity: 63.07\n",
      "2019-11-27 01:08:17.819306 Epoch [36/50], Step[3000/8080], Loss: 4.0066, Perplexity: 54.96\n",
      "2019-11-27 01:15:06.674761 Epoch [36/50], Step[4000/8080], Loss: 4.0353, Perplexity: 56.56\n",
      "2019-11-27 01:21:54.985792 Epoch [36/50], Step[5000/8080], Loss: 4.5153, Perplexity: 91.41\n",
      "2019-11-27 01:28:44.473445 Epoch [36/50], Step[6000/8080], Loss: 4.2501, Perplexity: 70.11\n",
      "2019-11-27 01:35:28.126313 Epoch [36/50], Step[7000/8080], Loss: 3.9637, Perplexity: 52.65\n",
      "2019-11-27 01:42:17.703778 Epoch [36/50], Step[8000/8080], Loss: 4.1220, Perplexity: 61.68\n",
      "2019-11-27 01:42:51.291911 Epoch [37/50], Step[0/8080], Loss: 4.2017, Perplexity: 66.80\n",
      "2019-11-27 01:49:41.176896 Epoch [37/50], Step[1000/8080], Loss: 4.0148, Perplexity: 55.41\n",
      "2019-11-27 01:56:32.356027 Epoch [37/50], Step[2000/8080], Loss: 4.1611, Perplexity: 64.14\n",
      "2019-11-27 02:03:24.221770 Epoch [37/50], Step[3000/8080], Loss: 3.9715, Perplexity: 53.07\n",
      "2019-11-27 02:10:15.638263 Epoch [37/50], Step[4000/8080], Loss: 4.0462, Perplexity: 57.18\n",
      "2019-11-27 02:17:06.617312 Epoch [37/50], Step[5000/8080], Loss: 4.5035, Perplexity: 90.33\n",
      "2019-11-27 02:23:54.972843 Epoch [37/50], Step[6000/8080], Loss: 4.2372, Perplexity: 69.21\n",
      "2019-11-27 02:30:49.194475 Epoch [37/50], Step[7000/8080], Loss: 3.9559, Perplexity: 52.24\n",
      "2019-11-27 02:37:37.106198 Epoch [37/50], Step[8000/8080], Loss: 4.1013, Perplexity: 60.42\n",
      "2019-11-27 02:38:09.771726 Epoch [38/50], Step[0/8080], Loss: 4.1996, Perplexity: 66.66\n",
      "2019-11-27 02:44:59.537743 Epoch [38/50], Step[1000/8080], Loss: 3.9974, Perplexity: 54.46\n",
      "2019-11-27 02:51:49.425623 Epoch [38/50], Step[2000/8080], Loss: 4.1549, Perplexity: 63.75\n",
      "2019-11-27 02:58:40.274316 Epoch [38/50], Step[3000/8080], Loss: 4.0000, Perplexity: 54.60\n",
      "2019-11-27 03:05:32.239508 Epoch [38/50], Step[4000/8080], Loss: 4.0461, Perplexity: 57.18\n",
      "2019-11-27 03:12:27.255963 Epoch [38/50], Step[5000/8080], Loss: 4.4999, Perplexity: 90.01\n",
      "2019-11-27 03:19:21.209204 Epoch [38/50], Step[6000/8080], Loss: 4.2431, Perplexity: 69.63\n",
      "2019-11-27 03:26:11.174649 Epoch [38/50], Step[7000/8080], Loss: 3.9826, Perplexity: 53.66\n",
      "2019-11-27 03:32:56.593687 Epoch [38/50], Step[8000/8080], Loss: 4.0794, Perplexity: 59.11\n",
      "2019-11-27 03:33:29.939136 Epoch [39/50], Step[0/8080], Loss: 4.1813, Perplexity: 65.45\n",
      "2019-11-27 03:40:21.566703 Epoch [39/50], Step[1000/8080], Loss: 4.0635, Perplexity: 58.18\n",
      "2019-11-27 03:47:09.804691 Epoch [39/50], Step[2000/8080], Loss: 4.1343, Perplexity: 62.44\n",
      "2019-11-27 03:54:01.588208 Epoch [39/50], Step[3000/8080], Loss: 3.9905, Perplexity: 54.08\n",
      "2019-11-27 04:00:52.915874 Epoch [39/50], Step[4000/8080], Loss: 4.0714, Perplexity: 58.64\n",
      "2019-11-27 04:07:46.423558 Epoch [39/50], Step[5000/8080], Loss: 4.5332, Perplexity: 93.06\n",
      "2019-11-27 04:14:42.341585 Epoch [39/50], Step[6000/8080], Loss: 4.2668, Perplexity: 71.29\n",
      "2019-11-27 04:21:29.847384 Epoch [39/50], Step[7000/8080], Loss: 3.9600, Perplexity: 52.46\n",
      "2019-11-27 04:28:16.398102 Epoch [39/50], Step[8000/8080], Loss: 4.1063, Perplexity: 60.72\n",
      "2019-11-27 04:28:49.048833 Epoch [40/50], Step[0/8080], Loss: 4.1870, Perplexity: 65.83\n",
      "2019-11-27 04:35:41.059567 Epoch [40/50], Step[1000/8080], Loss: 4.0527, Perplexity: 57.55\n",
      "2019-11-27 04:42:31.595182 Epoch [40/50], Step[2000/8080], Loss: 4.1342, Perplexity: 62.44\n",
      "2019-11-27 04:49:23.067631 Epoch [40/50], Step[3000/8080], Loss: 4.0114, Perplexity: 55.23\n",
      "2019-11-27 04:56:13.457371 Epoch [40/50], Step[4000/8080], Loss: 4.0428, Perplexity: 56.98\n",
      "2019-11-27 05:03:04.148069 Epoch [40/50], Step[5000/8080], Loss: 4.5120, Perplexity: 91.10\n",
      "2019-11-27 05:09:54.570383 Epoch [40/50], Step[6000/8080], Loss: 4.2463, Perplexity: 69.85\n",
      "2019-11-27 05:16:34.819965 Epoch [40/50], Step[7000/8080], Loss: 3.9758, Perplexity: 53.29\n",
      "2019-11-27 05:23:17.842804 Epoch [40/50], Step[8000/8080], Loss: 4.1208, Perplexity: 61.61\n",
      "2019-11-27 05:23:49.796055 Epoch [41/50], Step[0/8080], Loss: 4.1988, Perplexity: 66.60\n",
      "2019-11-27 05:30:34.477284 Epoch [41/50], Step[1000/8080], Loss: 4.0258, Perplexity: 56.02\n",
      "2019-11-27 05:37:11.021838 Epoch [41/50], Step[2000/8080], Loss: 4.1548, Perplexity: 63.74\n",
      "2019-11-27 05:43:52.958089 Epoch [41/50], Step[3000/8080], Loss: 3.9873, Perplexity: 53.91\n",
      "2019-11-27 05:50:34.013846 Epoch [41/50], Step[4000/8080], Loss: 4.0568, Perplexity: 57.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-27 05:57:11.921065 Epoch [41/50], Step[5000/8080], Loss: 4.5085, Perplexity: 90.78\n",
      "2019-11-27 06:03:53.958136 Epoch [41/50], Step[6000/8080], Loss: 4.2842, Perplexity: 72.54\n",
      "2019-11-27 06:10:32.121488 Epoch [41/50], Step[7000/8080], Loss: 3.9565, Perplexity: 52.27\n",
      "2019-11-27 06:17:11.585669 Epoch [41/50], Step[8000/8080], Loss: 4.1118, Perplexity: 61.06\n",
      "2019-11-27 06:17:43.752654 Epoch [42/50], Step[0/8080], Loss: 4.1660, Perplexity: 64.45\n",
      "2019-11-27 06:24:27.363837 Epoch [42/50], Step[1000/8080], Loss: 4.0776, Perplexity: 59.00\n",
      "2019-11-27 06:31:03.391964 Epoch [42/50], Step[2000/8080], Loss: 4.1632, Perplexity: 64.28\n",
      "2019-11-27 06:37:43.928041 Epoch [42/50], Step[3000/8080], Loss: 3.9776, Perplexity: 53.39\n",
      "2019-11-27 06:44:24.703259 Epoch [42/50], Step[4000/8080], Loss: 4.0352, Perplexity: 56.56\n",
      "2019-11-27 06:51:04.587880 Epoch [42/50], Step[5000/8080], Loss: 4.5281, Perplexity: 92.58\n",
      "2019-11-27 06:57:46.880454 Epoch [42/50], Step[6000/8080], Loss: 4.2589, Perplexity: 70.73\n",
      "2019-11-27 07:04:27.328959 Epoch [42/50], Step[7000/8080], Loss: 3.9584, Perplexity: 52.37\n",
      "2019-11-27 07:11:04.030279 Epoch [42/50], Step[8000/8080], Loss: 4.1161, Perplexity: 61.32\n",
      "2019-11-27 07:11:36.308737 Epoch [43/50], Step[0/8080], Loss: 4.1912, Perplexity: 66.10\n",
      "2019-11-27 07:18:15.165122 Epoch [43/50], Step[1000/8080], Loss: 4.0083, Perplexity: 55.05\n",
      "2019-11-27 07:24:54.120841 Epoch [43/50], Step[2000/8080], Loss: 4.1412, Perplexity: 62.88\n",
      "2019-11-27 07:31:27.135858 Epoch [43/50], Step[3000/8080], Loss: 4.0157, Perplexity: 55.46\n",
      "2019-11-27 07:38:04.057829 Epoch [43/50], Step[4000/8080], Loss: 4.0381, Perplexity: 56.72\n",
      "2019-11-27 07:44:43.160950 Epoch [43/50], Step[5000/8080], Loss: 4.5304, Perplexity: 92.80\n",
      "2019-11-27 07:51:24.862832 Epoch [43/50], Step[6000/8080], Loss: 4.2871, Perplexity: 72.75\n",
      "2019-11-27 07:58:05.908796 Epoch [43/50], Step[7000/8080], Loss: 3.9805, Perplexity: 53.54\n",
      "2019-11-27 08:04:45.552777 Epoch [43/50], Step[8000/8080], Loss: 4.0903, Perplexity: 59.76\n",
      "2019-11-27 08:05:17.206707 Epoch [44/50], Step[0/8080], Loss: 4.1972, Perplexity: 66.50\n",
      "2019-11-27 08:11:52.428426 Epoch [44/50], Step[1000/8080], Loss: 4.0400, Perplexity: 56.82\n",
      "2019-11-27 08:18:29.950251 Epoch [44/50], Step[2000/8080], Loss: 4.1209, Perplexity: 61.62\n",
      "2019-11-27 08:25:12.824892 Epoch [44/50], Step[3000/8080], Loss: 3.9694, Perplexity: 52.95\n",
      "2019-11-27 08:31:47.810980 Epoch [44/50], Step[4000/8080], Loss: 4.0242, Perplexity: 55.94\n",
      "2019-11-27 08:38:24.806041 Epoch [44/50], Step[5000/8080], Loss: 4.4993, Perplexity: 89.95\n",
      "2019-11-27 08:45:07.192718 Epoch [44/50], Step[6000/8080], Loss: 4.2651, Perplexity: 71.17\n",
      "2019-11-27 08:51:47.716278 Epoch [44/50], Step[7000/8080], Loss: 3.9701, Perplexity: 52.99\n",
      "2019-11-27 08:58:28.402626 Epoch [44/50], Step[8000/8080], Loss: 4.0995, Perplexity: 60.31\n",
      "2019-11-27 08:58:59.643501 Epoch [45/50], Step[0/8080], Loss: 4.1831, Perplexity: 65.57\n",
      "2019-11-27 09:05:39.387733 Epoch [45/50], Step[1000/8080], Loss: 4.0738, Perplexity: 58.78\n",
      "2019-11-27 09:12:15.603935 Epoch [45/50], Step[2000/8080], Loss: 4.1222, Perplexity: 61.70\n",
      "2019-11-27 09:18:52.871009 Epoch [45/50], Step[3000/8080], Loss: 3.9924, Perplexity: 54.19\n",
      "2019-11-27 09:25:31.194986 Epoch [45/50], Step[4000/8080], Loss: 4.0257, Perplexity: 56.02\n",
      "2019-11-27 09:32:12.236649 Epoch [45/50], Step[5000/8080], Loss: 4.4910, Perplexity: 89.21\n",
      "2019-11-27 09:38:47.662768 Epoch [45/50], Step[6000/8080], Loss: 4.2798, Perplexity: 72.23\n",
      "2019-11-27 09:45:27.005790 Epoch [45/50], Step[7000/8080], Loss: 3.9579, Perplexity: 52.35\n",
      "2019-11-27 09:52:05.156623 Epoch [45/50], Step[8000/8080], Loss: 4.1160, Perplexity: 61.31\n",
      "2019-11-27 09:52:36.898523 Epoch [46/50], Step[0/8080], Loss: 4.1820, Perplexity: 65.50\n",
      "2019-11-27 09:59:15.769591 Epoch [46/50], Step[1000/8080], Loss: 4.0514, Perplexity: 57.48\n",
      "2019-11-27 10:05:57.592166 Epoch [46/50], Step[2000/8080], Loss: 4.1187, Perplexity: 61.48\n",
      "2019-11-27 10:12:36.272503 Epoch [46/50], Step[3000/8080], Loss: 4.0209, Perplexity: 55.75\n",
      "2019-11-27 10:19:17.774833 Epoch [46/50], Step[4000/8080], Loss: 4.0040, Perplexity: 54.81\n",
      "2019-11-27 10:25:56.993149 Epoch [46/50], Step[5000/8080], Loss: 4.5028, Perplexity: 90.27\n",
      "2019-11-27 10:32:34.355211 Epoch [46/50], Step[6000/8080], Loss: 4.2318, Perplexity: 68.84\n",
      "2019-11-27 10:39:08.017957 Epoch [46/50], Step[7000/8080], Loss: 3.9674, Perplexity: 52.85\n",
      "2019-11-27 10:45:46.714379 Epoch [46/50], Step[8000/8080], Loss: 4.0943, Perplexity: 60.00\n",
      "2019-11-27 10:46:18.534691 Epoch [47/50], Step[0/8080], Loss: 4.1896, Perplexity: 65.99\n",
      "2019-11-27 10:53:01.976915 Epoch [47/50], Step[1000/8080], Loss: 4.0426, Perplexity: 56.97\n",
      "2019-11-27 10:59:41.348145 Epoch [47/50], Step[2000/8080], Loss: 4.0794, Perplexity: 59.11\n",
      "2019-11-27 11:06:21.885444 Epoch [47/50], Step[3000/8080], Loss: 4.0122, Perplexity: 55.27\n",
      "2019-11-27 11:13:05.017578 Epoch [47/50], Step[4000/8080], Loss: 3.9998, Perplexity: 54.59\n",
      "2019-11-27 11:19:44.315390 Epoch [47/50], Step[5000/8080], Loss: 4.5033, Perplexity: 90.32\n",
      "2019-11-27 11:26:23.146346 Epoch [47/50], Step[6000/8080], Loss: 4.2475, Perplexity: 69.93\n",
      "2019-11-27 11:33:00.267427 Epoch [47/50], Step[7000/8080], Loss: 3.9911, Perplexity: 54.11\n",
      "2019-11-27 11:39:43.448238 Epoch [47/50], Step[8000/8080], Loss: 4.1106, Perplexity: 60.98\n",
      "2019-11-27 11:40:16.070307 Epoch [48/50], Step[0/8080], Loss: 4.1704, Perplexity: 64.74\n",
      "2019-11-27 11:46:55.825065 Epoch [48/50], Step[1000/8080], Loss: 4.0734, Perplexity: 58.76\n",
      "2019-11-27 11:53:34.323996 Epoch [48/50], Step[2000/8080], Loss: 4.1189, Perplexity: 61.49\n",
      "2019-11-27 12:00:18.049716 Epoch [48/50], Step[3000/8080], Loss: 4.0113, Perplexity: 55.22\n",
      "2019-11-27 12:06:57.570450 Epoch [48/50], Step[4000/8080], Loss: 4.0411, Perplexity: 56.89\n",
      "2019-11-27 12:13:40.052053 Epoch [48/50], Step[5000/8080], Loss: 4.5394, Perplexity: 93.63\n",
      "2019-11-27 12:20:16.297500 Epoch [48/50], Step[6000/8080], Loss: 4.2383, Perplexity: 69.29\n",
      "2019-11-27 12:26:54.837368 Epoch [48/50], Step[7000/8080], Loss: 3.9766, Perplexity: 53.34\n",
      "2019-11-27 12:33:32.382571 Epoch [48/50], Step[8000/8080], Loss: 4.1201, Perplexity: 61.57\n",
      "2019-11-27 12:34:04.245400 Epoch [49/50], Step[0/8080], Loss: 4.1803, Perplexity: 65.38\n",
      "2019-11-27 12:40:41.888029 Epoch [49/50], Step[1000/8080], Loss: 4.0381, Perplexity: 56.72\n",
      "2019-11-27 12:47:22.713447 Epoch [49/50], Step[2000/8080], Loss: 4.1052, Perplexity: 60.65\n",
      "2019-11-27 12:54:02.116129 Epoch [49/50], Step[3000/8080], Loss: 4.0025, Perplexity: 54.74\n",
      "2019-11-27 13:00:38.552994 Epoch [49/50], Step[4000/8080], Loss: 4.0060, Perplexity: 54.93\n",
      "2019-11-27 13:07:16.917757 Epoch [49/50], Step[5000/8080], Loss: 4.5365, Perplexity: 93.36\n",
      "2019-11-27 13:13:54.949943 Epoch [49/50], Step[6000/8080], Loss: 4.2438, Perplexity: 69.68\n",
      "2019-11-27 13:20:39.638890 Epoch [49/50], Step[7000/8080], Loss: 3.9943, Perplexity: 54.29\n",
      "2019-11-27 13:27:21.270138 Epoch [49/50], Step[8000/8080], Loss: 4.0931, Perplexity: 59.93\n",
      "2019-11-27 13:27:53.287933 Epoch [50/50], Step[0/8080], Loss: 4.1763, Perplexity: 65.12\n",
      "2019-11-27 13:34:30.881027 Epoch [50/50], Step[1000/8080], Loss: 4.0681, Perplexity: 58.45\n",
      "2019-11-27 13:41:10.204158 Epoch [50/50], Step[2000/8080], Loss: 4.1256, Perplexity: 61.91\n",
      "2019-11-27 13:47:50.126560 Epoch [50/50], Step[3000/8080], Loss: 4.0119, Perplexity: 55.25\n",
      "2019-11-27 13:54:29.555922 Epoch [50/50], Step[4000/8080], Loss: 3.9869, Perplexity: 53.89\n",
      "2019-11-27 14:01:05.432061 Epoch [50/50], Step[5000/8080], Loss: 4.5045, Perplexity: 90.42\n",
      "2019-11-27 14:07:46.594332 Epoch [50/50], Step[6000/8080], Loss: 4.2460, Perplexity: 69.83\n",
      "2019-11-27 14:14:24.455687 Epoch [50/50], Step[7000/8080], Loss: 4.0068, Perplexity: 54.97\n",
      "2019-11-27 14:20:02.113908 Epoch [50/50], Step[8000/8080], Loss: 4.0941, Perplexity: 59.99\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    state = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Get images\n",
    "        images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "        for b, seq in enumerate(ids):\n",
    "            for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "                images[b, s] = np.load(f'char_img/noto_CJK/msr3/{idx}.npy').reshape(char_size,char_size,1)\n",
    "        images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "        images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "        images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "        \n",
    "        # Get encoded images\n",
    "        cnn_o = cnn_encoder(images)\n",
    "        cnn_o = cnn_o.view(inputs.size(0), inputs.size(1), -1)\n",
    "               \n",
    "        # Forward pass\n",
    "        state = detach(state)\n",
    "        outputs, state = model(cnn_o, state)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        cnn_encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(params, 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 1000 == 0:\n",
    "            print ('{} Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(datetime.now(), epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perplexity: 64.87206335331925\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "cnn_encoder.eval()\n",
    "\n",
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = ids[:, i:i+seq_length].to(device)\n",
    "    targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Get images\n",
    "    images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "    for b, seq in enumerate(ids):\n",
    "        for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "            images[b, s] = np.load(f'char_img/noto_CJK/msr3/{idx}.npy').reshape(char_size,char_size,1)\n",
    "    images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "    images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "    images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "\n",
    "    # Get encoded images\n",
    "    cnn_o = cnn_encoder(images)\n",
    "    cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(cnn_o, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "    \n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Train Perplexity: {perplexity / num_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = corpus.get_data('icwb2-data/testing/msr_test.utf8', batch_size)\n",
    "# filter out unknown character\n",
    "test_ids = test_ids.view(-1)\n",
    "mask = test_ids < vocab_size\n",
    "test_ids = test_ids[mask]\n",
    "num_batches = test_ids.size(0) // batch_size\n",
    "test_ids = test_ids[:num_batches*batch_size]\n",
    "test_ids = test_ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 70.38195446893054\n"
     ]
    }
   ],
   "source": [
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = test_ids[:, i:i+seq_length].to(device)\n",
    "    targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Get images\n",
    "    images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "    for b, seq in enumerate(test_ids):\n",
    "        for s, idx in enumerate(test_ids[b][i:i+seq_length]):\n",
    "            images[b, s] = np.load(f'char_img/noto_CJK/msr3/{idx}.npy').reshape(char_size,char_size,1)\n",
    "    images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "    images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "    images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "\n",
    "    # Get encoded images\n",
    "    cnn_o = cnn_encoder(images)\n",
    "    cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(cnn_o, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Test Perplexity: {perplexity / num_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch1.2)",
   "language": "python",
   "name": "pytorch1.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
