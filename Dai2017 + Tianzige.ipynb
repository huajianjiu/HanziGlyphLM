{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renderer\n",
    "from PIL import ImageFont\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "char_size = 12\n",
    "# char render\n",
    "def render(text, font=None):\n",
    "    if font is None:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\", char_size)\n",
    "    mask = font.getmask(text)\n",
    "    size = mask.size[::-1]\n",
    "    a = np.asarray(mask).reshape(size) / 255\n",
    "    res = cv2.resize(a, dsize=(char_size, char_size), interpolation=cv2.INTER_CUBIC)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model/data_utils.py\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, max_size=None):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 1\n",
    "        self.word2idx['⸘'] = 0 # as unk\n",
    "        self.idx2word[0] = '⸘'\n",
    "        self.max_size = max_size + 1\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx and self.idx < self.max_size:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, max_size=None):\n",
    "        self.dictionary = Dictionary(max_size=max_size)\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "#         with open(path, 'r') as f:\n",
    "#             tokens = 0\n",
    "#             for line in f:\n",
    "#                 words = line.split() + ['<eos>']\n",
    "#                 tokens += len(words)\n",
    "#                 for word in words: \n",
    "#                     self.dictionary.add_word(word)  \n",
    "\n",
    "        # split words to char and add to dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                tokens += len(chars)\n",
    "                for char in chars:\n",
    "                    self.dictionary.add_word(char)\n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                for char in chars:\n",
    "                    if char in self.dictionary.word2idx:\n",
    "                        ids[token] = self.dictionary.word2idx[char]\n",
    "                        token += 1\n",
    "                    else:\n",
    "                        ids[token] = self.dictionary.word2idx['⸘']\n",
    "                        token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, char_id, char_cnn_o, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(char_id) + char_cnn_o\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, h = self.gru(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "\"\"\"\n",
    "From https://github.com/ShannonAI/glyce/blob/master/glyce/glyph_cnn_models/glyph_group_cnn.py\n",
    "@author: wuwei\n",
    "@contact: wu.wei@pku.edu.cn\n",
    "@version: 1.0\n",
    "@file: cnn_for_fonts.py\n",
    "@time: 19-1-2 上午11:07\n",
    "用CNN将字体的灰度图卷积成特征向量\n",
    "\"\"\"\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class GlyphGroupCNN(nn.Module):     #55000\n",
    "    def __init__(self, cnn_type='simple', kernel_size=5, font_channels=1, shuffle=False, ntokens=4000,\n",
    "                 num_features=8*12*4, final_width=2, cnn_drop=0.5, groups=16):\n",
    "        super(GlyphGroupCNN, self).__init__()\n",
    "        self.aux_logits=False\n",
    "        self.cnn_type = cnn_type\n",
    "        output_channels = num_features\n",
    "        self.conv1 = nn.Conv2d(font_channels, output_channels, kernel_size)\n",
    "        midchannels = output_channels//4\n",
    "        self.mid_groups = max(groups//2, 1)\n",
    "        self.downsample = nn.Conv2d(output_channels, midchannels, kernel_size=1, groups=self.mid_groups)   #//2是因为参数量主要集中在下一层\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((final_width, final_width))\n",
    "        self.num_features = num_features\n",
    "        self.reweight_conv = nn.Conv2d(midchannels, output_channels, kernel_size=final_width, groups=groups)\n",
    "        self.output_channels=output_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.dropout = nn.Dropout(cnn_drop)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.base_conv(x)\n",
    "        x = F.relu(self.conv1(x) ) # [(seq_len*batchsize, Co, h, w), ...]*len(Ks)\n",
    "        x = self.max_pool(x)  # n, c, 2, 2\n",
    "        x = self.downsample(x)\n",
    "        if self.shuffle:\n",
    "            x = channel_shuffle(x, groups=2)\n",
    "        x = F.relu(self.reweight_conv(x))\n",
    "        if self.shuffle:\n",
    "            x = channel_shuffle(x, groups=2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        return x  # (seq_len*batchsize, nfeats)\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.uniform_(-initrange, initrange)\n",
    "                # init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.normal_(mean=1, std=0.001)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "embed_size = 256\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "seq_length = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Load dataset\n",
    "corpus = Corpus(max_size=4000)\n",
    "ids = corpus.get_data('icwb2-data/training/msr_training.utf8', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save char images for reference\n",
    "for char, idx in corpus.dictionary.word2idx.items():\n",
    "    np.save(f'char_img/noto_CJK/msr2/{idx}.npy', render(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "cnn_encoder = GlyphGroupCNN(num_features=256).to(device)\n",
    "model.train()\n",
    "cnn_encoder.train()\n",
    "params = list(model.parameters())+list(cnn_encoder.parameters())\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# Truncated backpropagation\n",
    "def detach(state):\n",
    "    return state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-25 16:44:04.337784 Epoch [1/50], Step[0/8080], Loss: 8.3254, Perplexity: 4127.25\n",
      "2019-11-25 16:50:21.965699 Epoch [1/50], Step[1000/8080], Loss: 5.0842, Perplexity: 161.45\n",
      "2019-11-25 16:56:39.449228 Epoch [1/50], Step[2000/8080], Loss: 4.6569, Perplexity: 105.31\n",
      "2019-11-25 17:02:40.757554 Epoch [1/50], Step[3000/8080], Loss: 4.4453, Perplexity: 85.23\n",
      "2019-11-25 17:08:18.068562 Epoch [1/50], Step[4000/8080], Loss: 4.3006, Perplexity: 73.74\n",
      "2019-11-25 17:14:42.974429 Epoch [1/50], Step[5000/8080], Loss: 4.6906, Perplexity: 108.92\n",
      "2019-11-25 17:21:07.573676 Epoch [1/50], Step[6000/8080], Loss: 4.6470, Perplexity: 104.27\n",
      "2019-11-25 17:27:41.312060 Epoch [1/50], Step[7000/8080], Loss: 4.1596, Perplexity: 64.05\n",
      "2019-11-25 17:34:11.123083 Epoch [1/50], Step[8000/8080], Loss: 4.3503, Perplexity: 77.50\n",
      "2019-11-25 17:34:46.625706 Epoch [2/50], Step[0/8080], Loss: 4.3467, Perplexity: 77.22\n",
      "2019-11-25 17:42:19.159301 Epoch [2/50], Step[1000/8080], Loss: 4.1743, Perplexity: 64.99\n",
      "2019-11-25 17:49:32.473567 Epoch [2/50], Step[2000/8080], Loss: 4.1395, Perplexity: 62.77\n",
      "2019-11-25 17:56:38.391485 Epoch [2/50], Step[3000/8080], Loss: 4.0865, Perplexity: 59.53\n",
      "2019-11-25 18:03:58.740461 Epoch [2/50], Step[4000/8080], Loss: 3.9545, Perplexity: 52.17\n",
      "2019-11-25 18:11:11.523584 Epoch [2/50], Step[5000/8080], Loss: 4.4991, Perplexity: 89.93\n",
      "2019-11-25 18:18:21.164525 Epoch [2/50], Step[6000/8080], Loss: 4.3482, Perplexity: 77.34\n",
      "2019-11-25 18:25:38.191105 Epoch [2/50], Step[7000/8080], Loss: 3.9390, Perplexity: 51.37\n",
      "2019-11-25 18:32:49.923253 Epoch [2/50], Step[8000/8080], Loss: 4.2099, Perplexity: 67.35\n",
      "2019-11-25 18:33:20.638594 Epoch [3/50], Step[0/8080], Loss: 4.1774, Perplexity: 65.20\n",
      "2019-11-25 18:39:42.097825 Epoch [3/50], Step[1000/8080], Loss: 4.0418, Perplexity: 56.93\n",
      "2019-11-25 18:46:04.636357 Epoch [3/50], Step[2000/8080], Loss: 4.0495, Perplexity: 57.37\n",
      "2019-11-25 18:52:21.102352 Epoch [3/50], Step[3000/8080], Loss: 3.9613, Perplexity: 52.53\n",
      "2019-11-25 18:58:43.808960 Epoch [3/50], Step[4000/8080], Loss: 3.8625, Perplexity: 47.58\n",
      "2019-11-25 19:05:06.792917 Epoch [3/50], Step[5000/8080], Loss: 4.4431, Perplexity: 85.04\n",
      "2019-11-25 19:12:20.224895 Epoch [3/50], Step[6000/8080], Loss: 4.2424, Perplexity: 69.57\n",
      "2019-11-25 19:19:19.707246 Epoch [3/50], Step[7000/8080], Loss: 3.8533, Perplexity: 47.15\n",
      "2019-11-25 19:26:09.240215 Epoch [3/50], Step[8000/8080], Loss: 4.1374, Perplexity: 62.64\n",
      "2019-11-25 19:26:39.538733 Epoch [4/50], Step[0/8080], Loss: 4.1250, Perplexity: 61.87\n",
      "2019-11-25 19:33:00.656476 Epoch [4/50], Step[1000/8080], Loss: 3.9709, Perplexity: 53.03\n",
      "2019-11-25 19:39:23.036777 Epoch [4/50], Step[2000/8080], Loss: 4.0009, Perplexity: 54.65\n",
      "2019-11-25 19:45:50.184294 Epoch [4/50], Step[3000/8080], Loss: 3.8987, Perplexity: 49.34\n",
      "2019-11-25 19:52:10.414680 Epoch [4/50], Step[4000/8080], Loss: 3.8321, Perplexity: 46.16\n",
      "2019-11-25 19:58:34.910099 Epoch [4/50], Step[5000/8080], Loss: 4.4272, Perplexity: 83.70\n",
      "2019-11-25 20:05:01.618994 Epoch [4/50], Step[6000/8080], Loss: 4.1940, Perplexity: 66.29\n",
      "2019-11-25 20:11:26.060419 Epoch [4/50], Step[7000/8080], Loss: 3.8095, Perplexity: 45.13\n",
      "2019-11-25 20:17:57.132088 Epoch [4/50], Step[8000/8080], Loss: 4.0951, Perplexity: 60.05\n",
      "2019-11-25 20:18:27.548241 Epoch [5/50], Step[0/8080], Loss: 4.1056, Perplexity: 60.68\n",
      "2019-11-25 20:24:51.009328 Epoch [5/50], Step[1000/8080], Loss: 3.9226, Perplexity: 50.53\n",
      "2019-11-25 20:31:13.375767 Epoch [5/50], Step[2000/8080], Loss: 3.9687, Perplexity: 52.92\n",
      "2019-11-25 20:37:37.015523 Epoch [5/50], Step[3000/8080], Loss: 3.8685, Perplexity: 47.87\n",
      "2019-11-25 20:43:57.725539 Epoch [5/50], Step[4000/8080], Loss: 3.7998, Perplexity: 44.69\n",
      "2019-11-25 20:50:23.218449 Epoch [5/50], Step[5000/8080], Loss: 4.4248, Perplexity: 83.49\n",
      "2019-11-25 20:56:48.044263 Epoch [5/50], Step[6000/8080], Loss: 4.1712, Perplexity: 64.79\n",
      "2019-11-25 21:03:07.246281 Epoch [5/50], Step[7000/8080], Loss: 3.7661, Perplexity: 43.21\n",
      "2019-11-25 21:09:27.044124 Epoch [5/50], Step[8000/8080], Loss: 4.0721, Perplexity: 58.68\n",
      "2019-11-25 21:09:57.967831 Epoch [6/50], Step[0/8080], Loss: 4.0807, Perplexity: 59.19\n",
      "2019-11-25 21:16:23.036951 Epoch [6/50], Step[1000/8080], Loss: 3.8924, Perplexity: 49.03\n",
      "2019-11-25 21:22:46.856429 Epoch [6/50], Step[2000/8080], Loss: 3.9760, Perplexity: 53.30\n",
      "2019-11-25 21:29:16.058139 Epoch [6/50], Step[3000/8080], Loss: 3.8585, Perplexity: 47.40\n",
      "2019-11-25 21:35:49.135010 Epoch [6/50], Step[4000/8080], Loss: 3.7717, Perplexity: 43.45\n",
      "2019-11-25 21:42:42.188643 Epoch [6/50], Step[5000/8080], Loss: 4.4139, Perplexity: 82.59\n",
      "2019-11-25 21:49:23.138164 Epoch [6/50], Step[6000/8080], Loss: 4.1556, Perplexity: 63.79\n",
      "2019-11-25 21:56:04.180231 Epoch [6/50], Step[7000/8080], Loss: 3.7349, Perplexity: 41.89\n",
      "2019-11-25 22:02:30.478988 Epoch [6/50], Step[8000/8080], Loss: 4.0606, Perplexity: 58.01\n",
      "2019-11-25 22:03:01.056241 Epoch [7/50], Step[0/8080], Loss: 4.0550, Perplexity: 57.68\n",
      "2019-11-25 22:09:19.624263 Epoch [7/50], Step[1000/8080], Loss: 3.8729, Perplexity: 48.08\n",
      "2019-11-25 22:15:47.250604 Epoch [7/50], Step[2000/8080], Loss: 3.9791, Perplexity: 53.47\n",
      "2019-11-25 22:22:12.010934 Epoch [7/50], Step[3000/8080], Loss: 3.8265, Perplexity: 45.90\n",
      "2019-11-25 22:28:34.365329 Epoch [7/50], Step[4000/8080], Loss: 3.7527, Perplexity: 42.64\n",
      "2019-11-25 22:34:58.370888 Epoch [7/50], Step[5000/8080], Loss: 4.3903, Perplexity: 80.67\n",
      "2019-11-25 22:41:19.954609 Epoch [7/50], Step[6000/8080], Loss: 4.1344, Perplexity: 62.45\n",
      "2019-11-25 22:48:07.216491 Epoch [7/50], Step[7000/8080], Loss: 3.7191, Perplexity: 41.23\n",
      "2019-11-25 22:55:15.146824 Epoch [7/50], Step[8000/8080], Loss: 4.0368, Perplexity: 56.64\n",
      "2019-11-25 22:55:50.409764 Epoch [8/50], Step[0/8080], Loss: 4.0434, Perplexity: 57.02\n",
      "2019-11-25 23:02:57.165900 Epoch [8/50], Step[1000/8080], Loss: 3.8442, Perplexity: 46.72\n",
      "2019-11-25 23:10:04.456951 Epoch [8/50], Step[2000/8080], Loss: 3.9615, Perplexity: 52.53\n",
      "2019-11-25 23:17:11.052671 Epoch [8/50], Step[3000/8080], Loss: 3.8154, Perplexity: 45.40\n",
      "2019-11-25 23:24:17.157989 Epoch [8/50], Step[4000/8080], Loss: 3.7184, Perplexity: 41.20\n",
      "2019-11-25 23:31:17.976913 Epoch [8/50], Step[5000/8080], Loss: 4.3563, Perplexity: 77.97\n",
      "2019-11-25 23:38:22.257389 Epoch [8/50], Step[6000/8080], Loss: 4.1083, Perplexity: 60.84\n",
      "2019-11-25 23:45:21.484322 Epoch [8/50], Step[7000/8080], Loss: 3.6910, Perplexity: 40.08\n",
      "2019-11-25 23:52:18.890580 Epoch [8/50], Step[8000/8080], Loss: 4.0372, Perplexity: 56.67\n",
      "2019-11-25 23:52:51.272569 Epoch [9/50], Step[0/8080], Loss: 4.0132, Perplexity: 55.33\n",
      "2019-11-25 23:59:43.926180 Epoch [9/50], Step[1000/8080], Loss: 3.8303, Perplexity: 46.08\n",
      "2019-11-26 00:06:35.237334 Epoch [9/50], Step[2000/8080], Loss: 3.9644, Perplexity: 52.69\n",
      "2019-11-26 00:13:26.876647 Epoch [9/50], Step[3000/8080], Loss: 3.8067, Perplexity: 45.00\n",
      "2019-11-26 00:20:20.205330 Epoch [9/50], Step[4000/8080], Loss: 3.7077, Perplexity: 40.76\n",
      "2019-11-26 00:27:09.355469 Epoch [9/50], Step[5000/8080], Loss: 4.3695, Perplexity: 79.01\n",
      "2019-11-26 00:34:02.141973 Epoch [9/50], Step[6000/8080], Loss: 4.1032, Perplexity: 60.53\n",
      "2019-11-26 00:40:49.960408 Epoch [9/50], Step[7000/8080], Loss: 3.6844, Perplexity: 39.82\n",
      "2019-11-26 00:47:40.723782 Epoch [9/50], Step[8000/8080], Loss: 4.0282, Perplexity: 56.16\n",
      "2019-11-26 00:48:14.284665 Epoch [10/50], Step[0/8080], Loss: 4.0129, Perplexity: 55.30\n",
      "2019-11-26 00:55:02.594273 Epoch [10/50], Step[1000/8080], Loss: 3.7775, Perplexity: 43.71\n",
      "2019-11-26 01:01:54.142716 Epoch [10/50], Step[2000/8080], Loss: 3.9582, Perplexity: 52.37\n",
      "2019-11-26 01:08:43.119007 Epoch [10/50], Step[3000/8080], Loss: 3.7779, Perplexity: 43.72\n",
      "2019-11-26 01:15:32.032357 Epoch [10/50], Step[4000/8080], Loss: 3.7003, Perplexity: 40.46\n",
      "2019-11-26 01:22:21.053177 Epoch [10/50], Step[5000/8080], Loss: 4.3477, Perplexity: 77.30\n",
      "2019-11-26 01:29:11.866648 Epoch [10/50], Step[6000/8080], Loss: 4.0798, Perplexity: 59.14\n",
      "2019-11-26 01:36:01.192791 Epoch [10/50], Step[7000/8080], Loss: 3.6975, Perplexity: 40.35\n",
      "2019-11-26 01:42:52.032047 Epoch [10/50], Step[8000/8080], Loss: 4.0071, Perplexity: 54.99\n",
      "2019-11-26 01:43:25.629918 Epoch [11/50], Step[0/8080], Loss: 4.0007, Perplexity: 54.63\n",
      "2019-11-26 01:50:17.346645 Epoch [11/50], Step[1000/8080], Loss: 3.7799, Perplexity: 43.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 01:57:04.684106 Epoch [11/50], Step[2000/8080], Loss: 3.9504, Perplexity: 51.96\n",
      "2019-11-26 02:03:55.588541 Epoch [11/50], Step[3000/8080], Loss: 3.7801, Perplexity: 43.82\n",
      "2019-11-26 02:10:48.046512 Epoch [11/50], Step[4000/8080], Loss: 3.6888, Perplexity: 40.00\n",
      "2019-11-26 02:17:34.528396 Epoch [11/50], Step[5000/8080], Loss: 4.3289, Perplexity: 75.86\n",
      "2019-11-26 02:24:23.865908 Epoch [11/50], Step[6000/8080], Loss: 4.0757, Perplexity: 58.89\n",
      "2019-11-26 02:31:15.048665 Epoch [11/50], Step[7000/8080], Loss: 3.6867, Perplexity: 39.91\n",
      "2019-11-26 02:38:05.125886 Epoch [11/50], Step[8000/8080], Loss: 3.9961, Perplexity: 54.39\n",
      "2019-11-26 02:38:37.910790 Epoch [12/50], Step[0/8080], Loss: 3.9975, Perplexity: 54.46\n",
      "2019-11-26 02:45:30.387381 Epoch [12/50], Step[1000/8080], Loss: 3.7632, Perplexity: 43.09\n",
      "2019-11-26 02:52:17.886011 Epoch [12/50], Step[2000/8080], Loss: 3.9313, Perplexity: 50.98\n",
      "2019-11-26 02:59:09.571361 Epoch [12/50], Step[3000/8080], Loss: 3.7830, Perplexity: 43.95\n",
      "2019-11-26 03:05:59.278419 Epoch [12/50], Step[4000/8080], Loss: 3.6922, Perplexity: 40.13\n",
      "2019-11-26 03:12:52.331830 Epoch [12/50], Step[5000/8080], Loss: 4.3391, Perplexity: 76.64\n",
      "2019-11-26 03:19:45.570454 Epoch [12/50], Step[6000/8080], Loss: 4.0634, Perplexity: 58.17\n",
      "2019-11-26 03:26:32.342651 Epoch [12/50], Step[7000/8080], Loss: 3.6587, Perplexity: 38.81\n",
      "2019-11-26 03:33:21.102468 Epoch [12/50], Step[8000/8080], Loss: 3.9830, Perplexity: 53.68\n",
      "2019-11-26 03:33:53.798360 Epoch [13/50], Step[0/8080], Loss: 3.9999, Perplexity: 54.59\n",
      "2019-11-26 03:40:47.715092 Epoch [13/50], Step[1000/8080], Loss: 3.7408, Perplexity: 42.13\n",
      "2019-11-26 03:47:41.681597 Epoch [13/50], Step[2000/8080], Loss: 3.8965, Perplexity: 49.23\n",
      "2019-11-26 03:54:33.350808 Epoch [13/50], Step[3000/8080], Loss: 3.7583, Perplexity: 42.87\n",
      "2019-11-26 04:01:20.615422 Epoch [13/50], Step[4000/8080], Loss: 3.6688, Perplexity: 39.20\n",
      "2019-11-26 04:08:12.403679 Epoch [13/50], Step[5000/8080], Loss: 4.3290, Perplexity: 75.87\n",
      "2019-11-26 04:15:10.717875 Epoch [13/50], Step[6000/8080], Loss: 4.0563, Perplexity: 57.76\n",
      "2019-11-26 04:22:00.717528 Epoch [13/50], Step[7000/8080], Loss: 3.6454, Perplexity: 38.30\n",
      "2019-11-26 04:28:48.170778 Epoch [13/50], Step[8000/8080], Loss: 3.9724, Perplexity: 53.11\n",
      "2019-11-26 04:29:20.445408 Epoch [14/50], Step[0/8080], Loss: 3.9625, Perplexity: 52.59\n",
      "2019-11-26 04:36:13.947019 Epoch [14/50], Step[1000/8080], Loss: 3.7397, Perplexity: 42.08\n",
      "2019-11-26 04:43:05.486714 Epoch [14/50], Step[2000/8080], Loss: 3.9160, Perplexity: 50.20\n",
      "2019-11-26 04:49:57.273961 Epoch [14/50], Step[3000/8080], Loss: 3.7691, Perplexity: 43.34\n",
      "2019-11-26 04:56:48.270122 Epoch [14/50], Step[4000/8080], Loss: 3.6816, Perplexity: 39.71\n",
      "2019-11-26 05:03:34.587341 Epoch [14/50], Step[5000/8080], Loss: 4.2820, Perplexity: 72.38\n",
      "2019-11-26 05:10:24.023213 Epoch [14/50], Step[6000/8080], Loss: 4.0290, Perplexity: 56.21\n",
      "2019-11-26 05:17:13.070440 Epoch [14/50], Step[7000/8080], Loss: 3.6536, Perplexity: 38.61\n",
      "2019-11-26 05:24:04.236688 Epoch [14/50], Step[8000/8080], Loss: 3.9682, Perplexity: 52.89\n",
      "2019-11-26 05:24:36.708191 Epoch [15/50], Step[0/8080], Loss: 3.9633, Perplexity: 52.63\n",
      "2019-11-26 05:31:24.603133 Epoch [15/50], Step[1000/8080], Loss: 3.7220, Perplexity: 41.35\n",
      "2019-11-26 05:38:14.933321 Epoch [15/50], Step[2000/8080], Loss: 3.9069, Perplexity: 49.75\n",
      "2019-11-26 05:45:02.877086 Epoch [15/50], Step[3000/8080], Loss: 3.7741, Perplexity: 43.56\n",
      "2019-11-26 05:51:47.124624 Epoch [15/50], Step[4000/8080], Loss: 3.6817, Perplexity: 39.71\n",
      "2019-11-26 05:58:36.170211 Epoch [15/50], Step[5000/8080], Loss: 4.2977, Perplexity: 73.53\n",
      "2019-11-26 06:05:26.202267 Epoch [15/50], Step[6000/8080], Loss: 4.0355, Perplexity: 56.57\n",
      "2019-11-26 06:12:12.963341 Epoch [15/50], Step[7000/8080], Loss: 3.6422, Perplexity: 38.17\n",
      "2019-11-26 06:19:05.921040 Epoch [15/50], Step[8000/8080], Loss: 3.9584, Perplexity: 52.37\n",
      "2019-11-26 06:19:38.165318 Epoch [16/50], Step[0/8080], Loss: 3.9415, Perplexity: 51.50\n",
      "2019-11-26 06:26:27.171530 Epoch [16/50], Step[1000/8080], Loss: 3.7361, Perplexity: 41.94\n",
      "2019-11-26 06:33:16.673187 Epoch [16/50], Step[2000/8080], Loss: 3.9192, Perplexity: 50.36\n",
      "2019-11-26 06:40:05.967276 Epoch [16/50], Step[3000/8080], Loss: 3.7578, Perplexity: 42.86\n",
      "2019-11-26 06:46:59.267542 Epoch [16/50], Step[4000/8080], Loss: 3.6625, Perplexity: 38.96\n",
      "2019-11-26 06:53:49.071958 Epoch [16/50], Step[5000/8080], Loss: 4.3133, Perplexity: 74.69\n",
      "2019-11-26 07:00:37.475103 Epoch [16/50], Step[6000/8080], Loss: 4.0036, Perplexity: 54.80\n",
      "2019-11-26 07:07:25.691721 Epoch [16/50], Step[7000/8080], Loss: 3.6245, Perplexity: 37.50\n",
      "2019-11-26 07:14:14.986646 Epoch [16/50], Step[8000/8080], Loss: 3.9501, Perplexity: 51.94\n",
      "2019-11-26 07:14:49.114854 Epoch [17/50], Step[0/8080], Loss: 3.9406, Perplexity: 51.45\n",
      "2019-11-26 07:21:40.294329 Epoch [17/50], Step[1000/8080], Loss: 3.7389, Perplexity: 42.05\n",
      "2019-11-26 07:28:31.459395 Epoch [17/50], Step[2000/8080], Loss: 3.9138, Perplexity: 50.09\n",
      "2019-11-26 07:35:21.480744 Epoch [17/50], Step[3000/8080], Loss: 3.7796, Perplexity: 43.80\n",
      "2019-11-26 07:42:14.270135 Epoch [17/50], Step[4000/8080], Loss: 3.6787, Perplexity: 39.60\n",
      "2019-11-26 07:49:02.032725 Epoch [17/50], Step[5000/8080], Loss: 4.3054, Perplexity: 74.10\n",
      "2019-11-26 07:55:52.689082 Epoch [17/50], Step[6000/8080], Loss: 4.0097, Perplexity: 55.13\n",
      "2019-11-26 08:02:40.434990 Epoch [17/50], Step[7000/8080], Loss: 3.6133, Perplexity: 37.09\n",
      "2019-11-26 08:09:29.374751 Epoch [17/50], Step[8000/8080], Loss: 3.9504, Perplexity: 51.96\n",
      "2019-11-26 08:10:02.162552 Epoch [18/50], Step[0/8080], Loss: 3.9130, Perplexity: 50.05\n",
      "2019-11-26 08:16:53.952898 Epoch [18/50], Step[1000/8080], Loss: 3.7367, Perplexity: 41.96\n",
      "2019-11-26 08:23:44.159058 Epoch [18/50], Step[2000/8080], Loss: 3.9104, Perplexity: 49.92\n",
      "2019-11-26 08:30:33.257094 Epoch [18/50], Step[3000/8080], Loss: 3.7807, Perplexity: 43.85\n",
      "2019-11-26 08:37:23.122233 Epoch [18/50], Step[4000/8080], Loss: 3.6760, Perplexity: 39.49\n",
      "2019-11-26 08:44:10.067212 Epoch [18/50], Step[5000/8080], Loss: 4.2770, Perplexity: 72.02\n",
      "2019-11-26 08:51:02.246843 Epoch [18/50], Step[6000/8080], Loss: 4.0021, Perplexity: 54.71\n",
      "2019-11-26 08:57:56.067032 Epoch [18/50], Step[7000/8080], Loss: 3.6149, Perplexity: 37.15\n",
      "2019-11-26 09:04:47.171353 Epoch [18/50], Step[8000/8080], Loss: 3.9322, Perplexity: 51.02\n",
      "2019-11-26 09:05:20.552492 Epoch [19/50], Step[0/8080], Loss: 3.9369, Perplexity: 51.26\n",
      "2019-11-26 09:12:13.830525 Epoch [19/50], Step[1000/8080], Loss: 3.7054, Perplexity: 40.66\n",
      "2019-11-26 09:19:05.929302 Epoch [19/50], Step[2000/8080], Loss: 3.9198, Perplexity: 50.39\n",
      "2019-11-26 09:25:57.432568 Epoch [19/50], Step[3000/8080], Loss: 3.7564, Perplexity: 42.80\n",
      "2019-11-26 09:32:45.382367 Epoch [19/50], Step[4000/8080], Loss: 3.7038, Perplexity: 40.60\n",
      "2019-11-26 09:39:37.102949 Epoch [19/50], Step[5000/8080], Loss: 4.2822, Perplexity: 72.40\n",
      "2019-11-26 09:46:28.677017 Epoch [19/50], Step[6000/8080], Loss: 4.0034, Perplexity: 54.78\n",
      "2019-11-26 09:53:21.848743 Epoch [19/50], Step[7000/8080], Loss: 3.6075, Perplexity: 36.87\n",
      "2019-11-26 10:00:13.278457 Epoch [19/50], Step[8000/8080], Loss: 3.9535, Perplexity: 52.12\n",
      "2019-11-26 10:00:47.414227 Epoch [20/50], Step[0/8080], Loss: 3.9203, Perplexity: 50.41\n",
      "2019-11-26 10:07:36.803507 Epoch [20/50], Step[1000/8080], Loss: 3.7237, Perplexity: 41.42\n",
      "2019-11-26 10:14:25.831918 Epoch [20/50], Step[2000/8080], Loss: 3.9153, Perplexity: 50.16\n",
      "2019-11-26 10:21:16.636848 Epoch [20/50], Step[3000/8080], Loss: 3.7643, Perplexity: 43.13\n",
      "2019-11-26 10:28:09.591682 Epoch [20/50], Step[4000/8080], Loss: 3.6699, Perplexity: 39.25\n",
      "2019-11-26 10:34:53.163930 Epoch [20/50], Step[5000/8080], Loss: 4.2950, Perplexity: 73.33\n",
      "2019-11-26 10:41:37.853604 Epoch [20/50], Step[6000/8080], Loss: 4.0067, Perplexity: 54.96\n",
      "2019-11-26 10:48:30.185013 Epoch [20/50], Step[7000/8080], Loss: 3.6061, Perplexity: 36.82\n",
      "2019-11-26 10:55:19.645791 Epoch [20/50], Step[8000/8080], Loss: 3.9370, Perplexity: 51.26\n",
      "2019-11-26 10:55:51.706444 Epoch [21/50], Step[0/8080], Loss: 3.9030, Perplexity: 49.55\n",
      "2019-11-26 11:02:44.513773 Epoch [21/50], Step[1000/8080], Loss: 3.7010, Perplexity: 40.49\n",
      "2019-11-26 11:09:37.247821 Epoch [21/50], Step[2000/8080], Loss: 3.8953, Perplexity: 49.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 11:16:24.594493 Epoch [21/50], Step[3000/8080], Loss: 3.7555, Perplexity: 42.75\n",
      "2019-11-26 11:23:15.306751 Epoch [21/50], Step[4000/8080], Loss: 3.7017, Perplexity: 40.52\n",
      "2019-11-26 11:30:10.400361 Epoch [21/50], Step[5000/8080], Loss: 4.3047, Perplexity: 74.05\n",
      "2019-11-26 11:37:04.232856 Epoch [21/50], Step[6000/8080], Loss: 3.9889, Perplexity: 54.00\n",
      "2019-11-26 11:43:55.435974 Epoch [21/50], Step[7000/8080], Loss: 3.6042, Perplexity: 36.75\n",
      "2019-11-26 11:50:45.800613 Epoch [21/50], Step[8000/8080], Loss: 3.9447, Perplexity: 51.66\n",
      "2019-11-26 11:51:18.129917 Epoch [22/50], Step[0/8080], Loss: 3.8864, Perplexity: 48.74\n",
      "2019-11-26 11:58:07.477336 Epoch [22/50], Step[1000/8080], Loss: 3.7012, Perplexity: 40.50\n",
      "2019-11-26 12:04:59.423795 Epoch [22/50], Step[2000/8080], Loss: 3.8749, Perplexity: 48.18\n",
      "2019-11-26 12:11:48.501669 Epoch [22/50], Step[3000/8080], Loss: 3.7330, Perplexity: 41.81\n",
      "2019-11-26 12:18:36.148179 Epoch [22/50], Step[4000/8080], Loss: 3.6774, Perplexity: 39.54\n",
      "2019-11-26 12:25:28.781077 Epoch [22/50], Step[5000/8080], Loss: 4.2713, Perplexity: 71.61\n",
      "2019-11-26 12:32:22.360966 Epoch [22/50], Step[6000/8080], Loss: 3.9766, Perplexity: 53.34\n",
      "2019-11-26 12:39:14.601667 Epoch [22/50], Step[7000/8080], Loss: 3.5886, Perplexity: 36.18\n",
      "2019-11-26 12:46:05.963213 Epoch [22/50], Step[8000/8080], Loss: 3.9249, Perplexity: 50.65\n",
      "2019-11-26 12:46:38.831700 Epoch [23/50], Step[0/8080], Loss: 3.8943, Perplexity: 49.12\n",
      "2019-11-26 12:53:31.790369 Epoch [23/50], Step[1000/8080], Loss: 3.7137, Perplexity: 41.01\n",
      "2019-11-26 13:00:26.046907 Epoch [23/50], Step[2000/8080], Loss: 3.8780, Perplexity: 48.33\n",
      "2019-11-26 13:07:15.454211 Epoch [23/50], Step[3000/8080], Loss: 3.7233, Perplexity: 41.40\n",
      "2019-11-26 13:14:04.116381 Epoch [23/50], Step[4000/8080], Loss: 3.6827, Perplexity: 39.75\n",
      "2019-11-26 13:20:53.857655 Epoch [23/50], Step[5000/8080], Loss: 4.2758, Perplexity: 71.94\n",
      "2019-11-26 13:27:41.566318 Epoch [23/50], Step[6000/8080], Loss: 3.9820, Perplexity: 53.62\n",
      "2019-11-26 13:34:38.469531 Epoch [23/50], Step[7000/8080], Loss: 3.6016, Perplexity: 36.66\n",
      "2019-11-26 13:41:31.612065 Epoch [23/50], Step[8000/8080], Loss: 3.9230, Perplexity: 50.55\n",
      "2019-11-26 13:42:05.246454 Epoch [24/50], Step[0/8080], Loss: 3.8929, Perplexity: 49.05\n",
      "2019-11-26 13:48:58.825876 Epoch [24/50], Step[1000/8080], Loss: 3.7116, Perplexity: 40.92\n",
      "2019-11-26 13:55:46.857147 Epoch [24/50], Step[2000/8080], Loss: 3.8762, Perplexity: 48.24\n",
      "2019-11-26 14:02:37.293527 Epoch [24/50], Step[3000/8080], Loss: 3.7280, Perplexity: 41.59\n",
      "2019-11-26 14:09:23.391579 Epoch [24/50], Step[4000/8080], Loss: 3.6979, Perplexity: 40.36\n",
      "2019-11-26 14:16:10.688735 Epoch [24/50], Step[5000/8080], Loss: 4.2679, Perplexity: 71.38\n",
      "2019-11-26 14:23:03.742689 Epoch [24/50], Step[6000/8080], Loss: 3.9738, Perplexity: 53.19\n",
      "2019-11-26 14:29:55.174608 Epoch [24/50], Step[7000/8080], Loss: 3.5981, Perplexity: 36.53\n",
      "2019-11-26 14:36:48.448645 Epoch [24/50], Step[8000/8080], Loss: 3.9159, Perplexity: 50.19\n",
      "2019-11-26 14:37:21.701527 Epoch [25/50], Step[0/8080], Loss: 3.8712, Perplexity: 48.00\n",
      "2019-11-26 14:44:16.167226 Epoch [25/50], Step[1000/8080], Loss: 3.6897, Perplexity: 40.03\n",
      "2019-11-26 14:51:07.215549 Epoch [25/50], Step[2000/8080], Loss: 3.8943, Perplexity: 49.12\n",
      "2019-11-26 14:57:58.509581 Epoch [25/50], Step[3000/8080], Loss: 3.7215, Perplexity: 41.33\n",
      "2019-11-26 15:04:54.016339 Epoch [25/50], Step[4000/8080], Loss: 3.6675, Perplexity: 39.15\n",
      "2019-11-26 15:11:47.252230 Epoch [25/50], Step[5000/8080], Loss: 4.2661, Perplexity: 71.25\n",
      "2019-11-26 15:18:45.220847 Epoch [25/50], Step[6000/8080], Loss: 3.9786, Perplexity: 53.44\n",
      "2019-11-26 15:25:08.577551 Epoch [25/50], Step[7000/8080], Loss: 3.5860, Perplexity: 36.09\n",
      "2019-11-26 15:31:29.887588 Epoch [25/50], Step[8000/8080], Loss: 3.9049, Perplexity: 49.64\n",
      "2019-11-26 15:32:00.090572 Epoch [26/50], Step[0/8080], Loss: 3.8624, Perplexity: 47.58\n",
      "2019-11-26 15:38:11.983873 Epoch [26/50], Step[1000/8080], Loss: 3.6906, Perplexity: 40.07\n",
      "2019-11-26 15:44:31.532574 Epoch [26/50], Step[2000/8080], Loss: 3.8581, Perplexity: 47.38\n",
      "2019-11-26 15:50:56.000444 Epoch [26/50], Step[3000/8080], Loss: 3.7285, Perplexity: 41.62\n",
      "2019-11-26 15:57:16.302893 Epoch [26/50], Step[4000/8080], Loss: 3.6717, Perplexity: 39.32\n",
      "2019-11-26 16:03:38.557338 Epoch [26/50], Step[5000/8080], Loss: 4.2591, Perplexity: 70.75\n",
      "2019-11-26 16:09:56.206280 Epoch [26/50], Step[6000/8080], Loss: 3.9669, Perplexity: 52.82\n",
      "2019-11-26 16:16:18.648231 Epoch [26/50], Step[7000/8080], Loss: 3.5761, Perplexity: 35.74\n",
      "2019-11-26 16:22:41.173845 Epoch [26/50], Step[8000/8080], Loss: 3.8945, Perplexity: 49.13\n",
      "2019-11-26 16:23:11.177956 Epoch [27/50], Step[0/8080], Loss: 3.8517, Perplexity: 47.07\n",
      "2019-11-26 16:29:35.451146 Epoch [27/50], Step[1000/8080], Loss: 3.6717, Perplexity: 39.32\n",
      "2019-11-26 16:36:24.938091 Epoch [27/50], Step[2000/8080], Loss: 3.8728, Perplexity: 48.08\n",
      "2019-11-26 16:43:26.663547 Epoch [27/50], Step[3000/8080], Loss: 3.7182, Perplexity: 41.19\n",
      "2019-11-26 16:50:32.695984 Epoch [27/50], Step[4000/8080], Loss: 3.6577, Perplexity: 38.77\n",
      "2019-11-26 16:57:28.328291 Epoch [27/50], Step[5000/8080], Loss: 4.2716, Perplexity: 71.63\n",
      "2019-11-26 17:04:42.108645 Epoch [27/50], Step[6000/8080], Loss: 3.9810, Perplexity: 53.57\n",
      "2019-11-26 17:12:11.914560 Epoch [27/50], Step[7000/8080], Loss: 3.5794, Perplexity: 35.85\n",
      "2019-11-26 17:19:32.156414 Epoch [27/50], Step[8000/8080], Loss: 3.9093, Perplexity: 49.86\n",
      "2019-11-26 17:20:08.965359 Epoch [28/50], Step[0/8080], Loss: 3.8442, Perplexity: 46.72\n",
      "2019-11-26 17:27:32.346414 Epoch [28/50], Step[1000/8080], Loss: 3.6662, Perplexity: 39.10\n",
      "2019-11-26 17:34:57.521032 Epoch [28/50], Step[2000/8080], Loss: 3.8726, Perplexity: 48.07\n",
      "2019-11-26 17:42:15.758907 Epoch [28/50], Step[3000/8080], Loss: 3.7352, Perplexity: 41.90\n",
      "2019-11-26 17:49:17.996377 Epoch [28/50], Step[4000/8080], Loss: 3.6497, Perplexity: 38.46\n",
      "2019-11-26 17:56:31.272561 Epoch [28/50], Step[5000/8080], Loss: 4.2742, Perplexity: 71.82\n",
      "2019-11-26 18:03:49.988105 Epoch [28/50], Step[6000/8080], Loss: 3.9690, Perplexity: 52.93\n",
      "2019-11-26 18:10:54.011117 Epoch [28/50], Step[7000/8080], Loss: 3.5557, Perplexity: 35.01\n",
      "2019-11-26 18:18:06.258696 Epoch [28/50], Step[8000/8080], Loss: 3.8958, Perplexity: 49.19\n",
      "2019-11-26 18:18:42.900227 Epoch [29/50], Step[0/8080], Loss: 3.8455, Perplexity: 46.78\n",
      "2019-11-26 18:26:17.186149 Epoch [29/50], Step[1000/8080], Loss: 3.7000, Perplexity: 40.45\n",
      "2019-11-26 18:33:46.043746 Epoch [29/50], Step[2000/8080], Loss: 3.8537, Perplexity: 47.17\n",
      "2019-11-26 18:41:16.316354 Epoch [29/50], Step[3000/8080], Loss: 3.7243, Perplexity: 41.44\n",
      "2019-11-26 18:48:43.377109 Epoch [29/50], Step[4000/8080], Loss: 3.6650, Perplexity: 39.06\n",
      "2019-11-26 18:56:22.622419 Epoch [29/50], Step[5000/8080], Loss: 4.2991, Perplexity: 73.63\n",
      "2019-11-26 19:03:59.088823 Epoch [29/50], Step[6000/8080], Loss: 3.9404, Perplexity: 51.44\n",
      "2019-11-26 19:11:09.893938 Epoch [29/50], Step[7000/8080], Loss: 3.5734, Perplexity: 35.64\n",
      "2019-11-26 19:18:04.094158 Epoch [29/50], Step[8000/8080], Loss: 3.8771, Perplexity: 48.28\n",
      "2019-11-26 19:18:38.262492 Epoch [30/50], Step[0/8080], Loss: 3.8511, Perplexity: 47.04\n",
      "2019-11-26 19:25:28.871389 Epoch [30/50], Step[1000/8080], Loss: 3.6681, Perplexity: 39.18\n",
      "2019-11-26 19:32:28.539073 Epoch [30/50], Step[2000/8080], Loss: 3.8243, Perplexity: 45.80\n",
      "2019-11-26 19:39:30.294604 Epoch [30/50], Step[3000/8080], Loss: 3.7436, Perplexity: 42.25\n",
      "2019-11-26 19:46:30.187434 Epoch [30/50], Step[4000/8080], Loss: 3.6471, Perplexity: 38.36\n",
      "2019-11-26 19:53:28.177531 Epoch [30/50], Step[5000/8080], Loss: 4.2764, Perplexity: 71.98\n",
      "2019-11-26 20:00:27.509891 Epoch [30/50], Step[6000/8080], Loss: 3.9310, Perplexity: 50.96\n",
      "2019-11-26 20:07:28.890470 Epoch [30/50], Step[7000/8080], Loss: 3.5688, Perplexity: 35.48\n",
      "2019-11-26 20:14:28.437153 Epoch [30/50], Step[8000/8080], Loss: 3.9049, Perplexity: 49.64\n",
      "2019-11-26 20:15:01.666395 Epoch [31/50], Step[0/8080], Loss: 3.8449, Perplexity: 46.75\n",
      "2019-11-26 20:22:00.633099 Epoch [31/50], Step[1000/8080], Loss: 3.6637, Perplexity: 39.00\n",
      "2019-11-26 20:29:00.844469 Epoch [31/50], Step[2000/8080], Loss: 3.8567, Perplexity: 47.31\n",
      "2019-11-26 20:35:57.434423 Epoch [31/50], Step[3000/8080], Loss: 3.7330, Perplexity: 41.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 20:42:59.989336 Epoch [31/50], Step[4000/8080], Loss: 3.6531, Perplexity: 38.59\n",
      "2019-11-26 20:50:00.214771 Epoch [31/50], Step[5000/8080], Loss: 4.2701, Perplexity: 71.53\n",
      "2019-11-26 20:56:56.937645 Epoch [31/50], Step[6000/8080], Loss: 3.9853, Perplexity: 53.80\n",
      "2019-11-26 21:03:50.743179 Epoch [31/50], Step[7000/8080], Loss: 3.5733, Perplexity: 35.63\n",
      "2019-11-26 21:10:49.224484 Epoch [31/50], Step[8000/8080], Loss: 3.8911, Perplexity: 48.96\n",
      "2019-11-26 21:11:21.845656 Epoch [32/50], Step[0/8080], Loss: 3.8752, Perplexity: 48.19\n",
      "2019-11-26 21:18:20.197078 Epoch [32/50], Step[1000/8080], Loss: 3.6515, Perplexity: 38.53\n",
      "2019-11-26 21:25:15.311932 Epoch [32/50], Step[2000/8080], Loss: 3.8456, Perplexity: 46.79\n",
      "2019-11-26 21:32:16.510291 Epoch [32/50], Step[3000/8080], Loss: 3.7303, Perplexity: 41.69\n",
      "2019-11-26 21:39:10.745286 Epoch [32/50], Step[4000/8080], Loss: 3.6540, Perplexity: 38.63\n",
      "2019-11-26 21:46:09.731599 Epoch [32/50], Step[5000/8080], Loss: 4.2921, Perplexity: 73.12\n",
      "2019-11-26 21:53:03.685989 Epoch [32/50], Step[6000/8080], Loss: 3.9331, Perplexity: 51.07\n",
      "2019-11-26 21:59:58.412869 Epoch [32/50], Step[7000/8080], Loss: 3.5858, Perplexity: 36.08\n",
      "2019-11-26 22:06:57.969639 Epoch [32/50], Step[8000/8080], Loss: 3.8717, Perplexity: 48.02\n",
      "2019-11-26 22:07:30.841420 Epoch [33/50], Step[0/8080], Loss: 3.8511, Perplexity: 47.04\n",
      "2019-11-26 22:14:30.973122 Epoch [33/50], Step[1000/8080], Loss: 3.6643, Perplexity: 39.03\n",
      "2019-11-26 22:21:24.012505 Epoch [33/50], Step[2000/8080], Loss: 3.8439, Perplexity: 46.71\n",
      "2019-11-26 22:28:26.781632 Epoch [33/50], Step[3000/8080], Loss: 3.7422, Perplexity: 42.19\n",
      "2019-11-26 22:35:24.352929 Epoch [33/50], Step[4000/8080], Loss: 3.6301, Perplexity: 37.72\n",
      "2019-11-26 22:42:21.644623 Epoch [33/50], Step[5000/8080], Loss: 4.2780, Perplexity: 72.10\n",
      "2019-11-26 22:49:20.614433 Epoch [33/50], Step[6000/8080], Loss: 3.9567, Perplexity: 52.28\n",
      "2019-11-26 22:56:19.015039 Epoch [33/50], Step[7000/8080], Loss: 3.5519, Perplexity: 34.88\n",
      "2019-11-26 23:03:14.506124 Epoch [33/50], Step[8000/8080], Loss: 3.8841, Perplexity: 48.62\n",
      "2019-11-26 23:03:48.279494 Epoch [34/50], Step[0/8080], Loss: 3.8542, Perplexity: 47.19\n",
      "2019-11-26 23:10:56.125799 Epoch [34/50], Step[1000/8080], Loss: 3.6552, Perplexity: 38.68\n",
      "2019-11-26 23:17:59.243757 Epoch [34/50], Step[2000/8080], Loss: 3.8447, Perplexity: 46.75\n",
      "2019-11-26 23:25:02.782113 Epoch [34/50], Step[3000/8080], Loss: 3.7426, Perplexity: 42.21\n",
      "2019-11-26 23:32:04.174063 Epoch [34/50], Step[4000/8080], Loss: 3.6139, Perplexity: 37.11\n",
      "2019-11-26 23:39:00.693416 Epoch [34/50], Step[5000/8080], Loss: 4.2625, Perplexity: 70.99\n",
      "2019-11-26 23:46:03.811877 Epoch [34/50], Step[6000/8080], Loss: 3.9630, Perplexity: 52.61\n",
      "2019-11-26 23:53:00.510514 Epoch [34/50], Step[7000/8080], Loss: 3.5609, Perplexity: 35.19\n",
      "2019-11-26 23:59:58.204480 Epoch [34/50], Step[8000/8080], Loss: 3.8814, Perplexity: 48.49\n",
      "2019-11-27 00:00:31.812246 Epoch [35/50], Step[0/8080], Loss: 3.8590, Perplexity: 47.42\n",
      "2019-11-27 00:07:27.525825 Epoch [35/50], Step[1000/8080], Loss: 3.6588, Perplexity: 38.81\n",
      "2019-11-27 00:14:29.083852 Epoch [35/50], Step[2000/8080], Loss: 3.8459, Perplexity: 46.80\n",
      "2019-11-27 00:21:30.391409 Epoch [35/50], Step[3000/8080], Loss: 3.7538, Perplexity: 42.68\n",
      "2019-11-27 00:28:27.796031 Epoch [35/50], Step[4000/8080], Loss: 3.6268, Perplexity: 37.59\n",
      "2019-11-27 00:35:24.963073 Epoch [35/50], Step[5000/8080], Loss: 4.2760, Perplexity: 71.95\n",
      "2019-11-27 00:42:20.046408 Epoch [35/50], Step[6000/8080], Loss: 3.9644, Perplexity: 52.69\n",
      "2019-11-27 00:49:18.345340 Epoch [35/50], Step[7000/8080], Loss: 3.5624, Perplexity: 35.25\n",
      "2019-11-27 00:56:13.050157 Epoch [35/50], Step[8000/8080], Loss: 3.8688, Perplexity: 47.89\n",
      "2019-11-27 00:56:46.826176 Epoch [36/50], Step[0/8080], Loss: 3.8639, Perplexity: 47.65\n",
      "2019-11-27 01:03:46.073858 Epoch [36/50], Step[1000/8080], Loss: 3.6553, Perplexity: 38.68\n",
      "2019-11-27 01:10:48.728351 Epoch [36/50], Step[2000/8080], Loss: 3.8448, Perplexity: 46.75\n",
      "2019-11-27 01:17:45.078333 Epoch [36/50], Step[3000/8080], Loss: 3.7327, Perplexity: 41.79\n",
      "2019-11-27 01:24:43.659834 Epoch [36/50], Step[4000/8080], Loss: 3.5953, Perplexity: 36.43\n",
      "2019-11-27 01:31:35.397899 Epoch [36/50], Step[5000/8080], Loss: 4.2418, Perplexity: 69.53\n",
      "2019-11-27 01:38:35.192896 Epoch [36/50], Step[6000/8080], Loss: 3.9706, Perplexity: 53.02\n",
      "2019-11-27 01:45:28.627006 Epoch [36/50], Step[7000/8080], Loss: 3.5491, Perplexity: 34.78\n",
      "2019-11-27 01:52:27.200510 Epoch [36/50], Step[8000/8080], Loss: 3.8754, Perplexity: 48.20\n",
      "2019-11-27 01:53:01.815389 Epoch [37/50], Step[0/8080], Loss: 3.8651, Perplexity: 47.71\n",
      "2019-11-27 01:59:57.578583 Epoch [37/50], Step[1000/8080], Loss: 3.6562, Perplexity: 38.71\n",
      "2019-11-27 02:06:58.825885 Epoch [37/50], Step[2000/8080], Loss: 3.8450, Perplexity: 46.76\n",
      "2019-11-27 02:13:57.128893 Epoch [37/50], Step[3000/8080], Loss: 3.7304, Perplexity: 41.69\n",
      "2019-11-27 02:20:58.100540 Epoch [37/50], Step[4000/8080], Loss: 3.6258, Perplexity: 37.55\n",
      "2019-11-27 02:27:59.328440 Epoch [37/50], Step[5000/8080], Loss: 4.2674, Perplexity: 71.34\n",
      "2019-11-27 02:34:58.048962 Epoch [37/50], Step[6000/8080], Loss: 3.9551, Perplexity: 52.20\n",
      "2019-11-27 02:41:58.634617 Epoch [37/50], Step[7000/8080], Loss: 3.5626, Perplexity: 35.26\n",
      "2019-11-27 02:48:54.637598 Epoch [37/50], Step[8000/8080], Loss: 3.8799, Perplexity: 48.42\n",
      "2019-11-27 02:49:28.616293 Epoch [38/50], Step[0/8080], Loss: 3.8516, Perplexity: 47.07\n",
      "2019-11-27 02:56:25.371193 Epoch [38/50], Step[1000/8080], Loss: 3.6535, Perplexity: 38.61\n",
      "2019-11-27 03:03:23.126581 Epoch [38/50], Step[2000/8080], Loss: 3.8376, Perplexity: 46.41\n",
      "2019-11-27 03:10:22.002542 Epoch [38/50], Step[3000/8080], Loss: 3.7361, Perplexity: 41.93\n",
      "2019-11-27 03:17:19.850669 Epoch [38/50], Step[4000/8080], Loss: 3.5813, Perplexity: 35.92\n",
      "2019-11-27 03:24:17.777358 Epoch [38/50], Step[5000/8080], Loss: 4.2539, Perplexity: 70.38\n",
      "2019-11-27 03:31:11.987295 Epoch [38/50], Step[6000/8080], Loss: 3.9496, Perplexity: 51.92\n",
      "2019-11-27 03:38:07.945304 Epoch [38/50], Step[7000/8080], Loss: 3.5642, Perplexity: 35.31\n",
      "2019-11-27 03:45:02.653502 Epoch [38/50], Step[8000/8080], Loss: 3.8953, Perplexity: 49.17\n",
      "2019-11-27 03:45:36.458682 Epoch [39/50], Step[0/8080], Loss: 3.8914, Perplexity: 48.98\n",
      "2019-11-27 03:52:36.981852 Epoch [39/50], Step[1000/8080], Loss: 3.6726, Perplexity: 39.36\n",
      "2019-11-27 03:59:37.841731 Epoch [39/50], Step[2000/8080], Loss: 3.8487, Perplexity: 46.93\n",
      "2019-11-27 04:06:36.041720 Epoch [39/50], Step[3000/8080], Loss: 3.7246, Perplexity: 41.45\n",
      "2019-11-27 04:13:38.168000 Epoch [39/50], Step[4000/8080], Loss: 3.6502, Perplexity: 38.48\n",
      "2019-11-27 04:20:38.313190 Epoch [39/50], Step[5000/8080], Loss: 4.2339, Perplexity: 68.99\n",
      "2019-11-27 04:27:34.236856 Epoch [39/50], Step[6000/8080], Loss: 3.9552, Perplexity: 52.21\n",
      "2019-11-27 04:34:32.450311 Epoch [39/50], Step[7000/8080], Loss: 3.5677, Perplexity: 35.44\n",
      "2019-11-27 04:41:35.290832 Epoch [39/50], Step[8000/8080], Loss: 3.8695, Perplexity: 47.92\n",
      "2019-11-27 04:42:07.886607 Epoch [40/50], Step[0/8080], Loss: 3.8579, Perplexity: 47.37\n",
      "2019-11-27 04:49:09.214393 Epoch [40/50], Step[1000/8080], Loss: 3.6558, Perplexity: 38.70\n",
      "2019-11-27 04:56:10.047650 Epoch [40/50], Step[2000/8080], Loss: 3.8243, Perplexity: 45.80\n",
      "2019-11-27 05:03:11.056065 Epoch [40/50], Step[3000/8080], Loss: 3.7043, Perplexity: 40.62\n",
      "2019-11-27 05:10:10.523838 Epoch [40/50], Step[4000/8080], Loss: 3.6284, Perplexity: 37.65\n",
      "2019-11-27 05:16:58.934934 Epoch [40/50], Step[5000/8080], Loss: 4.2818, Perplexity: 72.37\n",
      "2019-11-27 05:23:43.332617 Epoch [40/50], Step[6000/8080], Loss: 3.9745, Perplexity: 53.22\n",
      "2019-11-27 05:30:32.867581 Epoch [40/50], Step[7000/8080], Loss: 3.5671, Perplexity: 35.41\n",
      "2019-11-27 05:37:20.409669 Epoch [40/50], Step[8000/8080], Loss: 3.8743, Perplexity: 48.15\n",
      "2019-11-27 05:37:53.406236 Epoch [41/50], Step[0/8080], Loss: 3.8451, Perplexity: 46.76\n",
      "2019-11-27 05:44:42.295315 Epoch [41/50], Step[1000/8080], Loss: 3.6712, Perplexity: 39.30\n",
      "2019-11-27 05:51:28.164154 Epoch [41/50], Step[2000/8080], Loss: 3.8257, Perplexity: 45.86\n",
      "2019-11-27 05:58:15.195261 Epoch [41/50], Step[3000/8080], Loss: 3.7333, Perplexity: 41.82\n",
      "2019-11-27 06:05:04.663318 Epoch [41/50], Step[4000/8080], Loss: 3.6201, Perplexity: 37.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-27 06:11:48.602098 Epoch [41/50], Step[5000/8080], Loss: 4.2600, Perplexity: 70.81\n",
      "2019-11-27 06:18:38.005206 Epoch [41/50], Step[6000/8080], Loss: 4.0064, Perplexity: 54.95\n",
      "2019-11-27 06:25:23.578977 Epoch [41/50], Step[7000/8080], Loss: 3.5278, Perplexity: 34.05\n",
      "2019-11-27 06:32:09.730196 Epoch [41/50], Step[8000/8080], Loss: 3.8940, Perplexity: 49.11\n",
      "2019-11-27 06:32:43.504074 Epoch [42/50], Step[0/8080], Loss: 3.8349, Perplexity: 46.29\n",
      "2019-11-27 06:39:27.187743 Epoch [42/50], Step[1000/8080], Loss: 3.6717, Perplexity: 39.32\n",
      "2019-11-27 06:46:14.123452 Epoch [42/50], Step[2000/8080], Loss: 3.8392, Perplexity: 46.49\n",
      "2019-11-27 06:53:01.551189 Epoch [42/50], Step[3000/8080], Loss: 3.7238, Perplexity: 41.42\n",
      "2019-11-27 06:59:46.985931 Epoch [42/50], Step[4000/8080], Loss: 3.6165, Perplexity: 37.21\n",
      "2019-11-27 07:06:34.277138 Epoch [42/50], Step[5000/8080], Loss: 4.2637, Perplexity: 71.07\n",
      "2019-11-27 07:13:21.898472 Epoch [42/50], Step[6000/8080], Loss: 3.9841, Perplexity: 53.74\n",
      "2019-11-27 07:20:16.446592 Epoch [42/50], Step[7000/8080], Loss: 3.5630, Perplexity: 35.27\n",
      "2019-11-27 07:27:03.663977 Epoch [42/50], Step[8000/8080], Loss: 3.8741, Perplexity: 48.14\n",
      "2019-11-27 07:27:36.065649 Epoch [43/50], Step[0/8080], Loss: 3.8205, Perplexity: 45.63\n",
      "2019-11-27 07:34:16.947356 Epoch [43/50], Step[1000/8080], Loss: 3.6246, Perplexity: 37.51\n",
      "2019-11-27 07:41:03.650243 Epoch [43/50], Step[2000/8080], Loss: 3.8429, Perplexity: 46.66\n",
      "2019-11-27 07:47:53.976137 Epoch [43/50], Step[3000/8080], Loss: 3.7260, Perplexity: 41.51\n",
      "2019-11-27 07:54:39.368114 Epoch [43/50], Step[4000/8080], Loss: 3.6094, Perplexity: 36.94\n",
      "2019-11-27 08:01:27.434787 Epoch [43/50], Step[5000/8080], Loss: 4.2575, Perplexity: 70.64\n",
      "2019-11-27 08:08:12.336979 Epoch [43/50], Step[6000/8080], Loss: 3.9709, Perplexity: 53.03\n",
      "2019-11-27 08:14:56.774541 Epoch [43/50], Step[7000/8080], Loss: 3.5565, Perplexity: 35.04\n",
      "2019-11-27 08:21:43.667177 Epoch [43/50], Step[8000/8080], Loss: 3.8521, Perplexity: 47.09\n",
      "2019-11-27 08:22:15.752933 Epoch [44/50], Step[0/8080], Loss: 3.8477, Perplexity: 46.89\n",
      "2019-11-27 08:28:59.892542 Epoch [44/50], Step[1000/8080], Loss: 3.6288, Perplexity: 37.67\n",
      "2019-11-27 08:35:43.845399 Epoch [44/50], Step[2000/8080], Loss: 3.8293, Perplexity: 46.03\n",
      "2019-11-27 08:42:31.659725 Epoch [44/50], Step[3000/8080], Loss: 3.7236, Perplexity: 41.41\n",
      "2019-11-27 08:49:18.350868 Epoch [44/50], Step[4000/8080], Loss: 3.6411, Perplexity: 38.14\n",
      "2019-11-27 08:56:10.430403 Epoch [44/50], Step[5000/8080], Loss: 4.2749, Perplexity: 71.87\n",
      "2019-11-27 09:03:01.515737 Epoch [44/50], Step[6000/8080], Loss: 3.9477, Perplexity: 51.82\n",
      "2019-11-27 09:09:48.131770 Epoch [44/50], Step[7000/8080], Loss: 3.5454, Perplexity: 34.65\n",
      "2019-11-27 09:16:38.941951 Epoch [44/50], Step[8000/8080], Loss: 3.8711, Perplexity: 48.00\n",
      "2019-11-27 09:17:11.187457 Epoch [45/50], Step[0/8080], Loss: 3.8395, Perplexity: 46.50\n",
      "2019-11-27 09:23:58.586081 Epoch [45/50], Step[1000/8080], Loss: 3.6467, Perplexity: 38.35\n",
      "2019-11-27 09:30:40.860406 Epoch [45/50], Step[2000/8080], Loss: 3.8246, Perplexity: 45.82\n",
      "2019-11-27 09:37:27.980715 Epoch [45/50], Step[3000/8080], Loss: 3.7385, Perplexity: 42.04\n",
      "2019-11-27 09:44:15.378354 Epoch [45/50], Step[4000/8080], Loss: 3.5981, Perplexity: 36.53\n",
      "2019-11-27 09:51:02.917397 Epoch [45/50], Step[5000/8080], Loss: 4.2797, Perplexity: 72.22\n",
      "2019-11-27 09:57:48.424191 Epoch [45/50], Step[6000/8080], Loss: 3.9597, Perplexity: 52.44\n",
      "2019-11-27 10:04:33.763318 Epoch [45/50], Step[7000/8080], Loss: 3.5593, Perplexity: 35.14\n",
      "2019-11-27 10:11:19.689109 Epoch [45/50], Step[8000/8080], Loss: 3.8620, Perplexity: 47.56\n",
      "2019-11-27 10:11:52.465775 Epoch [46/50], Step[0/8080], Loss: 3.8217, Perplexity: 45.68\n",
      "2019-11-27 10:18:39.778819 Epoch [46/50], Step[1000/8080], Loss: 3.6459, Perplexity: 38.32\n",
      "2019-11-27 10:25:25.886188 Epoch [46/50], Step[2000/8080], Loss: 3.8028, Perplexity: 44.82\n",
      "2019-11-27 10:32:09.277320 Epoch [46/50], Step[3000/8080], Loss: 3.7349, Perplexity: 41.88\n",
      "2019-11-27 10:38:55.278721 Epoch [46/50], Step[4000/8080], Loss: 3.6198, Perplexity: 37.33\n",
      "2019-11-27 10:45:43.319431 Epoch [46/50], Step[5000/8080], Loss: 4.2551, Perplexity: 70.46\n",
      "2019-11-27 10:52:31.946503 Epoch [46/50], Step[6000/8080], Loss: 3.9603, Perplexity: 52.47\n",
      "2019-11-27 10:59:20.054731 Epoch [46/50], Step[7000/8080], Loss: 3.5691, Perplexity: 35.49\n",
      "2019-11-27 11:06:06.458735 Epoch [46/50], Step[8000/8080], Loss: 3.8655, Perplexity: 47.73\n",
      "2019-11-27 11:06:39.257906 Epoch [47/50], Step[0/8080], Loss: 3.8419, Perplexity: 46.61\n",
      "2019-11-27 11:13:26.975429 Epoch [47/50], Step[1000/8080], Loss: 3.6455, Perplexity: 38.30\n",
      "2019-11-27 11:20:14.767661 Epoch [47/50], Step[2000/8080], Loss: 3.8090, Perplexity: 45.11\n",
      "2019-11-27 11:27:04.073884 Epoch [47/50], Step[3000/8080], Loss: 3.7229, Perplexity: 41.39\n",
      "2019-11-27 11:33:50.330410 Epoch [47/50], Step[4000/8080], Loss: 3.5820, Perplexity: 35.94\n",
      "2019-11-27 11:40:33.372966 Epoch [47/50], Step[5000/8080], Loss: 4.2455, Perplexity: 69.79\n",
      "2019-11-27 11:47:22.830656 Epoch [47/50], Step[6000/8080], Loss: 3.9455, Perplexity: 51.70\n",
      "2019-11-27 11:54:11.486732 Epoch [47/50], Step[7000/8080], Loss: 3.5785, Perplexity: 35.82\n",
      "2019-11-27 12:00:56.328878 Epoch [47/50], Step[8000/8080], Loss: 3.8744, Perplexity: 48.15\n",
      "2019-11-27 12:01:28.899709 Epoch [48/50], Step[0/8080], Loss: 3.8611, Perplexity: 47.52\n",
      "2019-11-27 12:08:15.476035 Epoch [48/50], Step[1000/8080], Loss: 3.6185, Perplexity: 37.28\n",
      "2019-11-27 12:15:08.073272 Epoch [48/50], Step[2000/8080], Loss: 3.8217, Perplexity: 45.68\n",
      "2019-11-27 12:21:52.557974 Epoch [48/50], Step[3000/8080], Loss: 3.7321, Perplexity: 41.77\n",
      "2019-11-27 12:28:34.771083 Epoch [48/50], Step[4000/8080], Loss: 3.6106, Perplexity: 36.99\n",
      "2019-11-27 12:35:19.469151 Epoch [48/50], Step[5000/8080], Loss: 4.2526, Perplexity: 70.29\n",
      "2019-11-27 12:42:03.970299 Epoch [48/50], Step[6000/8080], Loss: 3.9505, Perplexity: 51.96\n",
      "2019-11-27 12:48:48.129726 Epoch [48/50], Step[7000/8080], Loss: 3.5498, Perplexity: 34.81\n",
      "2019-11-27 12:55:33.674975 Epoch [48/50], Step[8000/8080], Loss: 3.8698, Perplexity: 47.93\n",
      "2019-11-27 12:56:06.078784 Epoch [49/50], Step[0/8080], Loss: 3.8323, Perplexity: 46.17\n",
      "2019-11-27 13:02:50.114418 Epoch [49/50], Step[1000/8080], Loss: 3.6290, Perplexity: 37.67\n",
      "2019-11-27 13:09:35.623874 Epoch [49/50], Step[2000/8080], Loss: 3.8036, Perplexity: 44.86\n",
      "2019-11-27 13:16:21.255136 Epoch [49/50], Step[3000/8080], Loss: 3.7103, Perplexity: 40.87\n",
      "2019-11-27 13:23:10.375052 Epoch [49/50], Step[4000/8080], Loss: 3.5919, Perplexity: 36.30\n",
      "2019-11-27 13:29:56.829033 Epoch [49/50], Step[5000/8080], Loss: 4.2242, Perplexity: 68.32\n",
      "2019-11-27 13:36:46.785497 Epoch [49/50], Step[6000/8080], Loss: 3.9340, Perplexity: 51.11\n",
      "2019-11-27 13:43:33.949729 Epoch [49/50], Step[7000/8080], Loss: 3.5388, Perplexity: 34.42\n",
      "2019-11-27 13:50:18.461930 Epoch [49/50], Step[8000/8080], Loss: 3.8819, Perplexity: 48.51\n",
      "2019-11-27 13:50:51.953553 Epoch [50/50], Step[0/8080], Loss: 3.8581, Perplexity: 47.37\n",
      "2019-11-27 13:57:34.605062 Epoch [50/50], Step[1000/8080], Loss: 3.6342, Perplexity: 37.87\n",
      "2019-11-27 14:04:24.110797 Epoch [50/50], Step[2000/8080], Loss: 3.7834, Perplexity: 43.96\n",
      "2019-11-27 14:11:09.660225 Epoch [50/50], Step[3000/8080], Loss: 3.7026, Perplexity: 40.55\n",
      "2019-11-27 14:17:30.455856 Epoch [50/50], Step[4000/8080], Loss: 3.6348, Perplexity: 37.89\n",
      "2019-11-27 14:22:37.300548 Epoch [50/50], Step[5000/8080], Loss: 4.2513, Perplexity: 70.19\n",
      "2019-11-27 14:27:21.030372 Epoch [50/50], Step[6000/8080], Loss: 3.9521, Perplexity: 52.04\n",
      "2019-11-27 14:32:04.183344 Epoch [50/50], Step[7000/8080], Loss: 3.5707, Perplexity: 35.54\n",
      "2019-11-27 14:35:51.154476 Epoch [50/50], Step[8000/8080], Loss: 3.9123, Perplexity: 50.02\n"
     ]
    }
   ],
   "source": [
    "# Train the CNN + RNNLM\n",
    "for param in model.embed.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    state = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Get images\n",
    "        images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "        for b, seq in enumerate(ids):\n",
    "            for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "                images[b, s] = np.load(f'char_img/noto_CJK/msr2/{idx}.npy').reshape(char_size,char_size,1)\n",
    "        images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "        images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "        images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "        \n",
    "        # Get encoded images\n",
    "        cnn_o = cnn_encoder(images)\n",
    "        cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "               \n",
    "        # Forward pass\n",
    "        state = detach(state)\n",
    "        outputs, state = model(inputs, cnn_o, state)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        cnn_encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(params, 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 1000 == 0:\n",
    "            print ('{} Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(datetime.now(), epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perplexity: 43.96638490339131\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "cnn_encoder.eval()\n",
    "\n",
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = ids[:, i:i+seq_length].to(device)\n",
    "    targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Get images\n",
    "    images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "    for b, seq in enumerate(ids):\n",
    "        for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "            images[b, s] = np.load(f'char_img/noto_CJK/msr2/{idx}.npy').reshape(char_size,char_size,1)\n",
    "    images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "    images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "    images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "\n",
    "    # Get encoded images\n",
    "    cnn_o = cnn_encoder(images)\n",
    "    cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(inputs, cnn_o, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Train Perplexity: {perplexity / num_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = corpus.get_data('icwb2-data/testing/msr_test.utf8', batch_size)\n",
    "# filter out unknown character\n",
    "test_ids = test_ids.view(-1)\n",
    "mask = test_ids < vocab_size\n",
    "test_ids = test_ids[mask]\n",
    "num_batches = test_ids.size(0) // batch_size\n",
    "test_ids = test_ids[:num_batches*batch_size]\n",
    "test_ids = test_ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 58.55850155913826\n"
     ]
    }
   ],
   "source": [
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = test_ids[:, i:i+seq_length].to(device)\n",
    "    targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Get images\n",
    "    images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "    for b, seq in enumerate(test_ids):\n",
    "        for s, idx in enumerate(test_ids[b][i:i+seq_length]):\n",
    "            images[b, s] = np.load(f'char_img/noto_CJK/msr2/{idx}.npy').reshape(char_size,char_size,1)\n",
    "    images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "    images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "    images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "\n",
    "    # Get encoded images\n",
    "    cnn_o = cnn_encoder(images)\n",
    "    cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(inputs, cnn_o, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "    \n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Test Perplexity: {perplexity / num_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch1.2)",
   "language": "python",
   "name": "pytorch1.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
