{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renderer\n",
    "from PIL import ImageFont\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "char_size = 24\n",
    "# char render\n",
    "def render(text, font=None):\n",
    "    if font is None:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\", char_size)\n",
    "    mask = font.getmask(text)\n",
    "    size = mask.size[::-1]\n",
    "    a = np.asarray(mask).reshape(size) / 255\n",
    "    res = cv2.resize(a, dsize=(char_size, char_size), interpolation=cv2.INTER_CUBIC)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model/data_utils.py\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, max_size=None):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 1\n",
    "        self.word2idx['⸘'] = 0 # as unk\n",
    "        self.idx2word[0] = '⸘'\n",
    "        self.max_size = max_size + 1\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx and self.idx < self.max_size:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, max_size=None):\n",
    "        self.dictionary = Dictionary(max_size=max_size)\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "#         with open(path, 'r') as f:\n",
    "#             tokens = 0\n",
    "#             for line in f:\n",
    "#                 words = line.split() + ['<eos>']\n",
    "#                 tokens += len(words)\n",
    "#                 for word in words: \n",
    "#                     self.dictionary.add_word(word)  \n",
    "\n",
    "        # split words to char and add to dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                tokens += len(chars)\n",
    "                for char in chars:\n",
    "                    self.dictionary.add_word(char)\n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                for char in chars:\n",
    "                    if char in self.dictionary.word2idx:\n",
    "                        ids[token] = self.dictionary.word2idx[char]\n",
    "                        token += 1\n",
    "                    else:\n",
    "                        ids[token] = self.dictionary.word2idx['⸘']\n",
    "                        token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, char_id, char_cnn_o, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(char_id) + char_cnn_o\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, h = self.gru(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)\n",
    "\"\"\"\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    if type(stride) is not tuple:\n",
    "        stride = (stride, stride)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride[0]) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride[1]) + 1)\n",
    "    return h, w\n",
    "\n",
    "# Dai et al. 's CNN glyph encoder\n",
    "class Dai_CNN(nn.Module):\n",
    "    def __init__(self, embed_size, input_size=(24, 24)):\n",
    "        super(Dai_CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, (7, 7), stride=(2,2))\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.zeros_(self.conv1.bias)\n",
    "        h, w = conv_output_shape(input_size, (7, 7), (2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 16, (5, 5), stride=(2,2))\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        torch.nn.init.zeros_(self.conv2.bias)\n",
    "        h, w = conv_output_shape((h, w), (5, 5), (2, 2))\n",
    "                \n",
    "        self.fc = nn.Linear(16*h*w, embed_size)\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        torch.nn.init.zeros_(self.fc.bias)\n",
    "        \n",
    "        self.h, self.w = h, w\n",
    "        \n",
    "    def forward(self, char_img):\n",
    "        b = char_img.size(0)\n",
    "        x = self.conv1(char_img)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = x.view(-1, 16*self.h*self.w)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "embed_size = 300\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "seq_length = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Load dataset\n",
    "corpus = Corpus(max_size=4000)\n",
    "ids = corpus.get_data('icwb2-data/training/msr_training.utf8', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save char images for reference\n",
    "for char, idx in corpus.dictionary.word2idx.items():\n",
    "    np.save(f'char_img/noto_CJK/msr4/{idx}.npy', render(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "cnn_encoder = Dai_CNN(embed_size, input_size=(char_size, char_size)).to(device)\n",
    "model.train()\n",
    "cnn_encoder.train()\n",
    "params = list(model.parameters())+list(cnn_encoder.parameters())\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# Truncated backpropagation\n",
    "def detach(state):\n",
    "    return state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-25 16:34:58.072595 Epoch [1/50], Step[0/8080], Loss: 8.3401, Perplexity: 4188.49\n",
      "2019-11-25 16:39:44.468942 Epoch [1/50], Step[1000/8080], Loss: 5.0944, Perplexity: 163.11\n",
      "2019-11-25 16:44:32.734452 Epoch [1/50], Step[2000/8080], Loss: 4.5996, Perplexity: 99.45\n",
      "2019-11-25 16:50:38.531550 Epoch [1/50], Step[3000/8080], Loss: 4.4433, Perplexity: 85.06\n",
      "2019-11-25 16:56:43.342895 Epoch [1/50], Step[4000/8080], Loss: 4.2708, Perplexity: 71.58\n",
      "2019-11-25 17:02:34.221519 Epoch [1/50], Step[5000/8080], Loss: 4.7157, Perplexity: 111.68\n",
      "2019-11-25 17:08:04.106129 Epoch [1/50], Step[6000/8080], Loss: 4.6639, Perplexity: 106.05\n",
      "2019-11-25 17:14:12.028457 Epoch [1/50], Step[7000/8080], Loss: 4.1268, Perplexity: 61.98\n",
      "2019-11-25 17:20:25.275289 Epoch [1/50], Step[8000/8080], Loss: 4.3250, Perplexity: 75.57\n",
      "2019-11-25 17:20:55.196630 Epoch [2/50], Step[0/8080], Loss: 4.2807, Perplexity: 72.29\n",
      "2019-11-25 17:27:15.567507 Epoch [2/50], Step[1000/8080], Loss: 4.1963, Perplexity: 66.44\n",
      "2019-11-25 17:33:30.886565 Epoch [2/50], Step[2000/8080], Loss: 4.1216, Perplexity: 61.66\n",
      "2019-11-25 17:40:40.923429 Epoch [2/50], Step[3000/8080], Loss: 4.0985, Perplexity: 60.25\n",
      "2019-11-25 17:47:44.836694 Epoch [2/50], Step[4000/8080], Loss: 3.9418, Perplexity: 51.51\n",
      "2019-11-25 17:54:37.485378 Epoch [2/50], Step[5000/8080], Loss: 4.4886, Perplexity: 89.00\n",
      "2019-11-25 18:01:31.548858 Epoch [2/50], Step[6000/8080], Loss: 4.3703, Perplexity: 79.07\n",
      "2019-11-25 18:08:33.190915 Epoch [2/50], Step[7000/8080], Loss: 3.9378, Perplexity: 51.31\n",
      "2019-11-25 18:15:28.355992 Epoch [2/50], Step[8000/8080], Loss: 4.1835, Perplexity: 65.60\n",
      "2019-11-25 18:16:01.664872 Epoch [3/50], Step[0/8080], Loss: 4.1229, Perplexity: 61.74\n",
      "2019-11-25 18:23:03.831970 Epoch [3/50], Step[1000/8080], Loss: 4.0260, Perplexity: 56.04\n",
      "2019-11-25 18:29:55.025325 Epoch [3/50], Step[2000/8080], Loss: 4.0506, Perplexity: 57.43\n",
      "2019-11-25 18:36:22.473870 Epoch [3/50], Step[3000/8080], Loss: 4.0126, Perplexity: 55.29\n",
      "2019-11-25 18:42:36.291950 Epoch [3/50], Step[4000/8080], Loss: 3.8768, Perplexity: 48.27\n",
      "2019-11-25 18:48:39.793349 Epoch [3/50], Step[5000/8080], Loss: 4.4198, Perplexity: 83.08\n",
      "2019-11-25 18:54:49.167643 Epoch [3/50], Step[6000/8080], Loss: 4.2583, Perplexity: 70.69\n",
      "2019-11-25 19:01:00.257901 Epoch [3/50], Step[7000/8080], Loss: 3.8580, Perplexity: 47.37\n",
      "2019-11-25 19:07:37.814319 Epoch [3/50], Step[8000/8080], Loss: 4.1257, Perplexity: 61.91\n",
      "2019-11-25 19:08:10.884237 Epoch [4/50], Step[0/8080], Loss: 4.0538, Perplexity: 57.61\n",
      "2019-11-25 19:15:01.078716 Epoch [4/50], Step[1000/8080], Loss: 3.9411, Perplexity: 51.47\n",
      "2019-11-25 19:21:40.497552 Epoch [4/50], Step[2000/8080], Loss: 4.0181, Perplexity: 55.59\n",
      "2019-11-25 19:28:10.839814 Epoch [4/50], Step[3000/8080], Loss: 3.9712, Perplexity: 53.05\n",
      "2019-11-25 19:34:18.216380 Epoch [4/50], Step[4000/8080], Loss: 3.8213, Perplexity: 45.66\n",
      "2019-11-25 19:40:27.414368 Epoch [4/50], Step[5000/8080], Loss: 4.3956, Perplexity: 81.09\n",
      "2019-11-25 19:46:42.675241 Epoch [4/50], Step[6000/8080], Loss: 4.1752, Perplexity: 65.05\n",
      "2019-11-25 19:52:50.846175 Epoch [4/50], Step[7000/8080], Loss: 3.8038, Perplexity: 44.87\n",
      "2019-11-25 19:59:05.303698 Epoch [4/50], Step[8000/8080], Loss: 4.0900, Perplexity: 59.74\n",
      "2019-11-25 19:59:34.399013 Epoch [5/50], Step[0/8080], Loss: 4.0075, Perplexity: 55.01\n",
      "2019-11-25 20:05:53.529650 Epoch [5/50], Step[1000/8080], Loss: 3.8704, Perplexity: 47.96\n",
      "2019-11-25 20:12:06.031322 Epoch [5/50], Step[2000/8080], Loss: 3.9821, Perplexity: 53.63\n",
      "2019-11-25 20:18:26.608547 Epoch [5/50], Step[3000/8080], Loss: 3.9164, Perplexity: 50.22\n",
      "2019-11-25 20:24:40.698242 Epoch [5/50], Step[4000/8080], Loss: 3.7604, Perplexity: 42.97\n",
      "2019-11-25 20:30:51.724261 Epoch [5/50], Step[5000/8080], Loss: 4.3727, Perplexity: 79.25\n",
      "2019-11-25 20:37:04.327637 Epoch [5/50], Step[6000/8080], Loss: 4.1533, Perplexity: 63.64\n",
      "2019-11-25 20:43:12.956335 Epoch [5/50], Step[7000/8080], Loss: 3.7651, Perplexity: 43.17\n",
      "2019-11-25 20:49:27.350534 Epoch [5/50], Step[8000/8080], Loss: 4.0543, Perplexity: 57.65\n",
      "2019-11-25 20:49:57.671296 Epoch [6/50], Step[0/8080], Loss: 3.9869, Perplexity: 53.89\n",
      "2019-11-25 20:56:12.238152 Epoch [6/50], Step[1000/8080], Loss: 3.8209, Perplexity: 45.65\n",
      "2019-11-25 21:02:21.187459 Epoch [6/50], Step[2000/8080], Loss: 3.9677, Perplexity: 52.86\n",
      "2019-11-25 21:08:28.487787 Epoch [6/50], Step[3000/8080], Loss: 3.8891, Perplexity: 48.87\n",
      "2019-11-25 21:14:42.725794 Epoch [6/50], Step[4000/8080], Loss: 3.7445, Perplexity: 42.29\n",
      "2019-11-25 21:20:52.371264 Epoch [6/50], Step[5000/8080], Loss: 4.3538, Perplexity: 77.77\n",
      "2019-11-25 21:27:08.239331 Epoch [6/50], Step[6000/8080], Loss: 4.1235, Perplexity: 61.78\n",
      "2019-11-25 21:33:26.185092 Epoch [6/50], Step[7000/8080], Loss: 3.7430, Perplexity: 42.22\n",
      "2019-11-25 21:40:04.826842 Epoch [6/50], Step[8000/8080], Loss: 4.0158, Perplexity: 55.47\n",
      "2019-11-25 21:40:35.999204 Epoch [7/50], Step[0/8080], Loss: 3.9424, Perplexity: 51.54\n",
      "2019-11-25 21:47:15.547301 Epoch [7/50], Step[1000/8080], Loss: 3.8023, Perplexity: 44.81\n",
      "2019-11-25 21:53:38.709625 Epoch [7/50], Step[2000/8080], Loss: 3.9347, Perplexity: 51.15\n",
      "2019-11-25 22:00:04.809529 Epoch [7/50], Step[3000/8080], Loss: 3.8803, Perplexity: 48.44\n",
      "2019-11-25 22:06:15.441320 Epoch [7/50], Step[4000/8080], Loss: 3.7132, Perplexity: 40.99\n",
      "2019-11-25 22:12:32.297083 Epoch [7/50], Step[5000/8080], Loss: 4.2993, Perplexity: 73.65\n",
      "2019-11-25 22:18:42.968793 Epoch [7/50], Step[6000/8080], Loss: 4.0904, Perplexity: 59.77\n",
      "2019-11-25 22:24:55.459749 Epoch [7/50], Step[7000/8080], Loss: 3.7132, Perplexity: 40.98\n",
      "2019-11-25 22:31:05.198435 Epoch [7/50], Step[8000/8080], Loss: 3.9888, Perplexity: 53.99\n",
      "2019-11-25 22:31:34.462608 Epoch [8/50], Step[0/8080], Loss: 3.9259, Perplexity: 50.70\n",
      "2019-11-25 22:37:41.182588 Epoch [8/50], Step[1000/8080], Loss: 3.7692, Perplexity: 43.34\n",
      "2019-11-25 22:43:58.160751 Epoch [8/50], Step[2000/8080], Loss: 3.8986, Perplexity: 49.33\n",
      "2019-11-25 22:50:38.335429 Epoch [8/50], Step[3000/8080], Loss: 3.8629, Perplexity: 47.60\n",
      "2019-11-25 22:57:37.136014 Epoch [8/50], Step[4000/8080], Loss: 3.6789, Perplexity: 39.60\n",
      "2019-11-25 23:04:35.796169 Epoch [8/50], Step[5000/8080], Loss: 4.2741, Perplexity: 71.82\n",
      "2019-11-25 23:11:28.510648 Epoch [8/50], Step[6000/8080], Loss: 4.0547, Perplexity: 57.67\n",
      "2019-11-25 23:18:21.019777 Epoch [8/50], Step[7000/8080], Loss: 3.6992, Perplexity: 40.42\n",
      "2019-11-25 23:25:12.668200 Epoch [8/50], Step[8000/8080], Loss: 3.9700, Perplexity: 52.98\n",
      "2019-11-25 23:25:43.903172 Epoch [9/50], Step[0/8080], Loss: 3.8923, Perplexity: 49.03\n",
      "2019-11-25 23:32:32.781738 Epoch [9/50], Step[1000/8080], Loss: 3.7534, Perplexity: 42.67\n",
      "2019-11-25 23:39:25.631405 Epoch [9/50], Step[2000/8080], Loss: 3.8916, Perplexity: 48.99\n",
      "2019-11-25 23:46:12.353694 Epoch [9/50], Step[3000/8080], Loss: 3.8452, Perplexity: 46.77\n",
      "2019-11-25 23:52:54.895527 Epoch [9/50], Step[4000/8080], Loss: 3.6889, Perplexity: 40.00\n",
      "2019-11-25 23:59:34.492331 Epoch [9/50], Step[5000/8080], Loss: 4.2887, Perplexity: 72.87\n",
      "2019-11-26 00:06:14.883644 Epoch [9/50], Step[6000/8080], Loss: 4.0127, Perplexity: 55.29\n",
      "2019-11-26 00:12:51.643640 Epoch [9/50], Step[7000/8080], Loss: 3.6954, Perplexity: 40.26\n",
      "2019-11-26 00:19:27.879065 Epoch [9/50], Step[8000/8080], Loss: 3.9671, Perplexity: 52.83\n",
      "2019-11-26 00:20:00.163382 Epoch [10/50], Step[0/8080], Loss: 3.8933, Perplexity: 49.07\n",
      "2019-11-26 00:26:36.062684 Epoch [10/50], Step[1000/8080], Loss: 3.7585, Perplexity: 42.88\n",
      "2019-11-26 00:33:14.461902 Epoch [10/50], Step[2000/8080], Loss: 3.8746, Perplexity: 48.16\n",
      "2019-11-26 00:39:53.104395 Epoch [10/50], Step[3000/8080], Loss: 3.8461, Perplexity: 46.81\n",
      "2019-11-26 00:46:29.665621 Epoch [10/50], Step[4000/8080], Loss: 3.6723, Perplexity: 39.34\n",
      "2019-11-26 00:53:09.764223 Epoch [10/50], Step[5000/8080], Loss: 4.2728, Perplexity: 71.72\n",
      "2019-11-26 00:59:49.521415 Epoch [10/50], Step[6000/8080], Loss: 4.0023, Perplexity: 54.72\n",
      "2019-11-26 01:06:30.063847 Epoch [10/50], Step[7000/8080], Loss: 3.6703, Perplexity: 39.27\n",
      "2019-11-26 01:13:08.063493 Epoch [10/50], Step[8000/8080], Loss: 3.9753, Perplexity: 53.26\n",
      "2019-11-26 01:13:39.691916 Epoch [11/50], Step[0/8080], Loss: 3.8640, Perplexity: 47.66\n",
      "2019-11-26 01:20:14.860606 Epoch [11/50], Step[1000/8080], Loss: 3.7177, Perplexity: 41.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 01:26:53.356157 Epoch [11/50], Step[2000/8080], Loss: 3.8872, Perplexity: 48.77\n",
      "2019-11-26 01:33:31.217732 Epoch [11/50], Step[3000/8080], Loss: 3.8239, Perplexity: 45.78\n",
      "2019-11-26 01:40:06.401453 Epoch [11/50], Step[4000/8080], Loss: 3.6757, Perplexity: 39.48\n",
      "2019-11-26 01:46:47.300892 Epoch [11/50], Step[5000/8080], Loss: 4.2616, Perplexity: 70.92\n",
      "2019-11-26 01:53:24.876903 Epoch [11/50], Step[6000/8080], Loss: 3.9693, Perplexity: 52.95\n",
      "2019-11-26 02:00:04.160616 Epoch [11/50], Step[7000/8080], Loss: 3.6650, Perplexity: 39.06\n",
      "2019-11-26 02:06:42.169588 Epoch [11/50], Step[8000/8080], Loss: 3.9640, Perplexity: 52.67\n",
      "2019-11-26 02:07:14.615242 Epoch [12/50], Step[0/8080], Loss: 3.8633, Perplexity: 47.62\n",
      "2019-11-26 02:13:52.311128 Epoch [12/50], Step[1000/8080], Loss: 3.7098, Perplexity: 40.84\n",
      "2019-11-26 02:20:29.803552 Epoch [12/50], Step[2000/8080], Loss: 3.8713, Perplexity: 48.00\n",
      "2019-11-26 02:27:08.697202 Epoch [12/50], Step[3000/8080], Loss: 3.8278, Perplexity: 45.96\n",
      "2019-11-26 02:33:48.831177 Epoch [12/50], Step[4000/8080], Loss: 3.6807, Perplexity: 39.67\n",
      "2019-11-26 02:40:24.766025 Epoch [12/50], Step[5000/8080], Loss: 4.2694, Perplexity: 71.48\n",
      "2019-11-26 02:47:01.745150 Epoch [12/50], Step[6000/8080], Loss: 3.9600, Perplexity: 52.46\n",
      "2019-11-26 02:53:42.541996 Epoch [12/50], Step[7000/8080], Loss: 3.6717, Perplexity: 39.32\n",
      "2019-11-26 03:00:21.979735 Epoch [12/50], Step[8000/8080], Loss: 3.9484, Perplexity: 51.85\n",
      "2019-11-26 03:00:53.721968 Epoch [13/50], Step[0/8080], Loss: 3.8410, Perplexity: 46.57\n",
      "2019-11-26 03:07:31.482159 Epoch [13/50], Step[1000/8080], Loss: 3.7121, Perplexity: 40.94\n",
      "2019-11-26 03:14:07.394868 Epoch [13/50], Step[2000/8080], Loss: 3.8505, Perplexity: 47.02\n",
      "2019-11-26 03:20:45.384584 Epoch [13/50], Step[3000/8080], Loss: 3.8034, Perplexity: 44.85\n",
      "2019-11-26 03:27:16.926137 Epoch [13/50], Step[4000/8080], Loss: 3.6912, Perplexity: 40.09\n",
      "2019-11-26 03:33:56.060698 Epoch [13/50], Step[5000/8080], Loss: 4.2616, Perplexity: 70.92\n",
      "2019-11-26 03:40:38.456728 Epoch [13/50], Step[6000/8080], Loss: 3.9483, Perplexity: 51.84\n",
      "2019-11-26 03:47:16.071704 Epoch [13/50], Step[7000/8080], Loss: 3.6411, Perplexity: 38.13\n",
      "2019-11-26 03:53:57.053842 Epoch [13/50], Step[8000/8080], Loss: 3.9696, Perplexity: 52.97\n",
      "2019-11-26 03:54:29.893460 Epoch [14/50], Step[0/8080], Loss: 3.8532, Perplexity: 47.14\n",
      "2019-11-26 04:01:04.537051 Epoch [14/50], Step[1000/8080], Loss: 3.7165, Perplexity: 41.12\n",
      "2019-11-26 04:07:44.456848 Epoch [14/50], Step[2000/8080], Loss: 3.8507, Perplexity: 47.03\n",
      "2019-11-26 04:14:23.006785 Epoch [14/50], Step[3000/8080], Loss: 3.7876, Perplexity: 44.15\n",
      "2019-11-26 04:21:00.161245 Epoch [14/50], Step[4000/8080], Loss: 3.6687, Perplexity: 39.20\n",
      "2019-11-26 04:27:39.686591 Epoch [14/50], Step[5000/8080], Loss: 4.2640, Perplexity: 71.10\n",
      "2019-11-26 04:34:18.905165 Epoch [14/50], Step[6000/8080], Loss: 3.9411, Perplexity: 51.48\n",
      "2019-11-26 04:40:58.135599 Epoch [14/50], Step[7000/8080], Loss: 3.6352, Perplexity: 37.91\n",
      "2019-11-26 04:47:33.849039 Epoch [14/50], Step[8000/8080], Loss: 3.9722, Perplexity: 53.10\n",
      "2019-11-26 04:48:06.128849 Epoch [15/50], Step[0/8080], Loss: 3.8483, Perplexity: 46.91\n",
      "2019-11-26 04:54:49.863369 Epoch [15/50], Step[1000/8080], Loss: 3.6809, Perplexity: 39.68\n",
      "2019-11-26 05:01:23.996120 Epoch [15/50], Step[2000/8080], Loss: 3.8362, Perplexity: 46.35\n",
      "2019-11-26 05:08:00.065742 Epoch [15/50], Step[3000/8080], Loss: 3.7606, Perplexity: 42.97\n",
      "2019-11-26 05:14:41.192050 Epoch [15/50], Step[4000/8080], Loss: 3.6949, Perplexity: 40.24\n",
      "2019-11-26 05:21:19.764455 Epoch [15/50], Step[5000/8080], Loss: 4.2625, Perplexity: 70.99\n",
      "2019-11-26 05:27:56.273419 Epoch [15/50], Step[6000/8080], Loss: 3.9183, Perplexity: 50.32\n",
      "2019-11-26 05:34:33.359305 Epoch [15/50], Step[7000/8080], Loss: 3.6434, Perplexity: 38.22\n",
      "2019-11-26 05:41:08.312474 Epoch [15/50], Step[8000/8080], Loss: 3.9709, Perplexity: 53.03\n",
      "2019-11-26 05:41:40.576800 Epoch [16/50], Step[0/8080], Loss: 3.8349, Perplexity: 46.29\n",
      "2019-11-26 05:48:18.544419 Epoch [16/50], Step[1000/8080], Loss: 3.6881, Perplexity: 39.97\n",
      "2019-11-26 05:54:57.109470 Epoch [16/50], Step[2000/8080], Loss: 3.8597, Perplexity: 47.45\n",
      "2019-11-26 06:01:35.471811 Epoch [16/50], Step[3000/8080], Loss: 3.7726, Perplexity: 43.49\n",
      "2019-11-26 06:08:09.803767 Epoch [16/50], Step[4000/8080], Loss: 3.6788, Perplexity: 39.60\n",
      "2019-11-26 06:14:47.819345 Epoch [16/50], Step[5000/8080], Loss: 4.2441, Perplexity: 69.69\n",
      "2019-11-26 06:21:25.737053 Epoch [16/50], Step[6000/8080], Loss: 3.9163, Perplexity: 50.21\n",
      "2019-11-26 06:28:02.600381 Epoch [16/50], Step[7000/8080], Loss: 3.6506, Perplexity: 38.50\n",
      "2019-11-26 06:34:39.550202 Epoch [16/50], Step[8000/8080], Loss: 3.9486, Perplexity: 51.86\n"
     ]
    }
   ],
   "source": [
    "# Train the charID RNNLM only\n",
    "for param in cnn_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for epoch in range(num_epochs//3):\n",
    "    # Set initial hidden and cell states\n",
    "    state = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Get images\n",
    "        images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "        for b, seq in enumerate(ids):\n",
    "            for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "                images[b, s] = np.load(f'char_img/noto_CJK/msr4/{idx}.npy').reshape(char_size,char_size,1)\n",
    "        images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "        images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "        images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "        \n",
    "        # Get encoded images\n",
    "        cnn_o = cnn_encoder(images)\n",
    "        cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "               \n",
    "        # Forward pass\n",
    "        state = detach(state)\n",
    "        outputs, state = model(inputs, cnn_o, state)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        cnn_encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(params, 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 1000 == 0:\n",
    "            print ('{} Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(datetime.now(), epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 06:35:11.802643 Epoch [1/50], Step[0/8080], Loss: 3.8297, Perplexity: 46.05\n",
      "2019-11-26 06:41:53.660148 Epoch [1/50], Step[1000/8080], Loss: 3.6675, Perplexity: 39.15\n",
      "2019-11-26 06:48:35.315657 Epoch [1/50], Step[2000/8080], Loss: 3.8359, Perplexity: 46.34\n",
      "2019-11-26 06:55:19.225863 Epoch [1/50], Step[3000/8080], Loss: 3.7271, Perplexity: 41.56\n",
      "2019-11-26 07:01:59.101525 Epoch [1/50], Step[4000/8080], Loss: 3.6714, Perplexity: 39.31\n",
      "2019-11-26 07:08:39.327278 Epoch [1/50], Step[5000/8080], Loss: 4.2187, Perplexity: 67.94\n",
      "2019-11-26 07:15:23.734155 Epoch [1/50], Step[6000/8080], Loss: 3.8757, Perplexity: 48.21\n",
      "2019-11-26 07:22:02.632987 Epoch [1/50], Step[7000/8080], Loss: 3.6237, Perplexity: 37.48\n",
      "2019-11-26 07:28:43.033837 Epoch [1/50], Step[8000/8080], Loss: 3.9379, Perplexity: 51.31\n",
      "2019-11-26 07:29:15.034192 Epoch [2/50], Step[0/8080], Loss: 3.8115, Perplexity: 45.22\n",
      "2019-11-26 07:35:50.747305 Epoch [2/50], Step[1000/8080], Loss: 3.6427, Perplexity: 38.20\n",
      "2019-11-26 07:42:33.439763 Epoch [2/50], Step[2000/8080], Loss: 3.8499, Perplexity: 46.99\n",
      "2019-11-26 07:49:16.684314 Epoch [2/50], Step[3000/8080], Loss: 3.7175, Perplexity: 41.16\n",
      "2019-11-26 07:55:57.950414 Epoch [2/50], Step[4000/8080], Loss: 3.6546, Perplexity: 38.65\n",
      "2019-11-26 08:02:37.661808 Epoch [2/50], Step[5000/8080], Loss: 4.1964, Perplexity: 66.45\n",
      "2019-11-26 08:09:16.364231 Epoch [2/50], Step[6000/8080], Loss: 3.8798, Perplexity: 48.41\n",
      "2019-11-26 08:15:59.077979 Epoch [2/50], Step[7000/8080], Loss: 3.6243, Perplexity: 37.50\n",
      "2019-11-26 08:22:39.359956 Epoch [2/50], Step[8000/8080], Loss: 3.9229, Perplexity: 50.55\n",
      "2019-11-26 08:23:10.968197 Epoch [3/50], Step[0/8080], Loss: 3.8113, Perplexity: 45.21\n",
      "2019-11-26 08:29:51.181049 Epoch [3/50], Step[1000/8080], Loss: 3.6638, Perplexity: 39.01\n",
      "2019-11-26 08:36:33.677339 Epoch [3/50], Step[2000/8080], Loss: 3.8558, Perplexity: 47.27\n",
      "2019-11-26 08:43:13.542770 Epoch [3/50], Step[3000/8080], Loss: 3.7247, Perplexity: 41.46\n",
      "2019-11-26 08:49:53.778485 Epoch [3/50], Step[4000/8080], Loss: 3.6437, Perplexity: 38.23\n",
      "2019-11-26 08:56:27.665610 Epoch [3/50], Step[5000/8080], Loss: 4.2129, Perplexity: 67.55\n",
      "2019-11-26 09:03:03.234181 Epoch [3/50], Step[6000/8080], Loss: 3.8922, Perplexity: 49.02\n",
      "2019-11-26 09:09:41.187583 Epoch [3/50], Step[7000/8080], Loss: 3.5981, Perplexity: 36.53\n",
      "2019-11-26 09:16:20.038753 Epoch [3/50], Step[8000/8080], Loss: 3.9347, Perplexity: 51.15\n",
      "2019-11-26 09:16:51.948572 Epoch [4/50], Step[0/8080], Loss: 3.8344, Perplexity: 46.26\n",
      "2019-11-26 09:23:32.380906 Epoch [4/50], Step[1000/8080], Loss: 3.6714, Perplexity: 39.31\n",
      "2019-11-26 09:30:06.723788 Epoch [4/50], Step[2000/8080], Loss: 3.8361, Perplexity: 46.34\n",
      "2019-11-26 09:36:44.677057 Epoch [4/50], Step[3000/8080], Loss: 3.7154, Perplexity: 41.07\n",
      "2019-11-26 09:43:24.226222 Epoch [4/50], Step[4000/8080], Loss: 3.6663, Perplexity: 39.11\n",
      "2019-11-26 09:50:02.130881 Epoch [4/50], Step[5000/8080], Loss: 4.2239, Perplexity: 68.30\n",
      "2019-11-26 09:56:40.855528 Epoch [4/50], Step[6000/8080], Loss: 3.8728, Perplexity: 48.08\n",
      "2019-11-26 10:03:21.941110 Epoch [4/50], Step[7000/8080], Loss: 3.6118, Perplexity: 37.03\n",
      "2019-11-26 10:09:56.677841 Epoch [4/50], Step[8000/8080], Loss: 3.9219, Perplexity: 50.50\n",
      "2019-11-26 10:10:28.594908 Epoch [5/50], Step[0/8080], Loss: 3.8257, Perplexity: 45.86\n",
      "2019-11-26 10:17:06.987947 Epoch [5/50], Step[1000/8080], Loss: 3.6508, Perplexity: 38.50\n",
      "2019-11-26 10:23:43.039035 Epoch [5/50], Step[2000/8080], Loss: 3.8238, Perplexity: 45.78\n",
      "2019-11-26 10:30:22.421568 Epoch [5/50], Step[3000/8080], Loss: 3.7173, Perplexity: 41.15\n",
      "2019-11-26 10:36:57.890474 Epoch [5/50], Step[4000/8080], Loss: 3.6732, Perplexity: 39.38\n",
      "2019-11-26 10:43:36.960539 Epoch [5/50], Step[5000/8080], Loss: 4.1998, Perplexity: 66.67\n",
      "2019-11-26 10:50:17.679427 Epoch [5/50], Step[6000/8080], Loss: 3.8730, Perplexity: 48.08\n",
      "2019-11-26 10:56:55.402382 Epoch [5/50], Step[7000/8080], Loss: 3.6067, Perplexity: 36.85\n",
      "2019-11-26 11:03:35.481707 Epoch [5/50], Step[8000/8080], Loss: 3.9088, Perplexity: 49.84\n",
      "2019-11-26 11:04:07.831799 Epoch [6/50], Step[0/8080], Loss: 3.8324, Perplexity: 46.17\n",
      "2019-11-26 11:10:52.582506 Epoch [6/50], Step[1000/8080], Loss: 3.6567, Perplexity: 38.73\n",
      "2019-11-26 11:17:31.003987 Epoch [6/50], Step[2000/8080], Loss: 3.8423, Perplexity: 46.63\n",
      "2019-11-26 11:24:10.991880 Epoch [6/50], Step[3000/8080], Loss: 3.7146, Perplexity: 41.04\n",
      "2019-11-26 11:30:51.103370 Epoch [6/50], Step[4000/8080], Loss: 3.6521, Perplexity: 38.56\n",
      "2019-11-26 11:37:34.143792 Epoch [6/50], Step[5000/8080], Loss: 4.1995, Perplexity: 66.66\n",
      "2019-11-26 11:44:12.034403 Epoch [6/50], Step[6000/8080], Loss: 3.8658, Perplexity: 47.74\n",
      "2019-11-26 11:50:51.132940 Epoch [6/50], Step[7000/8080], Loss: 3.6206, Perplexity: 37.36\n",
      "2019-11-26 11:57:29.604286 Epoch [6/50], Step[8000/8080], Loss: 3.9280, Perplexity: 50.80\n",
      "2019-11-26 11:58:01.178563 Epoch [7/50], Step[0/8080], Loss: 3.8471, Perplexity: 46.86\n",
      "2019-11-26 12:04:40.638005 Epoch [7/50], Step[1000/8080], Loss: 3.6481, Perplexity: 38.40\n",
      "2019-11-26 12:11:20.918259 Epoch [7/50], Step[2000/8080], Loss: 3.8339, Perplexity: 46.24\n",
      "2019-11-26 12:17:58.696223 Epoch [7/50], Step[3000/8080], Loss: 3.6904, Perplexity: 40.06\n",
      "2019-11-26 12:24:38.080876 Epoch [7/50], Step[4000/8080], Loss: 3.6662, Perplexity: 39.10\n",
      "2019-11-26 12:31:20.157909 Epoch [7/50], Step[5000/8080], Loss: 4.2221, Perplexity: 68.18\n",
      "2019-11-26 12:38:05.736316 Epoch [7/50], Step[6000/8080], Loss: 3.8800, Perplexity: 48.43\n",
      "2019-11-26 12:44:44.796082 Epoch [7/50], Step[7000/8080], Loss: 3.6024, Perplexity: 36.69\n",
      "2019-11-26 12:51:29.949043 Epoch [7/50], Step[8000/8080], Loss: 3.9110, Perplexity: 49.95\n",
      "2019-11-26 12:52:02.061914 Epoch [8/50], Step[0/8080], Loss: 3.8357, Perplexity: 46.33\n",
      "2019-11-26 12:58:42.188536 Epoch [8/50], Step[1000/8080], Loss: 3.6482, Perplexity: 38.40\n",
      "2019-11-26 13:05:17.510582 Epoch [8/50], Step[2000/8080], Loss: 3.8515, Perplexity: 47.06\n",
      "2019-11-26 13:12:01.129143 Epoch [8/50], Step[3000/8080], Loss: 3.7248, Perplexity: 41.46\n",
      "2019-11-26 13:18:41.202270 Epoch [8/50], Step[4000/8080], Loss: 3.6461, Perplexity: 38.33\n",
      "2019-11-26 13:25:22.322947 Epoch [8/50], Step[5000/8080], Loss: 4.2150, Perplexity: 67.69\n",
      "2019-11-26 13:32:01.161268 Epoch [8/50], Step[6000/8080], Loss: 3.8716, Perplexity: 48.02\n",
      "2019-11-26 13:38:41.882758 Epoch [8/50], Step[7000/8080], Loss: 3.6064, Perplexity: 36.83\n",
      "2019-11-26 13:45:19.782392 Epoch [8/50], Step[8000/8080], Loss: 3.9042, Perplexity: 49.61\n",
      "2019-11-26 13:45:52.443144 Epoch [9/50], Step[0/8080], Loss: 3.8225, Perplexity: 45.72\n",
      "2019-11-26 13:52:35.017864 Epoch [9/50], Step[1000/8080], Loss: 3.6488, Perplexity: 38.43\n",
      "2019-11-26 13:59:14.758830 Epoch [9/50], Step[2000/8080], Loss: 3.8467, Perplexity: 46.84\n",
      "2019-11-26 14:05:50.113731 Epoch [9/50], Step[3000/8080], Loss: 3.7454, Perplexity: 42.32\n",
      "2019-11-26 14:12:26.932846 Epoch [9/50], Step[4000/8080], Loss: 3.6510, Perplexity: 38.51\n",
      "2019-11-26 14:19:04.884611 Epoch [9/50], Step[5000/8080], Loss: 4.1968, Perplexity: 66.47\n",
      "2019-11-26 14:25:47.202726 Epoch [9/50], Step[6000/8080], Loss: 3.8898, Perplexity: 48.90\n",
      "2019-11-26 14:32:30.170303 Epoch [9/50], Step[7000/8080], Loss: 3.6167, Perplexity: 37.21\n",
      "2019-11-26 14:39:12.082234 Epoch [9/50], Step[8000/8080], Loss: 3.9137, Perplexity: 50.08\n",
      "2019-11-26 14:39:44.355090 Epoch [10/50], Step[0/8080], Loss: 3.8266, Perplexity: 45.91\n",
      "2019-11-26 14:46:23.307963 Epoch [10/50], Step[1000/8080], Loss: 3.6639, Perplexity: 39.01\n",
      "2019-11-26 14:53:03.092970 Epoch [10/50], Step[2000/8080], Loss: 3.8564, Perplexity: 47.29\n",
      "2019-11-26 14:59:45.301425 Epoch [10/50], Step[3000/8080], Loss: 3.7235, Perplexity: 41.41\n",
      "2019-11-26 15:06:28.359567 Epoch [10/50], Step[4000/8080], Loss: 3.6673, Perplexity: 39.15\n",
      "2019-11-26 15:13:04.800578 Epoch [10/50], Step[5000/8080], Loss: 4.2028, Perplexity: 66.88\n",
      "2019-11-26 15:19:45.660780 Epoch [10/50], Step[6000/8080], Loss: 3.9032, Perplexity: 49.56\n",
      "2019-11-26 15:25:58.918729 Epoch [10/50], Step[7000/8080], Loss: 3.6325, Perplexity: 37.81\n",
      "2019-11-26 15:32:08.648661 Epoch [10/50], Step[8000/8080], Loss: 3.8953, Perplexity: 49.17\n",
      "2019-11-26 15:32:37.496589 Epoch [11/50], Step[0/8080], Loss: 3.8179, Perplexity: 45.51\n",
      "2019-11-26 15:38:42.600079 Epoch [11/50], Step[1000/8080], Loss: 3.6444, Perplexity: 38.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 15:44:49.533847 Epoch [11/50], Step[2000/8080], Loss: 3.8393, Perplexity: 46.50\n",
      "2019-11-26 15:50:59.593768 Epoch [11/50], Step[3000/8080], Loss: 3.7506, Perplexity: 42.55\n",
      "2019-11-26 15:57:09.218949 Epoch [11/50], Step[4000/8080], Loss: 3.6560, Perplexity: 38.70\n",
      "2019-11-26 16:03:20.958462 Epoch [11/50], Step[5000/8080], Loss: 4.2227, Perplexity: 68.22\n",
      "2019-11-26 16:09:27.367717 Epoch [11/50], Step[6000/8080], Loss: 3.8905, Perplexity: 48.93\n",
      "2019-11-26 16:15:42.681668 Epoch [11/50], Step[7000/8080], Loss: 3.6294, Perplexity: 37.69\n",
      "2019-11-26 16:21:52.130293 Epoch [11/50], Step[8000/8080], Loss: 3.8928, Perplexity: 49.05\n",
      "2019-11-26 16:22:20.546166 Epoch [12/50], Step[0/8080], Loss: 3.8212, Perplexity: 45.66\n",
      "2019-11-26 16:28:33.284201 Epoch [12/50], Step[1000/8080], Loss: 3.6425, Perplexity: 38.19\n",
      "2019-11-26 16:35:02.758678 Epoch [12/50], Step[2000/8080], Loss: 3.8267, Perplexity: 45.91\n",
      "2019-11-26 16:41:52.984385 Epoch [12/50], Step[3000/8080], Loss: 3.7374, Perplexity: 41.99\n",
      "2019-11-26 16:48:44.362845 Epoch [12/50], Step[4000/8080], Loss: 3.6613, Perplexity: 38.91\n",
      "2019-11-26 16:55:35.723389 Epoch [12/50], Step[5000/8080], Loss: 4.1903, Perplexity: 66.04\n",
      "2019-11-26 17:02:33.656164 Epoch [12/50], Step[6000/8080], Loss: 3.8666, Perplexity: 47.78\n",
      "2019-11-26 17:09:41.855263 Epoch [12/50], Step[7000/8080], Loss: 3.6349, Perplexity: 37.90\n",
      "2019-11-26 17:16:55.423333 Epoch [12/50], Step[8000/8080], Loss: 3.8927, Perplexity: 49.05\n",
      "2019-11-26 17:17:30.496410 Epoch [13/50], Step[0/8080], Loss: 3.7967, Perplexity: 44.56\n",
      "2019-11-26 17:24:48.346436 Epoch [13/50], Step[1000/8080], Loss: 3.6535, Perplexity: 38.61\n",
      "2019-11-26 17:32:00.446286 Epoch [13/50], Step[2000/8080], Loss: 3.8483, Perplexity: 46.91\n",
      "2019-11-26 17:39:15.081772 Epoch [13/50], Step[3000/8080], Loss: 3.7417, Perplexity: 42.17\n",
      "2019-11-26 17:46:07.488843 Epoch [13/50], Step[4000/8080], Loss: 3.6364, Perplexity: 37.95\n",
      "2019-11-26 17:53:06.019791 Epoch [13/50], Step[5000/8080], Loss: 4.1812, Perplexity: 65.44\n",
      "2019-11-26 18:00:19.450538 Epoch [13/50], Step[6000/8080], Loss: 3.8928, Perplexity: 49.05\n",
      "2019-11-26 18:07:16.107759 Epoch [13/50], Step[7000/8080], Loss: 3.6181, Perplexity: 37.27\n",
      "2019-11-26 18:14:09.018337 Epoch [13/50], Step[8000/8080], Loss: 3.9146, Perplexity: 50.13\n",
      "2019-11-26 18:14:42.362158 Epoch [14/50], Step[0/8080], Loss: 3.8204, Perplexity: 45.62\n",
      "2019-11-26 18:21:50.275119 Epoch [14/50], Step[1000/8080], Loss: 3.6616, Perplexity: 38.92\n",
      "2019-11-26 18:29:16.613095 Epoch [14/50], Step[2000/8080], Loss: 3.8307, Perplexity: 46.10\n",
      "2019-11-26 18:36:30.473585 Epoch [14/50], Step[3000/8080], Loss: 3.7390, Perplexity: 42.06\n",
      "2019-11-26 18:43:48.699384 Epoch [14/50], Step[4000/8080], Loss: 3.6522, Perplexity: 38.56\n",
      "2019-11-26 18:51:07.199333 Epoch [14/50], Step[5000/8080], Loss: 4.1836, Perplexity: 65.60\n",
      "2019-11-26 18:58:25.705468 Epoch [14/50], Step[6000/8080], Loss: 3.8775, Perplexity: 48.31\n",
      "2019-11-26 19:05:49.082977 Epoch [14/50], Step[7000/8080], Loss: 3.6186, Perplexity: 37.28\n",
      "2019-11-26 19:12:37.670355 Epoch [14/50], Step[8000/8080], Loss: 3.9050, Perplexity: 49.65\n",
      "2019-11-26 19:13:10.529755 Epoch [15/50], Step[0/8080], Loss: 3.7970, Perplexity: 44.57\n",
      "2019-11-26 19:19:56.098524 Epoch [15/50], Step[1000/8080], Loss: 3.6638, Perplexity: 39.01\n",
      "2019-11-26 19:26:40.176933 Epoch [15/50], Step[2000/8080], Loss: 3.8352, Perplexity: 46.30\n",
      "2019-11-26 19:33:34.991005 Epoch [15/50], Step[3000/8080], Loss: 3.7389, Perplexity: 42.05\n",
      "2019-11-26 19:40:25.004424 Epoch [15/50], Step[4000/8080], Loss: 3.6507, Perplexity: 38.50\n",
      "2019-11-26 19:47:13.343834 Epoch [15/50], Step[5000/8080], Loss: 4.2037, Perplexity: 66.93\n",
      "2019-11-26 19:53:59.237278 Epoch [15/50], Step[6000/8080], Loss: 3.9099, Perplexity: 49.90\n",
      "2019-11-26 20:00:46.664646 Epoch [15/50], Step[7000/8080], Loss: 3.6113, Perplexity: 37.01\n",
      "2019-11-26 20:07:32.706036 Epoch [15/50], Step[8000/8080], Loss: 3.9139, Perplexity: 50.09\n",
      "2019-11-26 20:08:05.554795 Epoch [16/50], Step[0/8080], Loss: 3.8063, Perplexity: 44.98\n",
      "2019-11-26 20:14:54.897831 Epoch [16/50], Step[1000/8080], Loss: 3.6610, Perplexity: 38.90\n",
      "2019-11-26 20:21:41.274698 Epoch [16/50], Step[2000/8080], Loss: 3.8551, Perplexity: 47.24\n",
      "2019-11-26 20:28:27.699497 Epoch [16/50], Step[3000/8080], Loss: 3.7461, Perplexity: 42.36\n",
      "2019-11-26 20:35:14.583250 Epoch [16/50], Step[4000/8080], Loss: 3.6502, Perplexity: 38.48\n",
      "2019-11-26 20:42:03.182493 Epoch [16/50], Step[5000/8080], Loss: 4.1819, Perplexity: 65.49\n",
      "2019-11-26 20:48:49.358205 Epoch [16/50], Step[6000/8080], Loss: 3.8667, Perplexity: 47.78\n",
      "2019-11-26 20:55:35.457363 Epoch [16/50], Step[7000/8080], Loss: 3.6316, Perplexity: 37.77\n",
      "2019-11-26 21:02:21.600987 Epoch [16/50], Step[8000/8080], Loss: 3.9019, Perplexity: 49.50\n"
     ]
    }
   ],
   "source": [
    "# Train the CNN + RNNLM without Char Embedding\n",
    "for param in model.embed.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in cnn_encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for epoch in range(num_epochs//3):\n",
    "    # Set initial hidden and cell states\n",
    "    state = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Get images\n",
    "        images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "        for b, seq in enumerate(ids):\n",
    "            for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "                images[b, s] = np.load(f'char_img/noto_CJK/msr4/{idx}.npy').reshape(char_size,char_size,1)\n",
    "        images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "        images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "        images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "        \n",
    "        # Get encoded images\n",
    "        cnn_o = cnn_encoder(images)\n",
    "        cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "               \n",
    "        # Forward pass\n",
    "        state = detach(state)\n",
    "        outputs, state = model(inputs, cnn_o, state)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        cnn_encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(params, 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 1000 == 0:\n",
    "            print ('{} Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(datetime.now(), epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 21:02:53.902375 Epoch [1/50], Step[0/8080], Loss: 3.8049, Perplexity: 44.92\n",
      "2019-11-26 21:09:43.977851 Epoch [1/50], Step[1000/8080], Loss: 3.6734, Perplexity: 39.39\n",
      "2019-11-26 21:16:31.442470 Epoch [1/50], Step[2000/8080], Loss: 3.8881, Perplexity: 48.82\n",
      "2019-11-26 21:23:17.103985 Epoch [1/50], Step[3000/8080], Loss: 3.7566, Perplexity: 42.80\n",
      "2019-11-26 21:30:08.216743 Epoch [1/50], Step[4000/8080], Loss: 3.6858, Perplexity: 39.88\n",
      "2019-11-26 21:36:55.311133 Epoch [1/50], Step[5000/8080], Loss: 4.2488, Perplexity: 70.02\n",
      "2019-11-26 21:43:44.661322 Epoch [1/50], Step[6000/8080], Loss: 3.9033, Perplexity: 49.57\n",
      "2019-11-26 21:50:31.997673 Epoch [1/50], Step[7000/8080], Loss: 3.6539, Perplexity: 38.62\n",
      "2019-11-26 21:57:18.693975 Epoch [1/50], Step[8000/8080], Loss: 3.9283, Perplexity: 50.82\n",
      "2019-11-26 21:57:51.727076 Epoch [2/50], Step[0/8080], Loss: 3.8039, Perplexity: 44.87\n",
      "2019-11-26 22:04:44.687927 Epoch [2/50], Step[1000/8080], Loss: 3.6599, Perplexity: 38.86\n",
      "2019-11-26 22:11:30.892832 Epoch [2/50], Step[2000/8080], Loss: 3.8671, Perplexity: 47.80\n",
      "2019-11-26 22:18:19.321092 Epoch [2/50], Step[3000/8080], Loss: 3.7526, Perplexity: 42.63\n",
      "2019-11-26 22:25:08.035591 Epoch [2/50], Step[4000/8080], Loss: 3.6549, Perplexity: 38.66\n",
      "2019-11-26 22:31:58.258783 Epoch [2/50], Step[5000/8080], Loss: 4.2047, Perplexity: 67.00\n",
      "2019-11-26 22:38:47.148952 Epoch [2/50], Step[6000/8080], Loss: 3.9329, Perplexity: 51.05\n",
      "2019-11-26 22:45:35.131616 Epoch [2/50], Step[7000/8080], Loss: 3.6423, Perplexity: 38.18\n",
      "2019-11-26 22:52:23.069892 Epoch [2/50], Step[8000/8080], Loss: 3.9196, Perplexity: 50.38\n",
      "2019-11-26 22:52:55.470442 Epoch [3/50], Step[0/8080], Loss: 3.8215, Perplexity: 45.67\n",
      "2019-11-26 22:59:39.727747 Epoch [3/50], Step[1000/8080], Loss: 3.6647, Perplexity: 39.04\n",
      "2019-11-26 23:06:31.502034 Epoch [3/50], Step[2000/8080], Loss: 3.8653, Perplexity: 47.72\n",
      "2019-11-26 23:13:23.611482 Epoch [3/50], Step[3000/8080], Loss: 3.7385, Perplexity: 42.03\n",
      "2019-11-26 23:20:13.087191 Epoch [3/50], Step[4000/8080], Loss: 3.6631, Perplexity: 38.98\n",
      "2019-11-26 23:26:59.448569 Epoch [3/50], Step[5000/8080], Loss: 4.2400, Perplexity: 69.40\n",
      "2019-11-26 23:33:49.494289 Epoch [3/50], Step[6000/8080], Loss: 3.9384, Perplexity: 51.33\n",
      "2019-11-26 23:40:36.882424 Epoch [3/50], Step[7000/8080], Loss: 3.6512, Perplexity: 38.52\n",
      "2019-11-26 23:47:23.872770 Epoch [3/50], Step[8000/8080], Loss: 3.9500, Perplexity: 51.94\n",
      "2019-11-26 23:47:56.172415 Epoch [4/50], Step[0/8080], Loss: 3.8034, Perplexity: 44.85\n",
      "2019-11-26 23:54:42.114977 Epoch [4/50], Step[1000/8080], Loss: 3.6494, Perplexity: 38.45\n",
      "2019-11-27 00:01:28.173162 Epoch [4/50], Step[2000/8080], Loss: 3.8957, Perplexity: 49.19\n",
      "2019-11-27 00:08:16.269337 Epoch [4/50], Step[3000/8080], Loss: 3.7590, Perplexity: 42.91\n",
      "2019-11-27 00:15:05.587874 Epoch [4/50], Step[4000/8080], Loss: 3.6950, Perplexity: 40.25\n",
      "2019-11-27 00:21:53.704448 Epoch [4/50], Step[5000/8080], Loss: 4.2377, Perplexity: 69.25\n",
      "2019-11-27 00:28:44.432244 Epoch [4/50], Step[6000/8080], Loss: 3.9298, Perplexity: 50.90\n",
      "2019-11-27 00:35:32.309884 Epoch [4/50], Step[7000/8080], Loss: 3.6310, Perplexity: 37.75\n",
      "2019-11-27 00:42:20.944096 Epoch [4/50], Step[8000/8080], Loss: 3.9277, Perplexity: 50.79\n",
      "2019-11-27 00:42:53.845991 Epoch [5/50], Step[0/8080], Loss: 3.7898, Perplexity: 44.25\n",
      "2019-11-27 00:49:41.965610 Epoch [5/50], Step[1000/8080], Loss: 3.6474, Perplexity: 38.37\n",
      "2019-11-27 00:56:28.895308 Epoch [5/50], Step[2000/8080], Loss: 3.8695, Perplexity: 47.92\n",
      "2019-11-27 01:03:19.145104 Epoch [5/50], Step[3000/8080], Loss: 3.7492, Perplexity: 42.49\n",
      "2019-11-27 01:10:07.034470 Epoch [5/50], Step[4000/8080], Loss: 3.6636, Perplexity: 39.00\n",
      "2019-11-27 01:16:55.165542 Epoch [5/50], Step[5000/8080], Loss: 4.2407, Perplexity: 69.45\n",
      "2019-11-27 01:23:43.450711 Epoch [5/50], Step[6000/8080], Loss: 3.9238, Perplexity: 50.59\n",
      "2019-11-27 01:30:25.894846 Epoch [5/50], Step[7000/8080], Loss: 3.6377, Perplexity: 38.00\n",
      "2019-11-27 01:37:10.557428 Epoch [5/50], Step[8000/8080], Loss: 3.9323, Perplexity: 51.02\n",
      "2019-11-27 01:37:43.366242 Epoch [6/50], Step[0/8080], Loss: 3.7886, Perplexity: 44.19\n",
      "2019-11-27 01:44:30.928924 Epoch [6/50], Step[1000/8080], Loss: 3.6395, Perplexity: 38.07\n",
      "2019-11-27 01:51:18.796601 Epoch [6/50], Step[2000/8080], Loss: 3.8610, Perplexity: 47.51\n",
      "2019-11-27 01:58:08.773253 Epoch [6/50], Step[3000/8080], Loss: 3.7587, Perplexity: 42.89\n",
      "2019-11-27 02:04:55.434534 Epoch [6/50], Step[4000/8080], Loss: 3.6814, Perplexity: 39.70\n",
      "2019-11-27 02:11:42.620843 Epoch [6/50], Step[5000/8080], Loss: 4.1862, Perplexity: 65.77\n",
      "2019-11-27 02:18:31.809360 Epoch [6/50], Step[6000/8080], Loss: 3.9068, Perplexity: 49.74\n",
      "2019-11-27 02:25:16.926865 Epoch [6/50], Step[7000/8080], Loss: 3.6315, Perplexity: 37.77\n",
      "2019-11-27 02:32:04.137152 Epoch [6/50], Step[8000/8080], Loss: 3.9311, Perplexity: 50.96\n",
      "2019-11-27 02:32:37.361735 Epoch [7/50], Step[0/8080], Loss: 3.8163, Perplexity: 45.44\n",
      "2019-11-27 02:39:28.422095 Epoch [7/50], Step[1000/8080], Loss: 3.6162, Perplexity: 37.19\n",
      "2019-11-27 02:46:16.075101 Epoch [7/50], Step[2000/8080], Loss: 3.8806, Perplexity: 48.45\n",
      "2019-11-27 02:53:01.020794 Epoch [7/50], Step[3000/8080], Loss: 3.7794, Perplexity: 43.79\n",
      "2019-11-27 02:59:50.462471 Epoch [7/50], Step[4000/8080], Loss: 3.6734, Perplexity: 39.39\n",
      "2019-11-27 03:06:40.774227 Epoch [7/50], Step[5000/8080], Loss: 4.1862, Perplexity: 65.77\n",
      "2019-11-27 03:13:25.650775 Epoch [7/50], Step[6000/8080], Loss: 3.9114, Perplexity: 49.97\n",
      "2019-11-27 03:20:13.149619 Epoch [7/50], Step[7000/8080], Loss: 3.6198, Perplexity: 37.33\n",
      "2019-11-27 03:27:02.030802 Epoch [7/50], Step[8000/8080], Loss: 3.9410, Perplexity: 51.47\n",
      "2019-11-27 03:27:35.275876 Epoch [8/50], Step[0/8080], Loss: 3.8145, Perplexity: 45.36\n",
      "2019-11-27 03:34:23.225942 Epoch [8/50], Step[1000/8080], Loss: 3.5921, Perplexity: 36.31\n",
      "2019-11-27 03:41:14.420294 Epoch [8/50], Step[2000/8080], Loss: 3.8601, Perplexity: 47.47\n",
      "2019-11-27 03:47:59.364208 Epoch [8/50], Step[3000/8080], Loss: 3.7555, Perplexity: 42.76\n",
      "2019-11-27 03:54:42.801602 Epoch [8/50], Step[4000/8080], Loss: 3.6727, Perplexity: 39.36\n",
      "2019-11-27 04:01:31.957386 Epoch [8/50], Step[5000/8080], Loss: 4.1970, Perplexity: 66.49\n",
      "2019-11-27 04:08:23.591035 Epoch [8/50], Step[6000/8080], Loss: 3.9244, Perplexity: 50.62\n",
      "2019-11-27 04:15:10.037779 Epoch [8/50], Step[7000/8080], Loss: 3.6106, Perplexity: 36.99\n",
      "2019-11-27 04:21:56.532690 Epoch [8/50], Step[8000/8080], Loss: 3.9212, Perplexity: 50.46\n",
      "2019-11-27 04:22:29.722100 Epoch [9/50], Step[0/8080], Loss: 3.8245, Perplexity: 45.81\n",
      "2019-11-27 04:29:19.960054 Epoch [9/50], Step[1000/8080], Loss: 3.6595, Perplexity: 38.84\n",
      "2019-11-27 04:36:05.990669 Epoch [9/50], Step[2000/8080], Loss: 3.8818, Perplexity: 48.51\n",
      "2019-11-27 04:42:56.498751 Epoch [9/50], Step[3000/8080], Loss: 3.7317, Perplexity: 41.75\n",
      "2019-11-27 04:49:43.507082 Epoch [9/50], Step[4000/8080], Loss: 3.6664, Perplexity: 39.11\n",
      "2019-11-27 04:56:36.278328 Epoch [9/50], Step[5000/8080], Loss: 4.2179, Perplexity: 67.89\n",
      "2019-11-27 05:03:29.394981 Epoch [9/50], Step[6000/8080], Loss: 3.9221, Perplexity: 50.51\n",
      "2019-11-27 05:10:16.682457 Epoch [9/50], Step[7000/8080], Loss: 3.6039, Perplexity: 36.74\n",
      "2019-11-27 05:16:53.223971 Epoch [9/50], Step[8000/8080], Loss: 3.9255, Perplexity: 50.68\n",
      "2019-11-27 05:17:23.837130 Epoch [10/50], Step[0/8080], Loss: 3.7975, Perplexity: 44.59\n",
      "2019-11-27 05:23:55.760459 Epoch [10/50], Step[1000/8080], Loss: 3.6224, Perplexity: 37.43\n",
      "2019-11-27 05:30:35.826934 Epoch [10/50], Step[2000/8080], Loss: 3.9036, Perplexity: 49.58\n",
      "2019-11-27 05:37:12.614363 Epoch [10/50], Step[3000/8080], Loss: 3.7265, Perplexity: 41.53\n",
      "2019-11-27 05:43:54.387251 Epoch [10/50], Step[4000/8080], Loss: 3.6467, Perplexity: 38.35\n",
      "2019-11-27 05:50:29.949143 Epoch [10/50], Step[5000/8080], Loss: 4.2015, Perplexity: 66.79\n",
      "2019-11-27 05:57:07.054017 Epoch [10/50], Step[6000/8080], Loss: 3.9098, Perplexity: 49.89\n",
      "2019-11-27 06:03:45.165993 Epoch [10/50], Step[7000/8080], Loss: 3.6124, Perplexity: 37.05\n",
      "2019-11-27 06:10:22.445147 Epoch [10/50], Step[8000/8080], Loss: 3.9018, Perplexity: 49.49\n",
      "2019-11-27 06:10:54.309787 Epoch [11/50], Step[0/8080], Loss: 3.8217, Perplexity: 45.68\n",
      "2019-11-27 06:17:34.807779 Epoch [11/50], Step[1000/8080], Loss: 3.6066, Perplexity: 36.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-27 06:24:10.799696 Epoch [11/50], Step[2000/8080], Loss: 3.8758, Perplexity: 48.22\n",
      "2019-11-27 06:30:50.435989 Epoch [11/50], Step[3000/8080], Loss: 3.7868, Perplexity: 44.11\n",
      "2019-11-27 06:37:29.160993 Epoch [11/50], Step[4000/8080], Loss: 3.6692, Perplexity: 39.22\n",
      "2019-11-27 06:44:10.851592 Epoch [11/50], Step[5000/8080], Loss: 4.1723, Perplexity: 64.86\n",
      "2019-11-27 06:50:51.983604 Epoch [11/50], Step[6000/8080], Loss: 3.8793, Perplexity: 48.39\n",
      "2019-11-27 06:57:30.485356 Epoch [11/50], Step[7000/8080], Loss: 3.6125, Perplexity: 37.06\n",
      "2019-11-27 07:04:03.348289 Epoch [11/50], Step[8000/8080], Loss: 3.9218, Perplexity: 50.49\n",
      "2019-11-27 07:04:36.664392 Epoch [12/50], Step[0/8080], Loss: 3.8147, Perplexity: 45.36\n",
      "2019-11-27 07:11:11.777896 Epoch [12/50], Step[1000/8080], Loss: 3.5736, Perplexity: 35.64\n",
      "2019-11-27 07:17:48.786294 Epoch [12/50], Step[2000/8080], Loss: 3.8825, Perplexity: 48.54\n",
      "2019-11-27 07:24:25.754819 Epoch [12/50], Step[3000/8080], Loss: 3.7569, Perplexity: 42.82\n",
      "2019-11-27 07:31:03.050152 Epoch [12/50], Step[4000/8080], Loss: 3.6512, Perplexity: 38.52\n",
      "2019-11-27 07:37:41.410425 Epoch [12/50], Step[5000/8080], Loss: 4.1409, Perplexity: 62.86\n",
      "2019-11-27 07:44:20.143713 Epoch [12/50], Step[6000/8080], Loss: 3.8556, Perplexity: 47.26\n",
      "2019-11-27 07:51:02.113506 Epoch [12/50], Step[7000/8080], Loss: 3.6125, Perplexity: 37.06\n",
      "2019-11-27 07:57:41.767791 Epoch [12/50], Step[8000/8080], Loss: 3.9317, Perplexity: 50.99\n",
      "2019-11-27 07:58:13.532908 Epoch [13/50], Step[0/8080], Loss: 3.7959, Perplexity: 44.52\n",
      "2019-11-27 08:04:48.188716 Epoch [13/50], Step[1000/8080], Loss: 3.5948, Perplexity: 36.41\n",
      "2019-11-27 08:11:24.699558 Epoch [13/50], Step[2000/8080], Loss: 3.9000, Perplexity: 49.40\n",
      "2019-11-27 08:18:03.527742 Epoch [13/50], Step[3000/8080], Loss: 3.7658, Perplexity: 43.20\n",
      "2019-11-27 08:24:43.530598 Epoch [13/50], Step[4000/8080], Loss: 3.6419, Perplexity: 38.16\n",
      "2019-11-27 08:31:23.511946 Epoch [13/50], Step[5000/8080], Loss: 4.1787, Perplexity: 65.28\n",
      "2019-11-27 08:37:57.316285 Epoch [13/50], Step[6000/8080], Loss: 3.8551, Perplexity: 47.23\n",
      "2019-11-27 08:44:36.395213 Epoch [13/50], Step[7000/8080], Loss: 3.6201, Perplexity: 37.34\n",
      "2019-11-27 08:51:14.320619 Epoch [13/50], Step[8000/8080], Loss: 3.8973, Perplexity: 49.27\n",
      "2019-11-27 08:51:46.164296 Epoch [14/50], Step[0/8080], Loss: 3.8105, Perplexity: 45.17\n",
      "2019-11-27 08:58:20.948807 Epoch [14/50], Step[1000/8080], Loss: 3.5868, Perplexity: 36.12\n",
      "2019-11-27 09:04:59.485738 Epoch [14/50], Step[2000/8080], Loss: 3.8801, Perplexity: 48.43\n",
      "2019-11-27 09:11:35.588989 Epoch [14/50], Step[3000/8080], Loss: 3.7367, Perplexity: 41.96\n",
      "2019-11-27 09:18:17.532650 Epoch [14/50], Step[4000/8080], Loss: 3.6465, Perplexity: 38.34\n",
      "2019-11-27 09:24:56.894870 Epoch [14/50], Step[5000/8080], Loss: 4.1785, Perplexity: 65.27\n",
      "2019-11-27 09:31:31.631792 Epoch [14/50], Step[6000/8080], Loss: 3.9013, Perplexity: 49.47\n",
      "2019-11-27 09:38:07.592508 Epoch [14/50], Step[7000/8080], Loss: 3.6061, Perplexity: 36.82\n",
      "2019-11-27 09:44:43.195765 Epoch [14/50], Step[8000/8080], Loss: 3.9129, Perplexity: 50.05\n",
      "2019-11-27 09:45:15.251389 Epoch [15/50], Step[0/8080], Loss: 3.8000, Perplexity: 44.70\n",
      "2019-11-27 09:51:54.998411 Epoch [15/50], Step[1000/8080], Loss: 3.5958, Perplexity: 36.45\n",
      "2019-11-27 09:58:32.983441 Epoch [15/50], Step[2000/8080], Loss: 3.8845, Perplexity: 48.64\n",
      "2019-11-27 10:05:11.665634 Epoch [15/50], Step[3000/8080], Loss: 3.7705, Perplexity: 43.40\n",
      "2019-11-27 10:11:43.944013 Epoch [15/50], Step[4000/8080], Loss: 3.6687, Perplexity: 39.20\n",
      "2019-11-27 10:18:25.960174 Epoch [15/50], Step[5000/8080], Loss: 4.1763, Perplexity: 65.12\n",
      "2019-11-27 10:25:00.465552 Epoch [15/50], Step[6000/8080], Loss: 3.8734, Perplexity: 48.11\n",
      "2019-11-27 10:31:32.865838 Epoch [15/50], Step[7000/8080], Loss: 3.6035, Perplexity: 36.73\n",
      "2019-11-27 10:38:13.770474 Epoch [15/50], Step[8000/8080], Loss: 3.9485, Perplexity: 51.86\n",
      "2019-11-27 10:38:46.101975 Epoch [16/50], Step[0/8080], Loss: 3.8102, Perplexity: 45.16\n",
      "2019-11-27 10:45:23.082382 Epoch [16/50], Step[1000/8080], Loss: 3.5904, Perplexity: 36.25\n",
      "2019-11-27 10:52:01.846153 Epoch [16/50], Step[2000/8080], Loss: 3.8562, Perplexity: 47.28\n",
      "2019-11-27 10:58:43.977409 Epoch [16/50], Step[3000/8080], Loss: 3.7469, Perplexity: 42.39\n",
      "2019-11-27 11:05:18.325505 Epoch [16/50], Step[4000/8080], Loss: 3.6617, Perplexity: 38.93\n",
      "2019-11-27 11:11:58.866127 Epoch [16/50], Step[5000/8080], Loss: 4.1895, Perplexity: 65.99\n",
      "2019-11-27 11:18:35.542926 Epoch [16/50], Step[6000/8080], Loss: 3.8805, Perplexity: 48.45\n",
      "2019-11-27 11:25:14.227053 Epoch [16/50], Step[7000/8080], Loss: 3.6014, Perplexity: 36.65\n",
      "2019-11-27 11:31:53.486520 Epoch [16/50], Step[8000/8080], Loss: 3.9096, Perplexity: 49.88\n",
      "2019-11-27 11:32:26.121909 Epoch [17/50], Step[0/8080], Loss: 3.8324, Perplexity: 46.17\n",
      "2019-11-27 11:39:03.424929 Epoch [17/50], Step[1000/8080], Loss: 3.5832, Perplexity: 35.99\n",
      "2019-11-27 11:45:43.580057 Epoch [17/50], Step[2000/8080], Loss: 3.8729, Perplexity: 48.08\n",
      "2019-11-27 11:52:20.879331 Epoch [17/50], Step[3000/8080], Loss: 3.7376, Perplexity: 42.00\n",
      "2019-11-27 11:58:57.417868 Epoch [17/50], Step[4000/8080], Loss: 3.6358, Perplexity: 37.93\n",
      "2019-11-27 12:05:39.761910 Epoch [17/50], Step[5000/8080], Loss: 4.1645, Perplexity: 64.36\n",
      "2019-11-27 12:12:14.434427 Epoch [17/50], Step[6000/8080], Loss: 3.8698, Perplexity: 47.93\n",
      "2019-11-27 12:18:54.485008 Epoch [17/50], Step[7000/8080], Loss: 3.5678, Perplexity: 35.44\n",
      "2019-11-27 12:25:30.512073 Epoch [17/50], Step[8000/8080], Loss: 3.9455, Perplexity: 51.70\n",
      "2019-11-27 12:26:01.821231 Epoch [18/50], Step[0/8080], Loss: 3.8226, Perplexity: 45.72\n",
      "2019-11-27 12:32:37.005930 Epoch [18/50], Step[1000/8080], Loss: 3.5841, Perplexity: 36.02\n",
      "2019-11-27 12:39:14.992576 Epoch [18/50], Step[2000/8080], Loss: 3.8636, Perplexity: 47.64\n",
      "2019-11-27 12:45:48.913627 Epoch [18/50], Step[3000/8080], Loss: 3.7481, Perplexity: 42.44\n",
      "2019-11-27 12:52:27.657769 Epoch [18/50], Step[4000/8080], Loss: 3.6367, Perplexity: 37.97\n",
      "2019-11-27 12:59:05.916944 Epoch [18/50], Step[5000/8080], Loss: 4.1826, Perplexity: 65.54\n",
      "2019-11-27 13:05:41.047630 Epoch [18/50], Step[6000/8080], Loss: 3.8822, Perplexity: 48.53\n",
      "2019-11-27 13:12:16.499902 Epoch [18/50], Step[7000/8080], Loss: 3.5890, Perplexity: 36.20\n",
      "2019-11-27 13:18:50.307043 Epoch [18/50], Step[8000/8080], Loss: 3.9294, Perplexity: 50.88\n"
     ]
    }
   ],
   "source": [
    "# Train the CNN + RNNLM\n",
    "for param in model.embed.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for epoch in range(num_epochs//3 + num_epochs%3):\n",
    "    # Set initial hidden and cell states\n",
    "    state = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Get images\n",
    "        images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "        for b, seq in enumerate(ids):\n",
    "            for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "                images[b, s] = np.load(f'char_img/noto_CJK/msr4/{idx}.npy').reshape(char_size,char_size,1)\n",
    "        images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "        images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "        images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "        \n",
    "        # Get encoded images\n",
    "        cnn_o = cnn_encoder(images)\n",
    "        cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "               \n",
    "        # Forward pass\n",
    "        state = detach(state)\n",
    "        outputs, state = model(inputs, cnn_o, state)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        cnn_encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(params, 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 1000 == 0:\n",
    "            print ('{} Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(datetime.now(), epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perplexity: 44.965010491396036\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "cnn_encoder.eval()\n",
    "\n",
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = ids[:, i:i+seq_length].to(device)\n",
    "    targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Get images\n",
    "    images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "    for b, seq in enumerate(ids):\n",
    "        for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "            images[b, s] = np.load(f'char_img/noto_CJK/msr4/{idx}.npy').reshape(char_size,char_size,1)\n",
    "    images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "    images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "    images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "\n",
    "    # Get encoded images\n",
    "    cnn_o = cnn_encoder(images)\n",
    "    cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(inputs, cnn_o, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Train Perplexity: {perplexity / num_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = corpus.get_data('icwb2-data/testing/msr_test.utf8', batch_size)\n",
    "# filter out unknown character\n",
    "test_ids = test_ids.view(-1)\n",
    "mask = test_ids < vocab_size\n",
    "test_ids = test_ids[mask]\n",
    "num_batches = test_ids.size(0) // batch_size\n",
    "test_ids = test_ids[:num_batches*batch_size]\n",
    "test_ids = test_ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 58.09019991798291\n"
     ]
    }
   ],
   "source": [
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = test_ids[:, i:i+seq_length].to(device)\n",
    "    targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Get images\n",
    "    images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "    for b, seq in enumerate(test_ids):\n",
    "        for s, idx in enumerate(test_ids[b][i:i+seq_length]):\n",
    "            images[b, s] = np.load(f'char_img/noto_CJK/msr4/{idx}.npy').reshape(char_size,char_size,1)\n",
    "    images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "    images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "    images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "\n",
    "    # Get encoded images\n",
    "    cnn_o = cnn_encoder(images)\n",
    "    cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(inputs, cnn_o, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "    \n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Test Perplexity: {perplexity / num_step}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch1.2)",
   "language": "python",
   "name": "pytorch1.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
