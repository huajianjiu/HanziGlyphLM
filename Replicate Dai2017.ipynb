{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renderer\n",
    "from PIL import ImageFont\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# char render\n",
    "def render(text, font=None):\n",
    "    if font is None:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\", 32)\n",
    "    mask = font.getmask(text)\n",
    "    size = mask.size[::-1]\n",
    "    a = np.asarray(mask).reshape(size) / 255\n",
    "    res = cv2.resize(a, dsize=(32, 32), interpolation=cv2.INTER_CUBIC)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5e01f6f5f8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test renderer\n",
    "b = np.zeros((10, 32,32))\n",
    "b[0] = render('?')\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "imshow(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model/data_utils.py\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "#         with open(path, 'r') as f:\n",
    "#             tokens = 0\n",
    "#             for line in f:\n",
    "#                 words = line.split() + ['<eos>']\n",
    "#                 tokens += len(words)\n",
    "#                 for word in words: \n",
    "#                     self.dictionary.add_word(word)  \n",
    "\n",
    "        # split words to char and add to dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                tokens += len(chars)\n",
    "                for char in chars:\n",
    "                    self.dictionary.add_word(char)\n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                for char in chars:\n",
    "                    ids[token] = self.dictionary.word2idx[char]\n",
    "                    token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test corpus\n",
    "batch_size = 4\n",
    "seq_length = 30\n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data('icwb2-data/training/msr_training.utf8', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n",
      "           12,   13,   14,   15,   16,   17,   18,   19,   20,   21,    7,   22,\n",
      "           23,   24,   25,   18,   10,   11],\n",
      "        [ 133,  519,  326,  391,   96, 1921, 1919, 1919,  577,  503,   29, 1004,\n",
      "          131, 2179,   18,   34,  872,  744,   96,  677,  498,   37,   38, 1403,\n",
      "          153,  335, 1620,  214, 1597,   13],\n",
      "        [ 584,  496,  582,   13,   22,  300,  496,  582,   18, 1481,   39,  496,\n",
      "         1274,   37,   38,  110,   82,   67,   25,  105,   92,  120,  319,  792,\n",
      "            8,    7,  399,  572,  982,  365],\n",
      "        [ 110,    8,   22,  243, 1199,  394,  156,  140,  103,  223, 1171,  694,\n",
      "           83,   18,  503,  326,  348,  874,   13,  548,   78,   79, 1198,  676,\n",
      "         1270,  350,  218,  408,  454,  977]])\n",
      "['“', '人', '们', '常', '说', '生', '活', '是', '一', '部']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5168"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ids[:, 0:seq_length])\n",
    "print(list(corpus.dictionary.word2idx.keys())[:10])\n",
    "len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, char_id, char_cnn_o, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(char_id) + char_cnn_o\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, h = self.gru(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)\n",
    "\"\"\"\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    if type(stride) is not tuple:\n",
    "        stride = (stride, stride)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride[0]) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride[1]) + 1)\n",
    "    return h, w\n",
    "\n",
    "# Dai et al. 's CNN glyph encoder\n",
    "class Dai_CNN(nn.Module):\n",
    "    def __init__(self, embed_size, input_size=(32, 32)):\n",
    "        super(Dai_CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, (7, 7), stride=(2,2))\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.zeros_(self.conv1.bias)\n",
    "        h, w = conv_output_shape(input_size, (7, 7), (2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 16, (5, 5), stride=(2,2))\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        torch.nn.init.zeros_(self.conv2.bias)\n",
    "        h, w = conv_output_shape((h, w), (5, 5), (2, 2))\n",
    "                \n",
    "        self.fc = nn.Linear(16*h*w, embed_size)\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        torch.nn.init.zeros_(self.fc.bias)\n",
    "        \n",
    "        self.h, self.w = h, w\n",
    "        \n",
    "    def forward(self, char_img):\n",
    "        b = char_img.size(0)\n",
    "        x = self.conv1(char_img)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = x.view(-1, 16*self.h*self.w)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "embed_size = 300\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "seq_length = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Load dataset\n",
    "corpus = Corpus()\n",
    "ids = corpus.get_data('icwb2-data/training/msr_training.utf8', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save char images for reference\n",
    "for char, idx in corpus.dictionary.word2idx.items():\n",
    "    np.save(f'char_img/noto_CJK/msr/{idx}.npy', render(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "cnn_encoder = Dai_CNN(embed_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Truncated backpropagation\n",
    "def detach(state):\n",
    "    return state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-15 20:35:33.248664 Epoch [1/50], Step[0/8619], Loss: 8.5738, Perplexity: 5291.28\n",
      "2019-11-15 20:38:05.545137 Epoch [1/50], Step[1000/8619], Loss: 4.8257, Perplexity: 124.67\n",
      "2019-11-15 20:40:28.080864 Epoch [1/50], Step[2000/8619], Loss: 4.7048, Perplexity: 110.47\n",
      "2019-11-15 20:42:54.297927 Epoch [1/50], Step[3000/8619], Loss: 4.3127, Perplexity: 74.64\n",
      "2019-11-15 20:45:18.832006 Epoch [1/50], Step[4000/8619], Loss: 4.3863, Perplexity: 80.34\n",
      "2019-11-15 20:47:38.981444 Epoch [1/50], Step[5000/8619], Loss: 4.3070, Perplexity: 74.22\n",
      "2019-11-15 20:49:58.340317 Epoch [1/50], Step[6000/8619], Loss: 4.4113, Perplexity: 82.38\n",
      "2019-11-15 20:52:19.563799 Epoch [1/50], Step[7000/8619], Loss: 4.4780, Perplexity: 88.06\n",
      "2019-11-15 20:54:37.744089 Epoch [1/50], Step[8000/8619], Loss: 4.3771, Perplexity: 79.61\n",
      "2019-11-15 20:56:04.055494 Epoch [2/50], Step[0/8619], Loss: 4.2765, Perplexity: 71.99\n",
      "2019-11-15 20:58:19.190925 Epoch [2/50], Step[1000/8619], Loss: 3.9914, Perplexity: 54.13\n",
      "2019-11-15 21:00:34.438605 Epoch [2/50], Step[2000/8619], Loss: 4.1844, Perplexity: 65.65\n",
      "2019-11-15 21:02:48.158432 Epoch [2/50], Step[3000/8619], Loss: 4.0166, Perplexity: 55.51\n",
      "2019-11-15 21:05:04.407900 Epoch [2/50], Step[4000/8619], Loss: 3.9811, Perplexity: 53.58\n",
      "2019-11-15 21:07:19.657073 Epoch [2/50], Step[5000/8619], Loss: 4.0618, Perplexity: 58.08\n",
      "2019-11-15 21:09:36.927204 Epoch [2/50], Step[6000/8619], Loss: 4.1859, Perplexity: 65.75\n",
      "2019-11-15 21:11:54.498549 Epoch [2/50], Step[7000/8619], Loss: 4.2537, Perplexity: 70.36\n",
      "2019-11-15 21:14:11.375617 Epoch [2/50], Step[8000/8619], Loss: 4.2437, Perplexity: 69.67\n",
      "2019-11-15 21:15:40.072735 Epoch [3/50], Step[0/8619], Loss: 4.1538, Perplexity: 63.67\n",
      "2019-11-15 21:17:57.276801 Epoch [3/50], Step[1000/8619], Loss: 3.8907, Perplexity: 48.95\n",
      "2019-11-15 21:20:16.746542 Epoch [3/50], Step[2000/8619], Loss: 4.1132, Perplexity: 61.14\n",
      "2019-11-15 21:22:33.606564 Epoch [3/50], Step[3000/8619], Loss: 3.9337, Perplexity: 51.10\n",
      "2019-11-15 21:24:47.992974 Epoch [3/50], Step[4000/8619], Loss: 3.8519, Perplexity: 47.08\n",
      "2019-11-15 21:27:03.624789 Epoch [3/50], Step[5000/8619], Loss: 3.9634, Perplexity: 52.64\n",
      "2019-11-15 21:29:20.406928 Epoch [3/50], Step[6000/8619], Loss: 4.0846, Perplexity: 59.42\n",
      "2019-11-15 21:31:40.245337 Epoch [3/50], Step[7000/8619], Loss: 4.1763, Perplexity: 65.13\n",
      "2019-11-15 21:34:04.077666 Epoch [3/50], Step[8000/8619], Loss: 4.1401, Perplexity: 62.81\n",
      "2019-11-15 21:35:33.245014 Epoch [4/50], Step[0/8619], Loss: 4.0417, Perplexity: 56.92\n",
      "2019-11-15 21:37:54.413609 Epoch [4/50], Step[1000/8619], Loss: 3.8278, Perplexity: 45.96\n",
      "2019-11-15 21:40:17.213776 Epoch [4/50], Step[2000/8619], Loss: 4.0369, Perplexity: 56.65\n",
      "2019-11-15 21:42:34.884654 Epoch [4/50], Step[3000/8619], Loss: 3.8975, Perplexity: 49.28\n",
      "2019-11-15 21:44:51.987611 Epoch [4/50], Step[4000/8619], Loss: 3.8019, Perplexity: 44.79\n",
      "2019-11-15 21:47:10.314319 Epoch [4/50], Step[5000/8619], Loss: 3.9270, Perplexity: 50.75\n",
      "2019-11-15 21:49:28.154664 Epoch [4/50], Step[6000/8619], Loss: 4.0741, Perplexity: 58.80\n",
      "2019-11-15 21:51:49.604340 Epoch [4/50], Step[7000/8619], Loss: 4.1268, Perplexity: 61.98\n",
      "2019-11-15 21:54:07.554098 Epoch [4/50], Step[8000/8619], Loss: 4.1089, Perplexity: 60.88\n",
      "2019-11-15 21:55:32.903306 Epoch [5/50], Step[0/8619], Loss: 4.0002, Perplexity: 54.61\n",
      "2019-11-15 21:57:50.084720 Epoch [5/50], Step[1000/8619], Loss: 3.7955, Perplexity: 44.50\n",
      "2019-11-15 22:00:08.673420 Epoch [5/50], Step[2000/8619], Loss: 4.0033, Perplexity: 54.78\n",
      "2019-11-15 22:02:30.736372 Epoch [5/50], Step[3000/8619], Loss: 3.8526, Perplexity: 47.12\n",
      "2019-11-15 22:04:46.062614 Epoch [5/50], Step[4000/8619], Loss: 3.7309, Perplexity: 41.72\n",
      "2019-11-15 22:07:02.826583 Epoch [5/50], Step[5000/8619], Loss: 3.9233, Perplexity: 50.57\n",
      "2019-11-15 22:09:19.530360 Epoch [5/50], Step[6000/8619], Loss: 4.0336, Perplexity: 56.46\n",
      "2019-11-15 22:11:35.648921 Epoch [5/50], Step[7000/8619], Loss: 4.0752, Perplexity: 58.86\n",
      "2019-11-15 22:13:56.781983 Epoch [5/50], Step[8000/8619], Loss: 4.0790, Perplexity: 59.08\n",
      "2019-11-15 22:15:23.606404 Epoch [6/50], Step[0/8619], Loss: 3.9836, Perplexity: 53.71\n",
      "2019-11-15 22:17:38.870137 Epoch [6/50], Step[1000/8619], Loss: 3.7831, Perplexity: 43.95\n",
      "2019-11-15 22:19:54.466223 Epoch [6/50], Step[2000/8619], Loss: 3.9689, Perplexity: 52.93\n",
      "2019-11-15 22:22:09.071817 Epoch [6/50], Step[3000/8619], Loss: 3.8242, Perplexity: 45.79\n",
      "2019-11-15 22:24:24.880927 Epoch [6/50], Step[4000/8619], Loss: 3.7033, Perplexity: 40.58\n",
      "2019-11-15 22:26:45.279757 Epoch [6/50], Step[5000/8619], Loss: 3.8841, Perplexity: 48.62\n",
      "2019-11-15 22:28:59.643404 Epoch [6/50], Step[6000/8619], Loss: 4.0325, Perplexity: 56.40\n",
      "2019-11-15 22:31:13.327024 Epoch [6/50], Step[7000/8619], Loss: 4.0264, Perplexity: 56.06\n",
      "2019-11-15 22:33:29.546016 Epoch [6/50], Step[8000/8619], Loss: 4.0667, Perplexity: 58.36\n",
      "2019-11-15 22:34:58.115742 Epoch [7/50], Step[0/8619], Loss: 3.9629, Perplexity: 52.61\n",
      "2019-11-15 22:37:11.659666 Epoch [7/50], Step[1000/8619], Loss: 3.7452, Perplexity: 42.32\n",
      "2019-11-15 22:39:26.331544 Epoch [7/50], Step[2000/8619], Loss: 3.9519, Perplexity: 52.04\n",
      "2019-11-15 22:41:42.539484 Epoch [7/50], Step[3000/8619], Loss: 3.8114, Perplexity: 45.21\n",
      "2019-11-15 22:43:58.950192 Epoch [7/50], Step[4000/8619], Loss: 3.6781, Perplexity: 39.57\n",
      "2019-11-15 22:46:15.125695 Epoch [7/50], Step[5000/8619], Loss: 3.8736, Perplexity: 48.11\n",
      "2019-11-15 22:48:31.654512 Epoch [7/50], Step[6000/8619], Loss: 4.0081, Perplexity: 55.04\n",
      "2019-11-15 22:50:48.022061 Epoch [7/50], Step[7000/8619], Loss: 4.0227, Perplexity: 55.85\n",
      "2019-11-15 22:53:06.952173 Epoch [7/50], Step[8000/8619], Loss: 4.0414, Perplexity: 56.91\n",
      "2019-11-15 22:54:33.497795 Epoch [8/50], Step[0/8619], Loss: 3.9443, Perplexity: 51.64\n",
      "2019-11-15 22:56:53.808391 Epoch [8/50], Step[1000/8619], Loss: 3.7512, Perplexity: 42.57\n",
      "2019-11-15 22:59:09.658868 Epoch [8/50], Step[2000/8619], Loss: 3.9206, Perplexity: 50.43\n",
      "2019-11-15 23:01:30.166706 Epoch [8/50], Step[3000/8619], Loss: 3.7883, Perplexity: 44.18\n",
      "2019-11-15 23:03:50.048131 Epoch [8/50], Step[4000/8619], Loss: 3.6410, Perplexity: 38.13\n",
      "2019-11-15 23:06:11.193274 Epoch [8/50], Step[5000/8619], Loss: 3.8675, Perplexity: 47.82\n",
      "2019-11-15 23:08:25.848912 Epoch [8/50], Step[6000/8619], Loss: 3.9901, Perplexity: 54.06\n",
      "2019-11-15 23:10:43.307970 Epoch [8/50], Step[7000/8619], Loss: 3.9951, Perplexity: 54.33\n",
      "2019-11-15 23:13:04.893669 Epoch [8/50], Step[8000/8619], Loss: 4.0380, Perplexity: 56.71\n",
      "2019-11-15 23:14:31.193578 Epoch [9/50], Step[0/8619], Loss: 3.9473, Perplexity: 51.79\n",
      "2019-11-15 23:16:46.187861 Epoch [9/50], Step[1000/8619], Loss: 3.7418, Perplexity: 42.17\n",
      "2019-11-15 23:19:07.267400 Epoch [9/50], Step[2000/8619], Loss: 3.9468, Perplexity: 51.77\n",
      "2019-11-15 23:21:24.123926 Epoch [9/50], Step[3000/8619], Loss: 3.7793, Perplexity: 43.78\n",
      "2019-11-15 23:23:41.727427 Epoch [9/50], Step[4000/8619], Loss: 3.6362, Perplexity: 37.95\n",
      "2019-11-15 23:25:57.532634 Epoch [9/50], Step[5000/8619], Loss: 3.8364, Perplexity: 46.36\n",
      "2019-11-15 23:28:19.289142 Epoch [9/50], Step[6000/8619], Loss: 3.9809, Perplexity: 53.57\n",
      "2019-11-15 23:30:43.415489 Epoch [9/50], Step[7000/8619], Loss: 3.9788, Perplexity: 53.45\n",
      "2019-11-15 23:32:58.783541 Epoch [9/50], Step[8000/8619], Loss: 4.0144, Perplexity: 55.39\n",
      "2019-11-15 23:34:22.027414 Epoch [10/50], Step[0/8619], Loss: 3.9223, Perplexity: 50.52\n",
      "2019-11-15 23:36:39.166069 Epoch [10/50], Step[1000/8619], Loss: 3.7133, Perplexity: 40.99\n",
      "2019-11-15 23:38:59.988681 Epoch [10/50], Step[2000/8619], Loss: 3.9626, Perplexity: 52.59\n",
      "2019-11-15 23:41:23.119127 Epoch [10/50], Step[3000/8619], Loss: 3.7675, Perplexity: 43.27\n",
      "2019-11-15 23:43:43.820476 Epoch [10/50], Step[4000/8619], Loss: 3.6108, Perplexity: 37.00\n",
      "2019-11-15 23:46:01.835561 Epoch [10/50], Step[5000/8619], Loss: 3.8032, Perplexity: 44.84\n",
      "2019-11-15 23:48:23.000772 Epoch [10/50], Step[6000/8619], Loss: 3.9469, Perplexity: 51.77\n",
      "2019-11-15 23:50:41.162743 Epoch [10/50], Step[7000/8619], Loss: 3.9766, Perplexity: 53.33\n",
      "2019-11-15 23:52:56.549718 Epoch [10/50], Step[8000/8619], Loss: 3.9744, Perplexity: 53.22\n",
      "2019-11-15 23:54:20.711831 Epoch [11/50], Step[0/8619], Loss: 3.9325, Perplexity: 51.03\n",
      "2019-11-15 23:56:36.910430 Epoch [11/50], Step[1000/8619], Loss: 3.7136, Perplexity: 41.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-15 23:58:52.471713 Epoch [11/50], Step[2000/8619], Loss: 3.9180, Perplexity: 50.30\n",
      "2019-11-16 00:01:07.866914 Epoch [11/50], Step[3000/8619], Loss: 3.7297, Perplexity: 41.67\n",
      "2019-11-16 00:03:29.224061 Epoch [11/50], Step[4000/8619], Loss: 3.6256, Perplexity: 37.55\n",
      "2019-11-16 00:05:50.479692 Epoch [11/50], Step[5000/8619], Loss: 3.8179, Perplexity: 45.51\n",
      "2019-11-16 00:08:08.455362 Epoch [11/50], Step[6000/8619], Loss: 3.9802, Perplexity: 53.53\n",
      "2019-11-16 00:10:28.785357 Epoch [11/50], Step[7000/8619], Loss: 3.9639, Perplexity: 52.66\n",
      "2019-11-16 00:12:45.263853 Epoch [11/50], Step[8000/8619], Loss: 3.9923, Perplexity: 54.18\n",
      "2019-11-16 00:14:10.907260 Epoch [12/50], Step[0/8619], Loss: 3.9282, Perplexity: 50.82\n",
      "2019-11-16 00:16:27.539015 Epoch [12/50], Step[1000/8619], Loss: 3.6994, Perplexity: 40.42\n",
      "2019-11-16 00:18:51.176727 Epoch [12/50], Step[2000/8619], Loss: 3.9268, Perplexity: 50.74\n",
      "2019-11-16 00:21:12.073478 Epoch [12/50], Step[3000/8619], Loss: 3.7216, Perplexity: 41.33\n",
      "2019-11-16 00:23:31.221916 Epoch [12/50], Step[4000/8619], Loss: 3.6240, Perplexity: 37.49\n",
      "2019-11-16 00:25:46.995263 Epoch [12/50], Step[5000/8619], Loss: 3.8121, Perplexity: 45.24\n",
      "2019-11-16 00:28:04.728234 Epoch [12/50], Step[6000/8619], Loss: 3.9591, Perplexity: 52.41\n",
      "2019-11-16 00:30:29.548537 Epoch [12/50], Step[7000/8619], Loss: 3.9452, Perplexity: 51.68\n",
      "2019-11-16 00:32:47.414012 Epoch [12/50], Step[8000/8619], Loss: 4.0194, Perplexity: 55.67\n",
      "2019-11-16 00:34:11.843645 Epoch [13/50], Step[0/8619], Loss: 3.8968, Perplexity: 49.25\n",
      "2019-11-16 00:36:28.517670 Epoch [13/50], Step[1000/8619], Loss: 3.7021, Perplexity: 40.53\n",
      "2019-11-16 00:38:44.426302 Epoch [13/50], Step[2000/8619], Loss: 3.9134, Perplexity: 50.07\n",
      "2019-11-16 00:41:02.344459 Epoch [13/50], Step[3000/8619], Loss: 3.6998, Perplexity: 40.44\n",
      "2019-11-16 00:43:16.511120 Epoch [13/50], Step[4000/8619], Loss: 3.6093, Perplexity: 36.94\n",
      "2019-11-16 00:45:34.055968 Epoch [13/50], Step[5000/8619], Loss: 3.8132, Perplexity: 45.30\n",
      "2019-11-16 00:47:52.616527 Epoch [13/50], Step[6000/8619], Loss: 3.9465, Perplexity: 51.76\n",
      "2019-11-16 00:50:15.259831 Epoch [13/50], Step[7000/8619], Loss: 3.9473, Perplexity: 51.80\n",
      "2019-11-16 00:52:32.618904 Epoch [13/50], Step[8000/8619], Loss: 4.0095, Perplexity: 55.12\n",
      "2019-11-16 00:53:57.199229 Epoch [14/50], Step[0/8619], Loss: 3.9027, Perplexity: 49.53\n",
      "2019-11-16 00:56:17.731815 Epoch [14/50], Step[1000/8619], Loss: 3.6637, Perplexity: 39.01\n",
      "2019-11-16 00:58:36.106656 Epoch [14/50], Step[2000/8619], Loss: 3.8860, Perplexity: 48.71\n",
      "2019-11-16 01:00:56.294437 Epoch [14/50], Step[3000/8619], Loss: 3.7161, Perplexity: 41.10\n",
      "2019-11-16 01:03:12.380337 Epoch [14/50], Step[4000/8619], Loss: 3.5975, Perplexity: 36.51\n",
      "2019-11-16 01:05:27.299665 Epoch [14/50], Step[5000/8619], Loss: 3.7902, Perplexity: 44.27\n",
      "2019-11-16 01:07:43.510995 Epoch [14/50], Step[6000/8619], Loss: 3.9732, Perplexity: 53.15\n",
      "2019-11-16 01:10:00.255471 Epoch [14/50], Step[7000/8619], Loss: 3.9213, Perplexity: 50.47\n",
      "2019-11-16 01:12:22.815771 Epoch [14/50], Step[8000/8619], Loss: 3.9992, Perplexity: 54.55\n",
      "2019-11-16 01:13:46.068333 Epoch [15/50], Step[0/8619], Loss: 3.9042, Perplexity: 49.61\n",
      "2019-11-16 01:16:05.876103 Epoch [15/50], Step[1000/8619], Loss: 3.6573, Perplexity: 38.76\n",
      "2019-11-16 01:18:20.953041 Epoch [15/50], Step[2000/8619], Loss: 3.8927, Perplexity: 49.05\n",
      "2019-11-16 01:20:37.382843 Epoch [15/50], Step[3000/8619], Loss: 3.6902, Perplexity: 40.05\n",
      "2019-11-16 01:22:52.240829 Epoch [15/50], Step[4000/8619], Loss: 3.5761, Perplexity: 35.73\n",
      "2019-11-16 01:25:10.891080 Epoch [15/50], Step[5000/8619], Loss: 3.8202, Perplexity: 45.61\n",
      "2019-11-16 01:27:30.302865 Epoch [15/50], Step[6000/8619], Loss: 3.9400, Perplexity: 51.42\n",
      "2019-11-16 01:29:51.655887 Epoch [15/50], Step[7000/8619], Loss: 3.9208, Perplexity: 50.44\n",
      "2019-11-16 01:32:12.279386 Epoch [15/50], Step[8000/8619], Loss: 3.9933, Perplexity: 54.23\n",
      "2019-11-16 01:33:35.626505 Epoch [16/50], Step[0/8619], Loss: 3.8775, Perplexity: 48.30\n",
      "2019-11-16 01:35:53.826779 Epoch [16/50], Step[1000/8619], Loss: 3.6373, Perplexity: 37.99\n",
      "2019-11-16 01:38:09.380924 Epoch [16/50], Step[2000/8619], Loss: 3.8737, Perplexity: 48.12\n",
      "2019-11-16 01:40:29.640152 Epoch [16/50], Step[3000/8619], Loss: 3.6891, Perplexity: 40.01\n",
      "2019-11-16 01:42:50.374252 Epoch [16/50], Step[4000/8619], Loss: 3.5459, Perplexity: 34.67\n",
      "2019-11-16 01:45:10.020415 Epoch [16/50], Step[5000/8619], Loss: 3.8180, Perplexity: 45.51\n",
      "2019-11-16 01:47:25.791892 Epoch [16/50], Step[6000/8619], Loss: 3.9332, Perplexity: 51.07\n",
      "2019-11-16 01:49:44.201872 Epoch [16/50], Step[7000/8619], Loss: 3.8956, Perplexity: 49.19\n",
      "2019-11-16 01:52:04.532347 Epoch [16/50], Step[8000/8619], Loss: 3.9849, Perplexity: 53.78\n",
      "2019-11-16 01:53:30.994517 Epoch [17/50], Step[0/8619], Loss: 3.8631, Perplexity: 47.61\n",
      "2019-11-16 01:55:45.889952 Epoch [17/50], Step[1000/8619], Loss: 3.6198, Perplexity: 37.33\n",
      "2019-11-16 01:58:01.315124 Epoch [17/50], Step[2000/8619], Loss: 3.8579, Perplexity: 47.37\n",
      "2019-11-16 02:00:16.895977 Epoch [17/50], Step[3000/8619], Loss: 3.6612, Perplexity: 38.91\n",
      "2019-11-16 02:02:30.738185 Epoch [17/50], Step[4000/8619], Loss: 3.5440, Perplexity: 34.61\n",
      "2019-11-16 02:04:45.723048 Epoch [17/50], Step[5000/8619], Loss: 3.8213, Perplexity: 45.66\n",
      "2019-11-16 02:07:05.007941 Epoch [17/50], Step[6000/8619], Loss: 3.9192, Perplexity: 50.36\n",
      "2019-11-16 02:09:19.048200 Epoch [17/50], Step[7000/8619], Loss: 3.9264, Perplexity: 50.72\n",
      "2019-11-16 02:11:40.454433 Epoch [17/50], Step[8000/8619], Loss: 3.9824, Perplexity: 53.64\n",
      "2019-11-16 02:13:07.268063 Epoch [18/50], Step[0/8619], Loss: 3.8612, Perplexity: 47.52\n",
      "2019-11-16 02:15:25.666489 Epoch [18/50], Step[1000/8619], Loss: 3.6278, Perplexity: 37.63\n",
      "2019-11-16 02:17:42.079879 Epoch [18/50], Step[2000/8619], Loss: 3.8541, Perplexity: 47.19\n",
      "2019-11-16 02:19:59.145212 Epoch [18/50], Step[3000/8619], Loss: 3.6622, Perplexity: 38.95\n",
      "2019-11-16 02:22:14.730917 Epoch [18/50], Step[4000/8619], Loss: 3.5641, Perplexity: 35.31\n",
      "2019-11-16 02:24:28.458664 Epoch [18/50], Step[5000/8619], Loss: 3.8138, Perplexity: 45.32\n",
      "2019-11-16 02:26:43.865068 Epoch [18/50], Step[6000/8619], Loss: 3.8823, Perplexity: 48.54\n",
      "2019-11-16 02:29:01.484282 Epoch [18/50], Step[7000/8619], Loss: 3.9126, Perplexity: 50.03\n",
      "2019-11-16 02:31:20.597719 Epoch [18/50], Step[8000/8619], Loss: 3.9805, Perplexity: 53.55\n",
      "2019-11-16 02:32:49.149907 Epoch [19/50], Step[0/8619], Loss: 3.8559, Perplexity: 47.27\n",
      "2019-11-16 02:35:03.846437 Epoch [19/50], Step[1000/8619], Loss: 3.6248, Perplexity: 37.52\n",
      "2019-11-16 02:37:20.101373 Epoch [19/50], Step[2000/8619], Loss: 3.8429, Perplexity: 46.66\n",
      "2019-11-16 02:39:38.732474 Epoch [19/50], Step[3000/8619], Loss: 3.6417, Perplexity: 38.16\n",
      "2019-11-16 02:41:55.184575 Epoch [19/50], Step[4000/8619], Loss: 3.5615, Perplexity: 35.22\n",
      "2019-11-16 02:44:15.735150 Epoch [19/50], Step[5000/8619], Loss: 3.7911, Perplexity: 44.31\n",
      "2019-11-16 02:46:39.664003 Epoch [19/50], Step[6000/8619], Loss: 3.8846, Perplexity: 48.65\n",
      "2019-11-16 02:48:56.242778 Epoch [19/50], Step[7000/8619], Loss: 3.8920, Perplexity: 49.01\n",
      "2019-11-16 02:51:16.679273 Epoch [19/50], Step[8000/8619], Loss: 3.9834, Perplexity: 53.70\n",
      "2019-11-16 02:52:40.101641 Epoch [20/50], Step[0/8619], Loss: 3.8569, Perplexity: 47.32\n",
      "2019-11-16 02:54:59.234317 Epoch [20/50], Step[1000/8619], Loss: 3.6388, Perplexity: 38.05\n",
      "2019-11-16 02:57:14.656905 Epoch [20/50], Step[2000/8619], Loss: 3.8506, Perplexity: 47.02\n",
      "2019-11-16 02:59:30.452918 Epoch [20/50], Step[3000/8619], Loss: 3.6132, Perplexity: 37.08\n",
      "2019-11-16 03:01:51.066072 Epoch [20/50], Step[4000/8619], Loss: 3.5278, Perplexity: 34.05\n",
      "2019-11-16 03:04:12.481065 Epoch [20/50], Step[5000/8619], Loss: 3.7908, Perplexity: 44.29\n",
      "2019-11-16 03:06:28.676360 Epoch [20/50], Step[6000/8619], Loss: 3.9119, Perplexity: 49.99\n",
      "2019-11-16 03:08:45.920134 Epoch [20/50], Step[7000/8619], Loss: 3.9194, Perplexity: 50.37\n",
      "2019-11-16 03:11:02.724449 Epoch [20/50], Step[8000/8619], Loss: 3.9994, Perplexity: 54.56\n",
      "2019-11-16 03:12:28.787301 Epoch [21/50], Step[0/8619], Loss: 3.8482, Perplexity: 46.91\n",
      "2019-11-16 03:14:48.931643 Epoch [21/50], Step[1000/8619], Loss: 3.6356, Perplexity: 37.92\n",
      "2019-11-16 03:17:06.988462 Epoch [21/50], Step[2000/8619], Loss: 3.8428, Perplexity: 46.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-16 03:19:27.798505 Epoch [21/50], Step[3000/8619], Loss: 3.6500, Perplexity: 38.48\n",
      "2019-11-16 03:21:43.420483 Epoch [21/50], Step[4000/8619], Loss: 3.5356, Perplexity: 34.32\n",
      "2019-11-16 03:23:58.810785 Epoch [21/50], Step[5000/8619], Loss: 3.8046, Perplexity: 44.91\n",
      "2019-11-16 03:26:17.208803 Epoch [21/50], Step[6000/8619], Loss: 3.8892, Perplexity: 48.87\n",
      "2019-11-16 03:28:37.925157 Epoch [21/50], Step[7000/8619], Loss: 3.8983, Perplexity: 49.32\n",
      "2019-11-16 03:30:51.912391 Epoch [21/50], Step[8000/8619], Loss: 3.9737, Perplexity: 53.18\n",
      "2019-11-16 03:32:18.529066 Epoch [22/50], Step[0/8619], Loss: 3.8400, Perplexity: 46.52\n",
      "2019-11-16 03:34:35.666008 Epoch [22/50], Step[1000/8619], Loss: 3.6326, Perplexity: 37.81\n",
      "2019-11-16 03:36:55.298365 Epoch [22/50], Step[2000/8619], Loss: 3.8390, Perplexity: 46.48\n",
      "2019-11-16 03:39:11.786274 Epoch [22/50], Step[3000/8619], Loss: 3.6508, Perplexity: 38.51\n",
      "2019-11-16 03:41:29.846537 Epoch [22/50], Step[4000/8619], Loss: 3.5209, Perplexity: 33.82\n",
      "2019-11-16 03:43:50.641681 Epoch [22/50], Step[5000/8619], Loss: 3.7867, Perplexity: 44.11\n",
      "2019-11-16 03:46:06.172630 Epoch [22/50], Step[6000/8619], Loss: 3.9021, Perplexity: 49.51\n",
      "2019-11-16 03:48:25.880465 Epoch [22/50], Step[7000/8619], Loss: 3.8928, Perplexity: 49.05\n",
      "2019-11-16 03:50:44.834933 Epoch [22/50], Step[8000/8619], Loss: 3.9899, Perplexity: 54.05\n",
      "2019-11-16 03:52:08.632582 Epoch [23/50], Step[0/8619], Loss: 3.8265, Perplexity: 45.90\n",
      "2019-11-16 03:54:23.022830 Epoch [23/50], Step[1000/8619], Loss: 3.6324, Perplexity: 37.80\n",
      "2019-11-16 03:56:41.152156 Epoch [23/50], Step[2000/8619], Loss: 3.8457, Perplexity: 46.79\n",
      "2019-11-16 03:59:01.247142 Epoch [23/50], Step[3000/8619], Loss: 3.6285, Perplexity: 37.66\n",
      "2019-11-16 04:01:18.624665 Epoch [23/50], Step[4000/8619], Loss: 3.5358, Perplexity: 34.32\n",
      "2019-11-16 04:03:32.491849 Epoch [23/50], Step[5000/8619], Loss: 3.8073, Perplexity: 45.03\n",
      "2019-11-16 04:05:55.380613 Epoch [23/50], Step[6000/8619], Loss: 3.8819, Perplexity: 48.52\n",
      "2019-11-16 04:08:15.296382 Epoch [23/50], Step[7000/8619], Loss: 3.8889, Perplexity: 48.86\n",
      "2019-11-16 04:10:31.782578 Epoch [23/50], Step[8000/8619], Loss: 3.9698, Perplexity: 52.97\n",
      "2019-11-16 04:11:55.817400 Epoch [24/50], Step[0/8619], Loss: 3.8288, Perplexity: 46.01\n",
      "2019-11-16 04:14:11.985046 Epoch [24/50], Step[1000/8619], Loss: 3.6304, Perplexity: 37.73\n",
      "2019-11-16 04:16:32.914237 Epoch [24/50], Step[2000/8619], Loss: 3.8289, Perplexity: 46.01\n",
      "2019-11-16 04:18:46.218351 Epoch [24/50], Step[3000/8619], Loss: 3.6357, Perplexity: 37.93\n",
      "2019-11-16 04:21:05.651881 Epoch [24/50], Step[4000/8619], Loss: 3.5002, Perplexity: 33.12\n",
      "2019-11-16 04:23:24.974442 Epoch [24/50], Step[5000/8619], Loss: 3.8118, Perplexity: 45.23\n",
      "2019-11-16 04:25:40.129895 Epoch [24/50], Step[6000/8619], Loss: 3.8899, Perplexity: 48.90\n",
      "2019-11-16 04:27:57.598624 Epoch [24/50], Step[7000/8619], Loss: 3.8948, Perplexity: 49.14\n",
      "2019-11-16 04:30:13.635163 Epoch [24/50], Step[8000/8619], Loss: 3.9856, Perplexity: 53.82\n",
      "2019-11-16 04:31:38.333343 Epoch [25/50], Step[0/8619], Loss: 3.8481, Perplexity: 46.91\n",
      "2019-11-16 04:33:55.690529 Epoch [25/50], Step[1000/8619], Loss: 3.6332, Perplexity: 37.83\n",
      "2019-11-16 04:36:11.098574 Epoch [25/50], Step[2000/8619], Loss: 3.8508, Perplexity: 47.03\n",
      "2019-11-16 04:38:28.313836 Epoch [25/50], Step[3000/8619], Loss: 3.6495, Perplexity: 38.46\n",
      "2019-11-16 04:40:46.146184 Epoch [25/50], Step[4000/8619], Loss: 3.5224, Perplexity: 33.86\n",
      "2019-11-16 04:43:04.842021 Epoch [25/50], Step[5000/8619], Loss: 3.7906, Perplexity: 44.28\n",
      "2019-11-16 04:45:21.845866 Epoch [25/50], Step[6000/8619], Loss: 3.8730, Perplexity: 48.09\n",
      "2019-11-16 04:47:37.564842 Epoch [25/50], Step[7000/8619], Loss: 3.8985, Perplexity: 49.33\n",
      "2019-11-16 04:49:53.396756 Epoch [25/50], Step[8000/8619], Loss: 3.9893, Perplexity: 54.01\n",
      "2019-11-16 04:51:17.318122 Epoch [26/50], Step[0/8619], Loss: 3.8415, Perplexity: 46.60\n",
      "2019-11-16 04:53:30.222631 Epoch [26/50], Step[1000/8619], Loss: 3.6073, Perplexity: 36.87\n",
      "2019-11-16 04:55:45.669445 Epoch [26/50], Step[2000/8619], Loss: 3.8481, Perplexity: 46.90\n",
      "2019-11-16 04:58:00.581399 Epoch [26/50], Step[3000/8619], Loss: 3.6674, Perplexity: 39.15\n",
      "2019-11-16 05:00:17.331083 Epoch [26/50], Step[4000/8619], Loss: 3.5175, Perplexity: 33.70\n",
      "2019-11-16 05:02:30.374558 Epoch [26/50], Step[5000/8619], Loss: 3.8069, Perplexity: 45.01\n",
      "2019-11-16 05:04:44.853451 Epoch [26/50], Step[6000/8619], Loss: 3.8798, Perplexity: 48.41\n",
      "2019-11-16 05:07:00.592454 Epoch [26/50], Step[7000/8619], Loss: 3.8827, Perplexity: 48.56\n",
      "2019-11-16 05:09:15.556583 Epoch [26/50], Step[8000/8619], Loss: 3.9588, Perplexity: 52.40\n",
      "2019-11-16 05:10:41.974553 Epoch [27/50], Step[0/8619], Loss: 3.8257, Perplexity: 45.86\n",
      "2019-11-16 05:13:00.868429 Epoch [27/50], Step[1000/8619], Loss: 3.6297, Perplexity: 37.70\n",
      "2019-11-16 05:15:22.334849 Epoch [27/50], Step[2000/8619], Loss: 3.8312, Perplexity: 46.12\n",
      "2019-11-16 05:17:38.004935 Epoch [27/50], Step[3000/8619], Loss: 3.6386, Perplexity: 38.04\n",
      "2019-11-16 05:19:53.556750 Epoch [27/50], Step[4000/8619], Loss: 3.4978, Perplexity: 33.04\n",
      "2019-11-16 05:22:09.653080 Epoch [27/50], Step[5000/8619], Loss: 3.7947, Perplexity: 44.47\n",
      "2019-11-16 05:24:24.401965 Epoch [27/50], Step[6000/8619], Loss: 3.8329, Perplexity: 46.20\n",
      "2019-11-16 05:26:45.797348 Epoch [27/50], Step[7000/8619], Loss: 3.8707, Perplexity: 47.98\n",
      "2019-11-16 05:28:59.779536 Epoch [27/50], Step[8000/8619], Loss: 3.9649, Perplexity: 52.71\n",
      "2019-11-16 05:30:23.300239 Epoch [28/50], Step[0/8619], Loss: 3.7943, Perplexity: 44.45\n",
      "2019-11-16 05:32:40.032382 Epoch [28/50], Step[1000/8619], Loss: 3.5935, Perplexity: 36.36\n",
      "2019-11-16 05:34:56.750323 Epoch [28/50], Step[2000/8619], Loss: 3.8394, Perplexity: 46.50\n",
      "2019-11-16 05:37:13.894835 Epoch [28/50], Step[3000/8619], Loss: 3.6460, Perplexity: 38.32\n",
      "2019-11-16 05:39:29.151169 Epoch [28/50], Step[4000/8619], Loss: 3.4919, Perplexity: 32.85\n",
      "2019-11-16 05:41:47.247353 Epoch [28/50], Step[5000/8619], Loss: 3.8311, Perplexity: 46.11\n",
      "2019-11-16 05:44:06.114787 Epoch [28/50], Step[6000/8619], Loss: 3.8493, Perplexity: 46.96\n",
      "2019-11-16 05:46:27.085021 Epoch [28/50], Step[7000/8619], Loss: 3.8583, Perplexity: 47.38\n",
      "2019-11-16 05:48:46.653587 Epoch [28/50], Step[8000/8619], Loss: 3.9592, Perplexity: 52.42\n",
      "2019-11-16 05:50:11.099352 Epoch [29/50], Step[0/8619], Loss: 3.8117, Perplexity: 45.23\n",
      "2019-11-16 05:52:25.344836 Epoch [29/50], Step[1000/8619], Loss: 3.6213, Perplexity: 37.39\n",
      "2019-11-16 05:54:43.500865 Epoch [29/50], Step[2000/8619], Loss: 3.8429, Perplexity: 46.66\n",
      "2019-11-16 05:57:02.570352 Epoch [29/50], Step[3000/8619], Loss: 3.6640, Perplexity: 39.02\n",
      "2019-11-16 05:59:17.828535 Epoch [29/50], Step[4000/8619], Loss: 3.5175, Perplexity: 33.70\n",
      "2019-11-16 06:01:31.650677 Epoch [29/50], Step[5000/8619], Loss: 3.8158, Perplexity: 45.41\n",
      "2019-11-16 06:03:48.925192 Epoch [29/50], Step[6000/8619], Loss: 3.8778, Perplexity: 48.32\n",
      "2019-11-16 06:06:08.774975 Epoch [29/50], Step[7000/8619], Loss: 3.8403, Perplexity: 46.54\n",
      "2019-11-16 06:08:24.937776 Epoch [29/50], Step[8000/8619], Loss: 3.9929, Perplexity: 54.21\n",
      "2019-11-16 06:09:51.886082 Epoch [30/50], Step[0/8619], Loss: 3.7947, Perplexity: 44.46\n",
      "2019-11-16 06:12:10.193184 Epoch [30/50], Step[1000/8619], Loss: 3.6266, Perplexity: 37.58\n",
      "2019-11-16 06:14:29.497489 Epoch [30/50], Step[2000/8619], Loss: 3.8330, Perplexity: 46.20\n",
      "2019-11-16 06:16:44.071130 Epoch [30/50], Step[3000/8619], Loss: 3.6437, Perplexity: 38.23\n",
      "2019-11-16 06:19:01.520533 Epoch [30/50], Step[4000/8619], Loss: 3.4748, Perplexity: 32.29\n",
      "2019-11-16 06:21:20.365610 Epoch [30/50], Step[5000/8619], Loss: 3.8465, Perplexity: 46.83\n",
      "2019-11-16 06:23:36.959754 Epoch [30/50], Step[6000/8619], Loss: 3.8592, Perplexity: 47.43\n",
      "2019-11-16 06:25:51.921813 Epoch [30/50], Step[7000/8619], Loss: 3.8588, Perplexity: 47.41\n",
      "2019-11-16 06:28:07.856962 Epoch [30/50], Step[8000/8619], Loss: 4.0067, Perplexity: 54.96\n",
      "2019-11-16 06:29:31.120562 Epoch [31/50], Step[0/8619], Loss: 3.7803, Perplexity: 43.83\n",
      "2019-11-16 06:31:48.979340 Epoch [31/50], Step[1000/8619], Loss: 3.6154, Perplexity: 37.17\n",
      "2019-11-16 06:34:08.667098 Epoch [31/50], Step[2000/8619], Loss: 3.8260, Perplexity: 45.88\n",
      "2019-11-16 06:36:25.624971 Epoch [31/50], Step[3000/8619], Loss: 3.6608, Perplexity: 38.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-16 06:38:49.960637 Epoch [31/50], Step[4000/8619], Loss: 3.4803, Perplexity: 32.47\n",
      "2019-11-16 06:41:10.279762 Epoch [31/50], Step[5000/8619], Loss: 3.7953, Perplexity: 44.49\n",
      "2019-11-16 06:43:26.082049 Epoch [31/50], Step[6000/8619], Loss: 3.8846, Perplexity: 48.65\n",
      "2019-11-16 06:45:42.763559 Epoch [31/50], Step[7000/8619], Loss: 3.8498, Perplexity: 46.98\n",
      "2019-11-16 06:48:01.530795 Epoch [31/50], Step[8000/8619], Loss: 3.9838, Perplexity: 53.72\n",
      "2019-11-16 06:49:31.990876 Epoch [32/50], Step[0/8619], Loss: 3.8078, Perplexity: 45.05\n",
      "2019-11-16 06:51:51.706573 Epoch [32/50], Step[1000/8619], Loss: 3.6077, Perplexity: 36.88\n",
      "2019-11-16 06:54:12.156310 Epoch [32/50], Step[2000/8619], Loss: 3.8347, Perplexity: 46.28\n",
      "2019-11-16 06:56:28.764737 Epoch [32/50], Step[3000/8619], Loss: 3.6356, Perplexity: 37.92\n",
      "2019-11-16 06:58:44.462308 Epoch [32/50], Step[4000/8619], Loss: 3.4901, Perplexity: 32.79\n",
      "2019-11-16 07:00:59.735360 Epoch [32/50], Step[5000/8619], Loss: 3.8108, Perplexity: 45.19\n",
      "2019-11-16 07:03:15.559290 Epoch [32/50], Step[6000/8619], Loss: 3.8924, Perplexity: 49.03\n",
      "2019-11-16 07:05:33.703432 Epoch [32/50], Step[7000/8619], Loss: 3.8218, Perplexity: 45.69\n",
      "2019-11-16 07:07:51.295096 Epoch [32/50], Step[8000/8619], Loss: 3.9692, Perplexity: 52.94\n",
      "2019-11-16 07:09:16.088930 Epoch [33/50], Step[0/8619], Loss: 3.7790, Perplexity: 43.77\n",
      "2019-11-16 07:11:31.603247 Epoch [33/50], Step[1000/8619], Loss: 3.6240, Perplexity: 37.49\n",
      "2019-11-16 07:13:50.526570 Epoch [33/50], Step[2000/8619], Loss: 3.8322, Perplexity: 46.16\n",
      "2019-11-16 07:16:08.711515 Epoch [33/50], Step[3000/8619], Loss: 3.6359, Perplexity: 37.94\n",
      "2019-11-16 07:18:32.947992 Epoch [33/50], Step[4000/8619], Loss: 3.4873, Perplexity: 32.70\n",
      "2019-11-16 07:20:52.922202 Epoch [33/50], Step[5000/8619], Loss: 3.8205, Perplexity: 45.63\n",
      "2019-11-16 07:23:10.064016 Epoch [33/50], Step[6000/8619], Loss: 3.8850, Perplexity: 48.67\n",
      "2019-11-16 07:25:25.111497 Epoch [33/50], Step[7000/8619], Loss: 3.8327, Perplexity: 46.19\n",
      "2019-11-16 07:27:41.435795 Epoch [33/50], Step[8000/8619], Loss: 3.9747, Perplexity: 53.23\n",
      "2019-11-16 07:29:06.413673 Epoch [34/50], Step[0/8619], Loss: 3.7892, Perplexity: 44.22\n",
      "2019-11-16 07:31:28.452442 Epoch [34/50], Step[1000/8619], Loss: 3.6123, Perplexity: 37.05\n",
      "2019-11-16 07:33:46.360664 Epoch [34/50], Step[2000/8619], Loss: 3.8321, Perplexity: 46.16\n",
      "2019-11-16 07:36:03.623511 Epoch [34/50], Step[3000/8619], Loss: 3.6068, Perplexity: 36.85\n",
      "2019-11-16 07:38:20.941377 Epoch [34/50], Step[4000/8619], Loss: 3.4856, Perplexity: 32.64\n",
      "2019-11-16 07:40:38.632771 Epoch [34/50], Step[5000/8619], Loss: 3.8187, Perplexity: 45.54\n",
      "2019-11-16 07:42:58.761836 Epoch [34/50], Step[6000/8619], Loss: 3.8607, Perplexity: 47.50\n",
      "2019-11-16 07:45:17.641063 Epoch [34/50], Step[7000/8619], Loss: 3.8205, Perplexity: 45.63\n",
      "2019-11-16 07:47:32.688356 Epoch [34/50], Step[8000/8619], Loss: 3.9756, Perplexity: 53.28\n",
      "2019-11-16 07:48:56.625291 Epoch [35/50], Step[0/8619], Loss: 3.7939, Perplexity: 44.43\n",
      "2019-11-16 07:51:14.025766 Epoch [35/50], Step[1000/8619], Loss: 3.5975, Perplexity: 36.51\n",
      "2019-11-16 07:53:35.130017 Epoch [35/50], Step[2000/8619], Loss: 3.8497, Perplexity: 46.98\n",
      "2019-11-16 07:55:58.250938 Epoch [35/50], Step[3000/8619], Loss: 3.6122, Perplexity: 37.05\n",
      "2019-11-16 07:58:17.473211 Epoch [35/50], Step[4000/8619], Loss: 3.5367, Perplexity: 34.35\n",
      "2019-11-16 08:00:35.536784 Epoch [35/50], Step[5000/8619], Loss: 3.8062, Perplexity: 44.98\n",
      "2019-11-16 08:02:51.223748 Epoch [35/50], Step[6000/8619], Loss: 3.8805, Perplexity: 48.45\n",
      "2019-11-16 08:05:08.715485 Epoch [35/50], Step[7000/8619], Loss: 3.8072, Perplexity: 45.02\n",
      "2019-11-16 08:07:25.045522 Epoch [35/50], Step[8000/8619], Loss: 3.9670, Perplexity: 52.83\n",
      "2019-11-16 08:08:49.992890 Epoch [36/50], Step[0/8619], Loss: 3.7786, Perplexity: 43.75\n",
      "2019-11-16 08:11:07.178346 Epoch [36/50], Step[1000/8619], Loss: 3.6091, Perplexity: 36.93\n",
      "2019-11-16 08:13:22.318200 Epoch [36/50], Step[2000/8619], Loss: 3.8307, Perplexity: 46.09\n",
      "2019-11-16 08:15:40.779127 Epoch [36/50], Step[3000/8619], Loss: 3.5999, Perplexity: 36.59\n",
      "2019-11-16 08:17:57.658405 Epoch [36/50], Step[4000/8619], Loss: 3.5229, Perplexity: 33.88\n",
      "2019-11-16 08:20:15.193615 Epoch [36/50], Step[5000/8619], Loss: 3.7934, Perplexity: 44.41\n",
      "2019-11-16 08:22:31.001129 Epoch [36/50], Step[6000/8619], Loss: 3.8630, Perplexity: 47.61\n",
      "2019-11-16 08:24:47.684568 Epoch [36/50], Step[7000/8619], Loss: 3.8188, Perplexity: 45.55\n",
      "2019-11-16 08:27:05.475287 Epoch [36/50], Step[8000/8619], Loss: 3.9858, Perplexity: 53.83\n",
      "2019-11-16 08:28:29.415705 Epoch [37/50], Step[0/8619], Loss: 3.7907, Perplexity: 44.29\n",
      "2019-11-16 08:30:46.878926 Epoch [37/50], Step[1000/8619], Loss: 3.6099, Perplexity: 36.96\n",
      "2019-11-16 08:33:07.282958 Epoch [37/50], Step[2000/8619], Loss: 3.8305, Perplexity: 46.09\n",
      "2019-11-16 08:35:27.039666 Epoch [37/50], Step[3000/8619], Loss: 3.6221, Perplexity: 37.42\n",
      "2019-11-16 08:37:44.892004 Epoch [37/50], Step[4000/8619], Loss: 3.5253, Perplexity: 33.96\n",
      "2019-11-16 08:40:05.363971 Epoch [37/50], Step[5000/8619], Loss: 3.8202, Perplexity: 45.61\n",
      "2019-11-16 08:42:23.884506 Epoch [37/50], Step[6000/8619], Loss: 3.8875, Perplexity: 48.79\n",
      "2019-11-16 08:44:42.059989 Epoch [37/50], Step[7000/8619], Loss: 3.8232, Perplexity: 45.75\n",
      "2019-11-16 08:47:02.953352 Epoch [37/50], Step[8000/8619], Loss: 3.9477, Perplexity: 51.81\n",
      "2019-11-16 08:48:26.006974 Epoch [38/50], Step[0/8619], Loss: 3.7822, Perplexity: 43.91\n",
      "2019-11-16 08:50:45.525884 Epoch [38/50], Step[1000/8619], Loss: 3.5844, Perplexity: 36.03\n",
      "2019-11-16 08:53:00.399751 Epoch [38/50], Step[2000/8619], Loss: 3.8204, Perplexity: 45.62\n",
      "2019-11-16 08:55:21.614533 Epoch [38/50], Step[3000/8619], Loss: 3.6186, Perplexity: 37.29\n",
      "2019-11-16 08:57:38.924143 Epoch [38/50], Step[4000/8619], Loss: 3.5167, Perplexity: 33.67\n",
      "2019-11-16 09:00:00.053249 Epoch [38/50], Step[5000/8619], Loss: 3.7978, Perplexity: 44.60\n",
      "2019-11-16 09:02:19.193848 Epoch [38/50], Step[6000/8619], Loss: 3.8552, Perplexity: 47.24\n",
      "2019-11-16 09:04:41.988066 Epoch [38/50], Step[7000/8619], Loss: 3.8215, Perplexity: 45.67\n",
      "2019-11-16 09:07:02.731524 Epoch [38/50], Step[8000/8619], Loss: 3.9417, Perplexity: 51.50\n",
      "2019-11-16 09:08:26.317104 Epoch [39/50], Step[0/8619], Loss: 3.7846, Perplexity: 44.02\n",
      "2019-11-16 09:10:47.026828 Epoch [39/50], Step[1000/8619], Loss: 3.6124, Perplexity: 37.05\n",
      "2019-11-16 09:13:02.356316 Epoch [39/50], Step[2000/8619], Loss: 3.8149, Perplexity: 45.37\n",
      "2019-11-16 09:15:22.019295 Epoch [39/50], Step[3000/8619], Loss: 3.6547, Perplexity: 38.66\n",
      "2019-11-16 09:17:37.118160 Epoch [39/50], Step[4000/8619], Loss: 3.4925, Perplexity: 32.87\n",
      "2019-11-16 09:19:57.510228 Epoch [39/50], Step[5000/8619], Loss: 3.8067, Perplexity: 45.00\n",
      "2019-11-16 09:22:16.817701 Epoch [39/50], Step[6000/8619], Loss: 3.8756, Perplexity: 48.21\n",
      "2019-11-16 09:24:36.539555 Epoch [39/50], Step[7000/8619], Loss: 3.8259, Perplexity: 45.87\n",
      "2019-11-16 09:26:58.732580 Epoch [39/50], Step[8000/8619], Loss: 4.0039, Perplexity: 54.81\n",
      "2019-11-16 09:28:24.914005 Epoch [40/50], Step[0/8619], Loss: 3.7738, Perplexity: 43.55\n",
      "2019-11-16 09:30:40.407270 Epoch [40/50], Step[1000/8619], Loss: 3.6181, Perplexity: 37.27\n",
      "2019-11-16 09:33:01.209228 Epoch [40/50], Step[2000/8619], Loss: 3.8132, Perplexity: 45.30\n",
      "2019-11-16 09:35:25.549201 Epoch [40/50], Step[3000/8619], Loss: 3.6264, Perplexity: 37.58\n",
      "2019-11-16 09:37:46.204186 Epoch [40/50], Step[4000/8619], Loss: 3.5089, Perplexity: 33.41\n",
      "2019-11-16 09:40:06.988845 Epoch [40/50], Step[5000/8619], Loss: 3.8173, Perplexity: 45.48\n",
      "2019-11-16 09:42:27.372173 Epoch [40/50], Step[6000/8619], Loss: 3.8740, Perplexity: 48.13\n",
      "2019-11-16 09:44:50.291257 Epoch [40/50], Step[7000/8619], Loss: 3.8469, Perplexity: 46.85\n",
      "2019-11-16 09:47:08.402277 Epoch [40/50], Step[8000/8619], Loss: 3.9779, Perplexity: 53.41\n",
      "2019-11-16 09:48:31.670143 Epoch [41/50], Step[0/8619], Loss: 3.7739, Perplexity: 43.55\n",
      "2019-11-16 09:50:51.226336 Epoch [41/50], Step[1000/8619], Loss: 3.6090, Perplexity: 36.93\n",
      "2019-11-16 09:53:11.543445 Epoch [41/50], Step[2000/8619], Loss: 3.8127, Perplexity: 45.27\n",
      "2019-11-16 09:55:32.546978 Epoch [41/50], Step[3000/8619], Loss: 3.6072, Perplexity: 36.86\n",
      "2019-11-16 09:57:46.995469 Epoch [41/50], Step[4000/8619], Loss: 3.5087, Perplexity: 33.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-16 10:00:07.374116 Epoch [41/50], Step[5000/8619], Loss: 3.7959, Perplexity: 44.52\n",
      "2019-11-16 10:02:22.991627 Epoch [41/50], Step[6000/8619], Loss: 3.8502, Perplexity: 47.00\n",
      "2019-11-16 10:04:38.438537 Epoch [41/50], Step[7000/8619], Loss: 3.8319, Perplexity: 46.15\n",
      "2019-11-16 10:06:58.481431 Epoch [41/50], Step[8000/8619], Loss: 3.9722, Perplexity: 53.10\n",
      "2019-11-16 10:08:26.418991 Epoch [42/50], Step[0/8619], Loss: 3.7645, Perplexity: 43.14\n",
      "2019-11-16 10:10:41.495993 Epoch [42/50], Step[1000/8619], Loss: 3.6258, Perplexity: 37.56\n",
      "2019-11-16 10:12:56.292149 Epoch [42/50], Step[2000/8619], Loss: 3.7981, Perplexity: 44.61\n",
      "2019-11-16 10:15:14.561289 Epoch [42/50], Step[3000/8619], Loss: 3.6217, Perplexity: 37.40\n",
      "2019-11-16 10:17:32.155865 Epoch [42/50], Step[4000/8619], Loss: 3.5082, Perplexity: 33.39\n",
      "2019-11-16 10:19:51.597347 Epoch [42/50], Step[5000/8619], Loss: 3.7891, Perplexity: 44.22\n",
      "2019-11-16 10:22:08.431375 Epoch [42/50], Step[6000/8619], Loss: 3.8843, Perplexity: 48.63\n",
      "2019-11-16 10:24:25.017675 Epoch [42/50], Step[7000/8619], Loss: 3.8218, Perplexity: 45.69\n",
      "2019-11-16 10:26:38.791312 Epoch [42/50], Step[8000/8619], Loss: 3.9724, Perplexity: 53.11\n",
      "2019-11-16 10:28:02.285928 Epoch [43/50], Step[0/8619], Loss: 3.7674, Perplexity: 43.27\n",
      "2019-11-16 10:30:17.993563 Epoch [43/50], Step[1000/8619], Loss: 3.6068, Perplexity: 36.85\n",
      "2019-11-16 10:32:35.836598 Epoch [43/50], Step[2000/8619], Loss: 3.8084, Perplexity: 45.08\n",
      "2019-11-16 10:34:51.355843 Epoch [43/50], Step[3000/8619], Loss: 3.6440, Perplexity: 38.25\n",
      "2019-11-16 10:37:05.992026 Epoch [43/50], Step[4000/8619], Loss: 3.5028, Perplexity: 33.21\n",
      "2019-11-16 10:39:25.927216 Epoch [43/50], Step[5000/8619], Loss: 3.8095, Perplexity: 45.13\n",
      "2019-11-16 10:41:44.173948 Epoch [43/50], Step[6000/8619], Loss: 3.8375, Perplexity: 46.41\n",
      "2019-11-16 10:44:00.564156 Epoch [43/50], Step[7000/8619], Loss: 3.8149, Perplexity: 45.37\n",
      "2019-11-16 10:46:15.342669 Epoch [43/50], Step[8000/8619], Loss: 3.9706, Perplexity: 53.02\n",
      "2019-11-16 10:47:37.867135 Epoch [44/50], Step[0/8619], Loss: 3.7736, Perplexity: 43.53\n",
      "2019-11-16 10:49:55.156841 Epoch [44/50], Step[1000/8619], Loss: 3.6220, Perplexity: 37.41\n",
      "2019-11-16 10:52:15.760068 Epoch [44/50], Step[2000/8619], Loss: 3.7952, Perplexity: 44.48\n",
      "2019-11-16 10:54:32.438143 Epoch [44/50], Step[3000/8619], Loss: 3.5855, Perplexity: 36.07\n",
      "2019-11-16 10:56:48.437094 Epoch [44/50], Step[4000/8619], Loss: 3.4824, Perplexity: 32.54\n",
      "2019-11-16 10:59:10.737094 Epoch [44/50], Step[5000/8619], Loss: 3.7597, Perplexity: 42.93\n",
      "2019-11-16 11:01:30.996625 Epoch [44/50], Step[6000/8619], Loss: 3.8458, Perplexity: 46.80\n",
      "2019-11-16 11:03:47.637171 Epoch [44/50], Step[7000/8619], Loss: 3.7923, Perplexity: 44.36\n",
      "2019-11-16 11:06:10.129499 Epoch [44/50], Step[8000/8619], Loss: 3.9724, Perplexity: 53.11\n",
      "2019-11-16 11:07:35.895462 Epoch [45/50], Step[0/8619], Loss: 3.7720, Perplexity: 43.47\n",
      "2019-11-16 11:09:51.296136 Epoch [45/50], Step[1000/8619], Loss: 3.6196, Perplexity: 37.32\n",
      "2019-11-16 11:12:07.283438 Epoch [45/50], Step[2000/8619], Loss: 3.8131, Perplexity: 45.29\n",
      "2019-11-16 11:14:29.058260 Epoch [45/50], Step[3000/8619], Loss: 3.6139, Perplexity: 37.11\n",
      "2019-11-16 11:16:53.431166 Epoch [45/50], Step[4000/8619], Loss: 3.5089, Perplexity: 33.41\n",
      "2019-11-16 11:19:15.235175 Epoch [45/50], Step[5000/8619], Loss: 3.7917, Perplexity: 44.33\n",
      "2019-11-16 11:21:33.928212 Epoch [45/50], Step[6000/8619], Loss: 3.8539, Perplexity: 47.17\n",
      "2019-11-16 11:23:54.084086 Epoch [45/50], Step[7000/8619], Loss: 3.7866, Perplexity: 44.11\n",
      "2019-11-16 11:26:12.757686 Epoch [45/50], Step[8000/8619], Loss: 3.9811, Perplexity: 53.57\n",
      "2019-11-16 11:27:38.283939 Epoch [46/50], Step[0/8619], Loss: 3.7446, Perplexity: 42.29\n",
      "2019-11-16 11:29:57.613011 Epoch [46/50], Step[1000/8619], Loss: 3.6182, Perplexity: 37.27\n",
      "2019-11-16 11:32:12.182626 Epoch [46/50], Step[2000/8619], Loss: 3.7995, Perplexity: 44.68\n",
      "2019-11-16 11:34:30.506737 Epoch [46/50], Step[3000/8619], Loss: 3.6176, Perplexity: 37.25\n",
      "2019-11-16 11:36:51.445050 Epoch [46/50], Step[4000/8619], Loss: 3.4950, Perplexity: 32.95\n",
      "2019-11-16 11:39:12.793214 Epoch [46/50], Step[5000/8619], Loss: 3.7945, Perplexity: 44.45\n",
      "2019-11-16 11:41:35.476271 Epoch [46/50], Step[6000/8619], Loss: 3.8632, Perplexity: 47.62\n",
      "2019-11-16 11:43:56.886049 Epoch [46/50], Step[7000/8619], Loss: 3.8302, Perplexity: 46.07\n",
      "2019-11-16 11:46:15.782774 Epoch [46/50], Step[8000/8619], Loss: 3.9765, Perplexity: 53.33\n",
      "2019-11-16 11:47:40.382236 Epoch [47/50], Step[0/8619], Loss: 3.7416, Perplexity: 42.16\n",
      "2019-11-16 11:50:02.083717 Epoch [47/50], Step[1000/8619], Loss: 3.5982, Perplexity: 36.53\n",
      "2019-11-16 11:52:20.574121 Epoch [47/50], Step[2000/8619], Loss: 3.7867, Perplexity: 44.11\n",
      "2019-11-16 11:54:38.189781 Epoch [47/50], Step[3000/8619], Loss: 3.6323, Perplexity: 37.80\n",
      "2019-11-16 11:56:56.205651 Epoch [47/50], Step[4000/8619], Loss: 3.4789, Perplexity: 32.43\n",
      "2019-11-16 11:59:17.900376 Epoch [47/50], Step[5000/8619], Loss: 3.8069, Perplexity: 45.01\n",
      "2019-11-16 12:01:32.655902 Epoch [47/50], Step[6000/8619], Loss: 3.8698, Perplexity: 47.93\n",
      "2019-11-16 12:03:52.803999 Epoch [47/50], Step[7000/8619], Loss: 3.8632, Perplexity: 47.62\n",
      "2019-11-16 12:06:12.764617 Epoch [47/50], Step[8000/8619], Loss: 3.9565, Perplexity: 52.28\n",
      "2019-11-16 12:07:37.414354 Epoch [48/50], Step[0/8619], Loss: 3.7604, Perplexity: 42.97\n",
      "2019-11-16 12:09:51.325849 Epoch [48/50], Step[1000/8619], Loss: 3.6064, Perplexity: 36.83\n",
      "2019-11-16 12:12:07.987201 Epoch [48/50], Step[2000/8619], Loss: 3.7838, Perplexity: 43.98\n",
      "2019-11-16 12:14:31.273740 Epoch [48/50], Step[3000/8619], Loss: 3.6149, Perplexity: 37.15\n",
      "2019-11-16 12:16:46.199343 Epoch [48/50], Step[4000/8619], Loss: 3.4610, Perplexity: 31.85\n",
      "2019-11-16 12:19:01.535930 Epoch [48/50], Step[5000/8619], Loss: 3.8258, Perplexity: 45.87\n",
      "2019-11-16 12:21:18.560984 Epoch [48/50], Step[6000/8619], Loss: 3.8590, Perplexity: 47.42\n",
      "2019-11-16 12:23:33.710069 Epoch [48/50], Step[7000/8619], Loss: 3.8439, Perplexity: 46.71\n",
      "2019-11-16 12:25:53.965196 Epoch [48/50], Step[8000/8619], Loss: 3.9432, Perplexity: 51.58\n",
      "2019-11-16 12:27:20.688488 Epoch [49/50], Step[0/8619], Loss: 3.7442, Perplexity: 42.27\n",
      "2019-11-16 12:29:42.330589 Epoch [49/50], Step[1000/8619], Loss: 3.6090, Perplexity: 36.93\n",
      "2019-11-16 12:32:04.179284 Epoch [49/50], Step[2000/8619], Loss: 3.8330, Perplexity: 46.20\n",
      "2019-11-16 12:34:18.536190 Epoch [49/50], Step[3000/8619], Loss: 3.6473, Perplexity: 38.37\n",
      "2019-11-16 12:36:33.751311 Epoch [49/50], Step[4000/8619], Loss: 3.4747, Perplexity: 32.29\n",
      "2019-11-16 12:38:52.153216 Epoch [49/50], Step[5000/8619], Loss: 3.8113, Perplexity: 45.21\n",
      "2019-11-16 12:41:11.138241 Epoch [49/50], Step[6000/8619], Loss: 3.8150, Perplexity: 45.38\n",
      "2019-11-16 12:43:31.463034 Epoch [49/50], Step[7000/8619], Loss: 3.7998, Perplexity: 44.69\n",
      "2019-11-16 12:45:51.623376 Epoch [49/50], Step[8000/8619], Loss: 3.9505, Perplexity: 51.96\n",
      "2019-11-16 12:47:16.580150 Epoch [50/50], Step[0/8619], Loss: 3.7705, Perplexity: 43.40\n",
      "2019-11-16 12:49:31.553489 Epoch [50/50], Step[1000/8619], Loss: 3.6316, Perplexity: 37.77\n",
      "2019-11-16 12:51:48.362734 Epoch [50/50], Step[2000/8619], Loss: 3.8119, Perplexity: 45.24\n",
      "2019-11-16 12:54:04.280808 Epoch [50/50], Step[3000/8619], Loss: 3.6033, Perplexity: 36.72\n",
      "2019-11-16 12:56:20.186259 Epoch [50/50], Step[4000/8619], Loss: 3.4816, Perplexity: 32.51\n",
      "2019-11-16 12:58:38.545083 Epoch [50/50], Step[5000/8619], Loss: 3.7845, Perplexity: 44.02\n",
      "2019-11-16 13:00:55.922931 Epoch [50/50], Step[6000/8619], Loss: 3.8543, Perplexity: 47.20\n",
      "2019-11-16 13:03:13.682957 Epoch [50/50], Step[7000/8619], Loss: 3.8276, Perplexity: 45.95\n",
      "2019-11-16 13:05:30.556571 Epoch [50/50], Step[8000/8619], Loss: 3.9298, Perplexity: 50.90\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    state = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Get images\n",
    "        images = np.zeros((inputs.size(0), inputs.size(1), 32, 32, 1))\n",
    "        for b, seq in enumerate(ids):\n",
    "            for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "                images[b, s] = np.load(f'char_img/noto_CJK/msr/{idx}.npy').reshape(32,32,1)\n",
    "        images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "        images = images.view(-1, 32, 32, 1) # B*N H W C\n",
    "        images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "        \n",
    "        # Get encoded images\n",
    "        cnn_o = cnn_encoder(images)\n",
    "        cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "               \n",
    "        # Forward pass\n",
    "        state = detach(state)\n",
    "        outputs, state = model(inputs, cnn_o, state)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 1000 == 0:\n",
    "            print ('{} Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(datetime.now(), epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perplexity: 43.0105865161436\n"
     ]
    }
   ],
   "source": [
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = ids[:, i:i+seq_length].to(device)\n",
    "    targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Get images\n",
    "    images = np.zeros((inputs.size(0), inputs.size(1), 32, 32, 1))\n",
    "    for b, seq in enumerate(ids):\n",
    "        for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "            images[b, s] = np.load(f'char_img/noto_CJK/msr/{idx}.npy').reshape(32,32,1)\n",
    "    images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "    images = images.view(-1, 32, 32, 1) # B*N H W C\n",
    "    images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "\n",
    "    # Get encoded images\n",
    "    cnn_o = cnn_encoder(images)\n",
    "    cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(inputs, cnn_o, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "    # Backward and optimize\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    \n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Train Perplexity: {perplexity / num_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = corpus.get_data('icwb2-data/testing/msr_test.utf8', batch_size)\n",
    "# filter out unknown character\n",
    "test_ids = test_ids.view(-1)\n",
    "mask = test_ids < vocab_size\n",
    "test_ids = test_ids[mask]\n",
    "num_batches = test_ids.size(0) // batch_size\n",
    "test_ids = test_ids[:num_batches*batch_size]\n",
    "test_ids = test_ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 50.7453561503389\n"
     ]
    }
   ],
   "source": [
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = test_ids[:, i:i+seq_length].to(device)\n",
    "    targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Get images\n",
    "    images = np.zeros((inputs.size(0), inputs.size(1), 32, 32, 1))\n",
    "    for b, seq in enumerate(test_ids):\n",
    "        for s, idx in enumerate(test_ids[b][i:i+seq_length]):\n",
    "            images[b, s] = np.load(f'char_img/noto_CJK/msr/{idx}.npy').reshape(32,32,1)\n",
    "    images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "    images = images.view(-1, 32, 32, 1) # B*N H W C\n",
    "    images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "\n",
    "    # Get encoded images\n",
    "    cnn_o = cnn_encoder(images)\n",
    "    cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(inputs, cnn_o, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "    # Backward and optimize\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    \n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Test Perplexity: {perplexity / num_step}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch1.2)",
   "language": "python",
   "name": "pytorch1.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
