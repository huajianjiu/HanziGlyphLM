{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renderer\n",
    "from PIL import ImageFont\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "char_size = 24\n",
    "# char render\n",
    "def render(text, font=None):\n",
    "    if font is None:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\", char_size)\n",
    "    mask = font.getmask(text)\n",
    "    size = mask.size[::-1]\n",
    "    a = np.asarray(mask).reshape(size) / 255\n",
    "    res = cv2.resize(a, dsize=(char_size, char_size), interpolation=cv2.INTER_CUBIC)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model/data_utils.py\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, max_size=None):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 1\n",
    "        self.word2idx['⸘'] = 0 # as unk\n",
    "        self.idx2word[0] = '⸘'\n",
    "        self.max_size = max_size + 1\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx and self.idx < self.max_size:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, max_size=None):\n",
    "        self.dictionary = Dictionary(max_size=max_size)\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "#         with open(path, 'r') as f:\n",
    "#             tokens = 0\n",
    "#             for line in f:\n",
    "#                 words = line.split() + ['<eos>']\n",
    "#                 tokens += len(words)\n",
    "#                 for word in words: \n",
    "#                     self.dictionary.add_word(word)  \n",
    "\n",
    "        # split words to char and add to dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                tokens += len(chars)\n",
    "                for char in chars:\n",
    "                    self.dictionary.add_word(char)\n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                for char in chars:\n",
    "                    if char in self.dictionary.word2idx:\n",
    "                        ids[token] = self.dictionary.word2idx[char]\n",
    "                        token += 1\n",
    "                    else:\n",
    "                        ids[token] = self.dictionary.word2idx['⸘']\n",
    "                        token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, char_id, char_cnn_o, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(char_id) + char_cnn_o\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, h = self.gru(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)\n",
    "\"\"\"\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    if type(stride) is not tuple:\n",
    "        stride = (stride, stride)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride[0]) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride[1]) + 1)\n",
    "    return h, w\n",
    "\n",
    "# Dai et al. 's CNN glyph encoder\n",
    "class Dai_CNN(nn.Module):\n",
    "    def __init__(self, embed_size, input_size=(24, 24)):\n",
    "        super(Dai_CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, (7, 7), stride=(2,2))\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.zeros_(self.conv1.bias)\n",
    "        h, w = conv_output_shape(input_size, (7, 7), (2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 16, (5, 5), stride=(2,2))\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        torch.nn.init.zeros_(self.conv2.bias)\n",
    "        h, w = conv_output_shape((h, w), (5, 5), (2, 2))\n",
    "                \n",
    "        self.fc = nn.Linear(16*h*w, embed_size)\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        torch.nn.init.zeros_(self.fc.bias)\n",
    "        \n",
    "        self.h, self.w = h, w\n",
    "        \n",
    "    def forward(self, char_img):\n",
    "        b = char_img.size(0)\n",
    "        x = self.conv1(char_img)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = x.view(-1, 16*self.h*self.w)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "embed_size = 300\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "seq_length = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Load dataset\n",
    "corpus = Corpus(max_size=4000)\n",
    "ids = corpus.get_data('icwb2-data/training/msr_training.utf8', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save char images for reference\n",
    "for char, idx in corpus.dictionary.word2idx.items():\n",
    "    np.save(f'char_img/noto_CJK/msr3/{idx}.npy', render(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "cnn_encoder = Dai_CNN(embed_size, input_size=(char_size, char_size)).to(device)\n",
    "model.train()\n",
    "cnn_encoder.train()\n",
    "params = list(model.parameters())+list(cnn_encoder.parameters())\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# Truncated backpropagation\n",
    "def detach(state):\n",
    "    return state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-25 16:34:54.712081 Epoch [1/50], Step[0/8080], Loss: 8.3154, Perplexity: 4086.51\n",
      "2019-11-25 16:39:45.120695 Epoch [1/50], Step[1000/8080], Loss: 5.1071, Perplexity: 165.19\n",
      "2019-11-25 16:44:40.094791 Epoch [1/50], Step[2000/8080], Loss: 4.5968, Perplexity: 99.17\n",
      "2019-11-25 16:50:47.388058 Epoch [1/50], Step[3000/8080], Loss: 4.4296, Perplexity: 83.89\n",
      "2019-11-25 16:56:59.091131 Epoch [1/50], Step[4000/8080], Loss: 4.2543, Perplexity: 70.41\n",
      "2019-11-25 17:02:48.925007 Epoch [1/50], Step[5000/8080], Loss: 4.7234, Perplexity: 112.55\n",
      "2019-11-25 17:08:18.938422 Epoch [1/50], Step[6000/8080], Loss: 4.5673, Perplexity: 96.28\n",
      "2019-11-25 17:14:35.499657 Epoch [1/50], Step[7000/8080], Loss: 4.1168, Perplexity: 61.36\n",
      "2019-11-25 17:20:52.902418 Epoch [1/50], Step[8000/8080], Loss: 4.3748, Perplexity: 79.43\n",
      "2019-11-25 17:21:23.394436 Epoch [2/50], Step[0/8080], Loss: 4.2921, Perplexity: 73.12\n",
      "2019-11-25 17:27:44.810220 Epoch [2/50], Step[1000/8080], Loss: 4.1367, Perplexity: 62.59\n",
      "2019-11-25 17:34:07.458108 Epoch [2/50], Step[2000/8080], Loss: 4.1313, Perplexity: 62.26\n",
      "2019-11-25 17:41:25.594810 Epoch [2/50], Step[3000/8080], Loss: 4.0992, Perplexity: 60.29\n",
      "2019-11-25 17:48:32.543164 Epoch [2/50], Step[4000/8080], Loss: 3.9450, Perplexity: 51.68\n",
      "2019-11-25 17:55:30.160132 Epoch [2/50], Step[5000/8080], Loss: 4.5464, Perplexity: 94.30\n",
      "2019-11-25 18:02:36.385288 Epoch [2/50], Step[6000/8080], Loss: 4.3222, Perplexity: 75.36\n",
      "2019-11-25 18:09:42.488988 Epoch [2/50], Step[7000/8080], Loss: 3.9421, Perplexity: 51.53\n",
      "2019-11-25 18:16:41.662463 Epoch [2/50], Step[8000/8080], Loss: 4.2407, Perplexity: 69.46\n",
      "2019-11-25 18:17:14.935032 Epoch [3/50], Step[0/8080], Loss: 4.1173, Perplexity: 61.39\n",
      "2019-11-25 18:24:16.953716 Epoch [3/50], Step[1000/8080], Loss: 3.9887, Perplexity: 53.99\n",
      "2019-11-25 18:31:20.047150 Epoch [3/50], Step[2000/8080], Loss: 4.0633, Perplexity: 58.17\n",
      "2019-11-25 18:37:36.515831 Epoch [3/50], Step[3000/8080], Loss: 4.0126, Perplexity: 55.29\n",
      "2019-11-25 18:43:48.490664 Epoch [3/50], Step[4000/8080], Loss: 3.8281, Perplexity: 45.98\n",
      "2019-11-25 18:49:53.153667 Epoch [3/50], Step[5000/8080], Loss: 4.4413, Perplexity: 84.89\n",
      "2019-11-25 18:56:02.030457 Epoch [3/50], Step[6000/8080], Loss: 4.1975, Perplexity: 66.52\n",
      "2019-11-25 19:02:17.140306 Epoch [3/50], Step[7000/8080], Loss: 3.8605, Perplexity: 47.49\n",
      "2019-11-25 19:09:02.381235 Epoch [3/50], Step[8000/8080], Loss: 4.1469, Perplexity: 63.24\n",
      "2019-11-25 19:09:35.646752 Epoch [4/50], Step[0/8080], Loss: 4.0519, Perplexity: 57.51\n",
      "2019-11-25 19:16:26.471461 Epoch [4/50], Step[1000/8080], Loss: 3.9202, Perplexity: 50.41\n",
      "2019-11-25 19:23:09.008858 Epoch [4/50], Step[2000/8080], Loss: 4.0139, Perplexity: 55.36\n",
      "2019-11-25 19:29:32.634475 Epoch [4/50], Step[3000/8080], Loss: 3.9603, Perplexity: 52.47\n",
      "2019-11-25 19:35:41.611900 Epoch [4/50], Step[4000/8080], Loss: 3.7801, Perplexity: 43.82\n",
      "2019-11-25 19:41:55.858546 Epoch [4/50], Step[5000/8080], Loss: 4.3994, Perplexity: 81.40\n",
      "2019-11-25 19:48:07.871462 Epoch [4/50], Step[6000/8080], Loss: 4.1439, Perplexity: 63.05\n",
      "2019-11-25 19:54:16.356851 Epoch [4/50], Step[7000/8080], Loss: 3.8047, Perplexity: 44.91\n",
      "2019-11-25 20:00:35.125191 Epoch [4/50], Step[8000/8080], Loss: 4.0770, Perplexity: 58.97\n",
      "2019-11-25 20:01:04.570635 Epoch [5/50], Step[0/8080], Loss: 3.9990, Perplexity: 54.55\n",
      "2019-11-25 20:07:18.761377 Epoch [5/50], Step[1000/8080], Loss: 3.8681, Perplexity: 47.85\n",
      "2019-11-25 20:13:36.516275 Epoch [5/50], Step[2000/8080], Loss: 3.9881, Perplexity: 53.95\n",
      "2019-11-25 20:19:57.916274 Epoch [5/50], Step[3000/8080], Loss: 3.9433, Perplexity: 51.59\n",
      "2019-11-25 20:26:11.806936 Epoch [5/50], Step[4000/8080], Loss: 3.7154, Perplexity: 41.08\n",
      "2019-11-25 20:32:26.835730 Epoch [5/50], Step[5000/8080], Loss: 4.3919, Perplexity: 80.79\n",
      "2019-11-25 20:38:42.178169 Epoch [5/50], Step[6000/8080], Loss: 4.1135, Perplexity: 61.16\n",
      "2019-11-25 20:44:58.225542 Epoch [5/50], Step[7000/8080], Loss: 3.7633, Perplexity: 43.09\n",
      "2019-11-25 20:51:13.183137 Epoch [5/50], Step[8000/8080], Loss: 4.0484, Perplexity: 57.31\n",
      "2019-11-25 20:51:42.606765 Epoch [6/50], Step[0/8080], Loss: 3.9696, Perplexity: 52.96\n",
      "2019-11-25 20:57:55.955025 Epoch [6/50], Step[1000/8080], Loss: 3.8489, Perplexity: 46.94\n",
      "2019-11-25 21:04:04.561786 Epoch [6/50], Step[2000/8080], Loss: 3.9793, Perplexity: 53.48\n",
      "2019-11-25 21:10:15.846203 Epoch [6/50], Step[3000/8080], Loss: 3.9070, Perplexity: 49.75\n",
      "2019-11-25 21:16:32.156511 Epoch [6/50], Step[4000/8080], Loss: 3.7278, Perplexity: 41.59\n",
      "2019-11-25 21:22:44.926665 Epoch [6/50], Step[5000/8080], Loss: 4.3743, Perplexity: 79.38\n",
      "2019-11-25 21:29:08.876721 Epoch [6/50], Step[6000/8080], Loss: 4.0869, Perplexity: 59.56\n",
      "2019-11-25 21:35:31.171623 Epoch [6/50], Step[7000/8080], Loss: 3.7109, Perplexity: 40.89\n",
      "2019-11-25 21:42:13.758465 Epoch [6/50], Step[8000/8080], Loss: 4.0217, Perplexity: 55.79\n",
      "2019-11-25 21:42:44.113867 Epoch [7/50], Step[0/8080], Loss: 3.9470, Perplexity: 51.78\n",
      "2019-11-25 21:49:21.591421 Epoch [7/50], Step[1000/8080], Loss: 3.8525, Perplexity: 47.11\n",
      "2019-11-25 21:55:47.417383 Epoch [7/50], Step[2000/8080], Loss: 3.9630, Perplexity: 52.61\n",
      "2019-11-25 22:02:03.990222 Epoch [7/50], Step[3000/8080], Loss: 3.8916, Perplexity: 48.99\n",
      "2019-11-25 22:08:15.942145 Epoch [7/50], Step[4000/8080], Loss: 3.6942, Perplexity: 40.21\n",
      "2019-11-25 22:14:32.699011 Epoch [7/50], Step[5000/8080], Loss: 4.3567, Perplexity: 78.00\n",
      "2019-11-25 22:20:47.996305 Epoch [7/50], Step[6000/8080], Loss: 4.0485, Perplexity: 57.31\n",
      "2019-11-25 22:27:01.755562 Epoch [7/50], Step[7000/8080], Loss: 3.6999, Perplexity: 40.44\n",
      "2019-11-25 22:33:12.194881 Epoch [7/50], Step[8000/8080], Loss: 4.0167, Perplexity: 55.52\n",
      "2019-11-25 22:33:42.722569 Epoch [8/50], Step[0/8080], Loss: 3.9235, Perplexity: 50.58\n",
      "2019-11-25 22:39:51.414367 Epoch [8/50], Step[1000/8080], Loss: 3.8397, Perplexity: 46.51\n",
      "2019-11-25 22:46:21.409869 Epoch [8/50], Step[2000/8080], Loss: 3.9763, Perplexity: 53.32\n",
      "2019-11-25 22:53:12.166193 Epoch [8/50], Step[3000/8080], Loss: 3.8639, Perplexity: 47.65\n",
      "2019-11-25 23:00:12.180155 Epoch [8/50], Step[4000/8080], Loss: 3.6982, Perplexity: 40.38\n",
      "2019-11-25 23:07:10.480127 Epoch [8/50], Step[5000/8080], Loss: 4.3126, Perplexity: 74.64\n",
      "2019-11-25 23:14:02.868936 Epoch [8/50], Step[6000/8080], Loss: 4.0347, Perplexity: 56.53\n",
      "2019-11-25 23:20:59.423027 Epoch [8/50], Step[7000/8080], Loss: 3.7032, Perplexity: 40.58\n",
      "2019-11-25 23:27:49.612377 Epoch [8/50], Step[8000/8080], Loss: 4.0188, Perplexity: 55.63\n",
      "2019-11-25 23:28:22.291957 Epoch [9/50], Step[0/8080], Loss: 3.9013, Perplexity: 49.47\n",
      "2019-11-25 23:35:17.363940 Epoch [9/50], Step[1000/8080], Loss: 3.8232, Perplexity: 45.75\n",
      "2019-11-25 23:42:07.075236 Epoch [9/50], Step[2000/8080], Loss: 3.9534, Perplexity: 52.11\n",
      "2019-11-25 23:48:54.642328 Epoch [9/50], Step[3000/8080], Loss: 3.8402, Perplexity: 46.54\n",
      "2019-11-25 23:55:37.516055 Epoch [9/50], Step[4000/8080], Loss: 3.6811, Perplexity: 39.69\n",
      "2019-11-26 00:02:19.582565 Epoch [9/50], Step[5000/8080], Loss: 4.2905, Perplexity: 73.00\n",
      "2019-11-26 00:08:59.229014 Epoch [9/50], Step[6000/8080], Loss: 4.0285, Perplexity: 56.17\n",
      "2019-11-26 00:15:39.746029 Epoch [9/50], Step[7000/8080], Loss: 3.6898, Perplexity: 40.04\n",
      "2019-11-26 00:22:20.974792 Epoch [9/50], Step[8000/8080], Loss: 3.9977, Perplexity: 54.47\n",
      "2019-11-26 00:22:53.522552 Epoch [10/50], Step[0/8080], Loss: 3.8986, Perplexity: 49.34\n",
      "2019-11-26 00:29:33.746328 Epoch [10/50], Step[1000/8080], Loss: 3.7921, Perplexity: 44.35\n",
      "2019-11-26 00:36:13.462936 Epoch [10/50], Step[2000/8080], Loss: 3.9499, Perplexity: 51.93\n",
      "2019-11-26 00:42:51.809745 Epoch [10/50], Step[3000/8080], Loss: 3.8093, Perplexity: 45.12\n",
      "2019-11-26 00:49:30.398031 Epoch [10/50], Step[4000/8080], Loss: 3.6435, Perplexity: 38.22\n",
      "2019-11-26 00:56:17.570482 Epoch [10/50], Step[5000/8080], Loss: 4.2910, Perplexity: 73.04\n",
      "2019-11-26 01:02:57.497995 Epoch [10/50], Step[6000/8080], Loss: 3.9956, Perplexity: 54.36\n",
      "2019-11-26 01:09:39.030488 Epoch [10/50], Step[7000/8080], Loss: 3.6716, Perplexity: 39.32\n",
      "2019-11-26 01:16:21.119339 Epoch [10/50], Step[8000/8080], Loss: 4.0016, Perplexity: 54.69\n",
      "2019-11-26 01:16:53.488778 Epoch [11/50], Step[0/8080], Loss: 3.8877, Perplexity: 48.80\n",
      "2019-11-26 01:23:32.815041 Epoch [11/50], Step[1000/8080], Loss: 3.7476, Perplexity: 42.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 01:30:16.915658 Epoch [11/50], Step[2000/8080], Loss: 3.9425, Perplexity: 51.55\n",
      "2019-11-26 01:36:52.826100 Epoch [11/50], Step[3000/8080], Loss: 3.8018, Perplexity: 44.78\n",
      "2019-11-26 01:43:34.879967 Epoch [11/50], Step[4000/8080], Loss: 3.6738, Perplexity: 39.40\n",
      "2019-11-26 01:50:13.777066 Epoch [11/50], Step[5000/8080], Loss: 4.3004, Perplexity: 73.73\n",
      "2019-11-26 01:56:53.495094 Epoch [11/50], Step[6000/8080], Loss: 3.9745, Perplexity: 53.22\n",
      "2019-11-26 02:03:35.639254 Epoch [11/50], Step[7000/8080], Loss: 3.6711, Perplexity: 39.30\n",
      "2019-11-26 02:10:17.674630 Epoch [11/50], Step[8000/8080], Loss: 3.9831, Perplexity: 53.68\n",
      "2019-11-26 02:10:50.021813 Epoch [12/50], Step[0/8080], Loss: 3.8599, Perplexity: 47.46\n",
      "2019-11-26 02:17:31.057327 Epoch [12/50], Step[1000/8080], Loss: 3.7524, Perplexity: 42.62\n",
      "2019-11-26 02:24:13.188626 Epoch [12/50], Step[2000/8080], Loss: 3.9298, Perplexity: 50.90\n",
      "2019-11-26 02:30:53.917969 Epoch [12/50], Step[3000/8080], Loss: 3.7845, Perplexity: 44.01\n",
      "2019-11-26 02:37:36.051883 Epoch [12/50], Step[4000/8080], Loss: 3.6822, Perplexity: 39.73\n",
      "2019-11-26 02:44:15.788084 Epoch [12/50], Step[5000/8080], Loss: 4.2866, Perplexity: 72.72\n",
      "2019-11-26 02:50:56.162140 Epoch [12/50], Step[6000/8080], Loss: 3.9625, Perplexity: 52.59\n",
      "2019-11-26 02:57:37.846049 Epoch [12/50], Step[7000/8080], Loss: 3.6826, Perplexity: 39.75\n",
      "2019-11-26 03:04:17.864992 Epoch [12/50], Step[8000/8080], Loss: 3.9955, Perplexity: 54.36\n",
      "2019-11-26 03:04:49.874265 Epoch [13/50], Step[0/8080], Loss: 3.8479, Perplexity: 46.89\n",
      "2019-11-26 03:11:33.705034 Epoch [13/50], Step[1000/8080], Loss: 3.7303, Perplexity: 41.69\n",
      "2019-11-26 03:18:17.211669 Epoch [13/50], Step[2000/8080], Loss: 3.8904, Perplexity: 48.93\n",
      "2019-11-26 03:24:56.327315 Epoch [13/50], Step[3000/8080], Loss: 3.7603, Perplexity: 42.96\n",
      "2019-11-26 03:31:36.900186 Epoch [13/50], Step[4000/8080], Loss: 3.6787, Perplexity: 39.59\n",
      "2019-11-26 03:38:19.067810 Epoch [13/50], Step[5000/8080], Loss: 4.2868, Perplexity: 72.74\n",
      "2019-11-26 03:44:55.916281 Epoch [13/50], Step[6000/8080], Loss: 3.9582, Perplexity: 52.36\n",
      "2019-11-26 03:51:32.576066 Epoch [13/50], Step[7000/8080], Loss: 3.6507, Perplexity: 38.50\n",
      "2019-11-26 03:58:09.411054 Epoch [13/50], Step[8000/8080], Loss: 3.9967, Perplexity: 54.42\n",
      "2019-11-26 03:58:40.687284 Epoch [14/50], Step[0/8080], Loss: 3.8402, Perplexity: 46.54\n",
      "2019-11-26 04:05:21.150819 Epoch [14/50], Step[1000/8080], Loss: 3.7401, Perplexity: 42.10\n",
      "2019-11-26 04:12:01.999842 Epoch [14/50], Step[2000/8080], Loss: 3.9028, Perplexity: 49.54\n",
      "2019-11-26 04:18:42.945803 Epoch [14/50], Step[3000/8080], Loss: 3.7452, Perplexity: 42.32\n",
      "2019-11-26 04:25:22.246716 Epoch [14/50], Step[4000/8080], Loss: 3.6475, Perplexity: 38.38\n",
      "2019-11-26 04:32:03.379665 Epoch [14/50], Step[5000/8080], Loss: 4.3094, Perplexity: 74.40\n",
      "2019-11-26 04:38:44.182974 Epoch [14/50], Step[6000/8080], Loss: 3.9240, Perplexity: 50.60\n",
      "2019-11-26 04:45:24.283146 Epoch [14/50], Step[7000/8080], Loss: 3.6623, Perplexity: 38.95\n",
      "2019-11-26 04:52:11.989850 Epoch [14/50], Step[8000/8080], Loss: 3.9897, Perplexity: 54.04\n",
      "2019-11-26 04:52:43.821574 Epoch [15/50], Step[0/8080], Loss: 3.8206, Perplexity: 45.63\n",
      "2019-11-26 04:59:20.865970 Epoch [15/50], Step[1000/8080], Loss: 3.7286, Perplexity: 41.62\n",
      "2019-11-26 05:06:00.730863 Epoch [15/50], Step[2000/8080], Loss: 3.8921, Perplexity: 49.01\n",
      "2019-11-26 05:12:43.944114 Epoch [15/50], Step[3000/8080], Loss: 3.7320, Perplexity: 41.76\n",
      "2019-11-26 05:19:28.520491 Epoch [15/50], Step[4000/8080], Loss: 3.6574, Perplexity: 38.76\n",
      "2019-11-26 05:26:07.893954 Epoch [15/50], Step[5000/8080], Loss: 4.2687, Perplexity: 71.43\n",
      "2019-11-26 05:32:48.277412 Epoch [15/50], Step[6000/8080], Loss: 3.9161, Perplexity: 50.20\n",
      "2019-11-26 05:39:26.989136 Epoch [15/50], Step[7000/8080], Loss: 3.6813, Perplexity: 39.70\n",
      "2019-11-26 05:46:05.969471 Epoch [15/50], Step[8000/8080], Loss: 3.9603, Perplexity: 52.47\n",
      "2019-11-26 05:46:37.397415 Epoch [16/50], Step[0/8080], Loss: 3.8213, Perplexity: 45.66\n",
      "2019-11-26 05:53:16.473536 Epoch [16/50], Step[1000/8080], Loss: 3.7124, Perplexity: 40.95\n",
      "2019-11-26 05:59:55.120339 Epoch [16/50], Step[2000/8080], Loss: 3.9154, Perplexity: 50.17\n",
      "2019-11-26 06:06:32.145099 Epoch [16/50], Step[3000/8080], Loss: 3.7466, Perplexity: 42.38\n",
      "2019-11-26 06:13:11.983578 Epoch [16/50], Step[4000/8080], Loss: 3.6568, Perplexity: 38.74\n",
      "2019-11-26 06:19:50.092741 Epoch [16/50], Step[5000/8080], Loss: 4.2987, Perplexity: 73.61\n",
      "2019-11-26 06:26:28.731988 Epoch [16/50], Step[6000/8080], Loss: 3.9168, Perplexity: 50.24\n",
      "2019-11-26 06:33:10.693347 Epoch [16/50], Step[7000/8080], Loss: 3.6721, Perplexity: 39.33\n",
      "2019-11-26 06:39:51.072558 Epoch [16/50], Step[8000/8080], Loss: 3.9615, Perplexity: 52.53\n",
      "2019-11-26 06:40:22.986182 Epoch [17/50], Step[0/8080], Loss: 3.8096, Perplexity: 45.13\n",
      "2019-11-26 06:47:04.019165 Epoch [17/50], Step[1000/8080], Loss: 3.7238, Perplexity: 41.42\n",
      "2019-11-26 06:53:44.788528 Epoch [17/50], Step[2000/8080], Loss: 3.9166, Perplexity: 50.23\n",
      "2019-11-26 07:00:28.125538 Epoch [17/50], Step[3000/8080], Loss: 3.7230, Perplexity: 41.39\n",
      "2019-11-26 07:07:05.080852 Epoch [17/50], Step[4000/8080], Loss: 3.6638, Perplexity: 39.01\n",
      "2019-11-26 07:13:43.475581 Epoch [17/50], Step[5000/8080], Loss: 4.2799, Perplexity: 72.23\n",
      "2019-11-26 07:20:21.972033 Epoch [17/50], Step[6000/8080], Loss: 3.9167, Perplexity: 50.24\n",
      "2019-11-26 07:26:59.143378 Epoch [17/50], Step[7000/8080], Loss: 3.6700, Perplexity: 39.25\n",
      "2019-11-26 07:33:40.499329 Epoch [17/50], Step[8000/8080], Loss: 3.9515, Perplexity: 52.02\n",
      "2019-11-26 07:34:11.686437 Epoch [18/50], Step[0/8080], Loss: 3.7984, Perplexity: 44.63\n",
      "2019-11-26 07:40:49.846012 Epoch [18/50], Step[1000/8080], Loss: 3.7072, Perplexity: 40.74\n",
      "2019-11-26 07:47:32.964558 Epoch [18/50], Step[2000/8080], Loss: 3.9105, Perplexity: 49.92\n",
      "2019-11-26 07:54:13.275854 Epoch [18/50], Step[3000/8080], Loss: 3.7243, Perplexity: 41.44\n",
      "2019-11-26 08:00:55.602379 Epoch [18/50], Step[4000/8080], Loss: 3.6554, Perplexity: 38.68\n",
      "2019-11-26 08:07:33.510727 Epoch [18/50], Step[5000/8080], Loss: 4.2750, Perplexity: 71.88\n",
      "2019-11-26 08:14:13.759984 Epoch [18/50], Step[6000/8080], Loss: 3.8843, Perplexity: 48.63\n",
      "2019-11-26 08:20:50.289002 Epoch [18/50], Step[7000/8080], Loss: 3.6640, Perplexity: 39.02\n",
      "2019-11-26 08:27:35.621517 Epoch [18/50], Step[8000/8080], Loss: 3.9467, Perplexity: 51.77\n",
      "2019-11-26 08:28:08.090146 Epoch [19/50], Step[0/8080], Loss: 3.8008, Perplexity: 44.74\n",
      "2019-11-26 08:34:48.636136 Epoch [19/50], Step[1000/8080], Loss: 3.7027, Perplexity: 40.56\n",
      "2019-11-26 08:41:27.133137 Epoch [19/50], Step[2000/8080], Loss: 3.8925, Perplexity: 49.03\n",
      "2019-11-26 08:48:04.636994 Epoch [19/50], Step[3000/8080], Loss: 3.7391, Perplexity: 42.06\n",
      "2019-11-26 08:54:42.162116 Epoch [19/50], Step[4000/8080], Loss: 3.6841, Perplexity: 39.81\n",
      "2019-11-26 09:01:25.011009 Epoch [19/50], Step[5000/8080], Loss: 4.2972, Perplexity: 73.50\n",
      "2019-11-26 09:08:03.359321 Epoch [19/50], Step[6000/8080], Loss: 3.8815, Perplexity: 48.50\n",
      "2019-11-26 09:14:41.425968 Epoch [19/50], Step[7000/8080], Loss: 3.6857, Perplexity: 39.87\n",
      "2019-11-26 09:21:25.644248 Epoch [19/50], Step[8000/8080], Loss: 3.9282, Perplexity: 50.81\n",
      "2019-11-26 09:21:57.572671 Epoch [20/50], Step[0/8080], Loss: 3.7942, Perplexity: 44.44\n",
      "2019-11-26 09:28:40.555698 Epoch [20/50], Step[1000/8080], Loss: 3.6777, Perplexity: 39.55\n",
      "2019-11-26 09:35:20.265570 Epoch [20/50], Step[2000/8080], Loss: 3.8735, Perplexity: 48.11\n",
      "2019-11-26 09:42:00.781392 Epoch [20/50], Step[3000/8080], Loss: 3.7448, Perplexity: 42.30\n",
      "2019-11-26 09:48:40.153152 Epoch [20/50], Step[4000/8080], Loss: 3.6615, Perplexity: 38.92\n",
      "2019-11-26 09:55:21.798251 Epoch [20/50], Step[5000/8080], Loss: 4.2894, Perplexity: 72.92\n",
      "2019-11-26 10:02:03.069317 Epoch [20/50], Step[6000/8080], Loss: 3.8788, Perplexity: 48.36\n",
      "2019-11-26 10:08:45.011171 Epoch [20/50], Step[7000/8080], Loss: 3.6594, Perplexity: 38.84\n",
      "2019-11-26 10:15:26.205933 Epoch [20/50], Step[8000/8080], Loss: 3.9224, Perplexity: 50.52\n",
      "2019-11-26 10:15:57.799987 Epoch [21/50], Step[0/8080], Loss: 3.7860, Perplexity: 44.08\n",
      "2019-11-26 10:22:36.473268 Epoch [21/50], Step[1000/8080], Loss: 3.6708, Perplexity: 39.28\n",
      "2019-11-26 10:29:17.035990 Epoch [21/50], Step[2000/8080], Loss: 3.8853, Perplexity: 48.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 10:35:55.389299 Epoch [21/50], Step[3000/8080], Loss: 3.7121, Perplexity: 40.94\n",
      "2019-11-26 10:42:33.039552 Epoch [21/50], Step[4000/8080], Loss: 3.6738, Perplexity: 39.40\n",
      "2019-11-26 10:49:13.183945 Epoch [21/50], Step[5000/8080], Loss: 4.2801, Perplexity: 72.24\n",
      "2019-11-26 10:55:54.588817 Epoch [21/50], Step[6000/8080], Loss: 3.8493, Perplexity: 46.96\n",
      "2019-11-26 11:02:35.853341 Epoch [21/50], Step[7000/8080], Loss: 3.6576, Perplexity: 38.77\n",
      "2019-11-26 11:09:16.094941 Epoch [21/50], Step[8000/8080], Loss: 3.9380, Perplexity: 51.31\n",
      "2019-11-26 11:09:48.166277 Epoch [22/50], Step[0/8080], Loss: 3.7851, Perplexity: 44.04\n",
      "2019-11-26 11:16:28.177199 Epoch [22/50], Step[1000/8080], Loss: 3.6933, Perplexity: 40.18\n",
      "2019-11-26 11:23:05.061838 Epoch [22/50], Step[2000/8080], Loss: 3.8816, Perplexity: 48.50\n",
      "2019-11-26 11:29:47.394436 Epoch [22/50], Step[3000/8080], Loss: 3.7296, Perplexity: 41.66\n",
      "2019-11-26 11:36:30.857320 Epoch [22/50], Step[4000/8080], Loss: 3.6752, Perplexity: 39.46\n",
      "2019-11-26 11:43:10.846832 Epoch [22/50], Step[5000/8080], Loss: 4.2813, Perplexity: 72.34\n",
      "2019-11-26 11:49:44.545080 Epoch [22/50], Step[6000/8080], Loss: 3.8534, Perplexity: 47.15\n",
      "2019-11-26 11:56:29.283478 Epoch [22/50], Step[7000/8080], Loss: 3.6645, Perplexity: 39.04\n",
      "2019-11-26 12:03:06.179863 Epoch [22/50], Step[8000/8080], Loss: 3.9286, Perplexity: 50.84\n",
      "2019-11-26 12:03:38.227174 Epoch [23/50], Step[0/8080], Loss: 3.7761, Perplexity: 43.65\n",
      "2019-11-26 12:10:19.147523 Epoch [23/50], Step[1000/8080], Loss: 3.6622, Perplexity: 38.95\n",
      "2019-11-26 12:16:58.280674 Epoch [23/50], Step[2000/8080], Loss: 3.8963, Perplexity: 49.22\n",
      "2019-11-26 12:23:43.213834 Epoch [23/50], Step[3000/8080], Loss: 3.7561, Perplexity: 42.78\n",
      "2019-11-26 12:30:21.841324 Epoch [23/50], Step[4000/8080], Loss: 3.6703, Perplexity: 39.26\n",
      "2019-11-26 12:37:03.190084 Epoch [23/50], Step[5000/8080], Loss: 4.2671, Perplexity: 71.31\n",
      "2019-11-26 12:43:44.346871 Epoch [23/50], Step[6000/8080], Loss: 3.8356, Perplexity: 46.32\n",
      "2019-11-26 12:50:28.733822 Epoch [23/50], Step[7000/8080], Loss: 3.6426, Perplexity: 38.19\n",
      "2019-11-26 12:57:12.026610 Epoch [23/50], Step[8000/8080], Loss: 3.9116, Perplexity: 49.98\n",
      "2019-11-26 12:57:44.313053 Epoch [24/50], Step[0/8080], Loss: 3.7685, Perplexity: 43.31\n",
      "2019-11-26 13:04:24.856332 Epoch [24/50], Step[1000/8080], Loss: 3.6761, Perplexity: 39.49\n",
      "2019-11-26 13:11:09.692384 Epoch [24/50], Step[2000/8080], Loss: 3.8835, Perplexity: 48.59\n",
      "2019-11-26 13:17:51.852114 Epoch [24/50], Step[3000/8080], Loss: 3.7404, Perplexity: 42.11\n",
      "2019-11-26 13:24:33.756133 Epoch [24/50], Step[4000/8080], Loss: 3.6576, Perplexity: 38.77\n",
      "2019-11-26 13:31:15.805143 Epoch [24/50], Step[5000/8080], Loss: 4.2838, Perplexity: 72.52\n",
      "2019-11-26 13:37:54.864473 Epoch [24/50], Step[6000/8080], Loss: 3.8655, Perplexity: 47.73\n",
      "2019-11-26 13:44:35.559973 Epoch [24/50], Step[7000/8080], Loss: 3.6299, Perplexity: 37.71\n",
      "2019-11-26 13:51:17.837968 Epoch [24/50], Step[8000/8080], Loss: 3.9455, Perplexity: 51.70\n",
      "2019-11-26 13:51:48.897172 Epoch [25/50], Step[0/8080], Loss: 3.7635, Perplexity: 43.10\n",
      "2019-11-26 13:58:29.881422 Epoch [25/50], Step[1000/8080], Loss: 3.6786, Perplexity: 39.59\n",
      "2019-11-26 14:05:10.930596 Epoch [25/50], Step[2000/8080], Loss: 3.8863, Perplexity: 48.73\n",
      "2019-11-26 14:11:52.316480 Epoch [25/50], Step[3000/8080], Loss: 3.7427, Perplexity: 42.21\n",
      "2019-11-26 14:18:35.000352 Epoch [25/50], Step[4000/8080], Loss: 3.6558, Perplexity: 38.70\n",
      "2019-11-26 14:25:16.830316 Epoch [25/50], Step[5000/8080], Loss: 4.2629, Perplexity: 71.02\n",
      "2019-11-26 14:31:56.874642 Epoch [25/50], Step[6000/8080], Loss: 3.8507, Perplexity: 47.03\n",
      "2019-11-26 14:38:43.183007 Epoch [25/50], Step[7000/8080], Loss: 3.6377, Perplexity: 38.01\n",
      "2019-11-26 14:45:22.268951 Epoch [25/50], Step[8000/8080], Loss: 3.9454, Perplexity: 51.70\n",
      "2019-11-26 14:45:55.323241 Epoch [26/50], Step[0/8080], Loss: 3.7677, Perplexity: 43.28\n",
      "2019-11-26 14:52:34.526307 Epoch [26/50], Step[1000/8080], Loss: 3.6808, Perplexity: 39.68\n",
      "2019-11-26 14:59:12.949946 Epoch [26/50], Step[2000/8080], Loss: 3.8766, Perplexity: 48.26\n",
      "2019-11-26 15:05:50.991979 Epoch [26/50], Step[3000/8080], Loss: 3.7325, Perplexity: 41.78\n",
      "2019-11-26 15:12:29.973335 Epoch [26/50], Step[4000/8080], Loss: 3.6602, Perplexity: 38.87\n",
      "2019-11-26 15:19:15.126952 Epoch [26/50], Step[5000/8080], Loss: 4.2645, Perplexity: 71.13\n",
      "2019-11-26 15:25:26.631675 Epoch [26/50], Step[6000/8080], Loss: 3.8552, Perplexity: 47.24\n",
      "2019-11-26 15:31:35.642366 Epoch [26/50], Step[7000/8080], Loss: 3.6469, Perplexity: 38.35\n",
      "2019-11-26 15:37:36.313093 Epoch [26/50], Step[8000/8080], Loss: 3.9241, Perplexity: 50.61\n",
      "2019-11-26 15:38:05.524327 Epoch [27/50], Step[0/8080], Loss: 3.7675, Perplexity: 43.27\n",
      "2019-11-26 15:44:17.536605 Epoch [27/50], Step[1000/8080], Loss: 3.6946, Perplexity: 40.23\n",
      "2019-11-26 15:50:29.392249 Epoch [27/50], Step[2000/8080], Loss: 3.8872, Perplexity: 48.78\n",
      "2019-11-26 15:56:37.370934 Epoch [27/50], Step[3000/8080], Loss: 3.7317, Perplexity: 41.75\n",
      "2019-11-26 16:02:48.516054 Epoch [27/50], Step[4000/8080], Loss: 3.6475, Perplexity: 38.38\n",
      "2019-11-26 16:08:54.168060 Epoch [27/50], Step[5000/8080], Loss: 4.2722, Perplexity: 71.68\n",
      "2019-11-26 16:15:07.705874 Epoch [27/50], Step[6000/8080], Loss: 3.8298, Perplexity: 46.05\n",
      "2019-11-26 16:21:15.124607 Epoch [27/50], Step[7000/8080], Loss: 3.6098, Perplexity: 36.96\n",
      "2019-11-26 16:27:25.882741 Epoch [27/50], Step[8000/8080], Loss: 3.9262, Perplexity: 50.71\n",
      "2019-11-26 16:27:55.953852 Epoch [28/50], Step[0/8080], Loss: 3.7781, Perplexity: 43.73\n",
      "2019-11-26 16:34:26.940403 Epoch [28/50], Step[1000/8080], Loss: 3.6830, Perplexity: 39.76\n",
      "2019-11-26 16:41:16.662562 Epoch [28/50], Step[2000/8080], Loss: 3.8709, Perplexity: 47.99\n",
      "2019-11-26 16:48:11.325299 Epoch [28/50], Step[3000/8080], Loss: 3.7543, Perplexity: 42.71\n",
      "2019-11-26 16:55:01.456234 Epoch [28/50], Step[4000/8080], Loss: 3.6433, Perplexity: 38.22\n",
      "2019-11-26 17:01:57.182579 Epoch [28/50], Step[5000/8080], Loss: 4.2595, Perplexity: 70.77\n",
      "2019-11-26 17:09:06.893240 Epoch [28/50], Step[6000/8080], Loss: 3.8548, Perplexity: 47.22\n",
      "2019-11-26 17:16:22.004753 Epoch [28/50], Step[7000/8080], Loss: 3.6072, Perplexity: 36.86\n",
      "2019-11-26 17:23:43.708704 Epoch [28/50], Step[8000/8080], Loss: 3.9480, Perplexity: 51.83\n",
      "2019-11-26 17:24:19.702480 Epoch [29/50], Step[0/8080], Loss: 3.7691, Perplexity: 43.34\n",
      "2019-11-26 17:31:32.282666 Epoch [29/50], Step[1000/8080], Loss: 3.6979, Perplexity: 40.36\n",
      "2019-11-26 17:38:46.427201 Epoch [29/50], Step[2000/8080], Loss: 3.8749, Perplexity: 48.18\n",
      "2019-11-26 17:45:39.630840 Epoch [29/50], Step[3000/8080], Loss: 3.7238, Perplexity: 41.42\n",
      "2019-11-26 17:52:40.183566 Epoch [29/50], Step[4000/8080], Loss: 3.6455, Perplexity: 38.30\n",
      "2019-11-26 17:59:52.044775 Epoch [29/50], Step[5000/8080], Loss: 4.2820, Perplexity: 72.38\n",
      "2019-11-26 18:06:53.874170 Epoch [29/50], Step[6000/8080], Loss: 3.8341, Perplexity: 46.25\n",
      "2019-11-26 18:13:50.126170 Epoch [29/50], Step[7000/8080], Loss: 3.6323, Perplexity: 37.80\n",
      "2019-11-26 18:20:56.193817 Epoch [29/50], Step[8000/8080], Loss: 3.9114, Perplexity: 49.97\n",
      "2019-11-26 18:21:30.479637 Epoch [30/50], Step[0/8080], Loss: 3.7898, Perplexity: 44.25\n",
      "2019-11-26 18:28:55.483370 Epoch [30/50], Step[1000/8080], Loss: 3.6834, Perplexity: 39.78\n",
      "2019-11-26 18:36:12.935845 Epoch [30/50], Step[2000/8080], Loss: 3.8716, Perplexity: 48.02\n",
      "2019-11-26 18:43:33.161345 Epoch [30/50], Step[3000/8080], Loss: 3.7422, Perplexity: 42.19\n",
      "2019-11-26 18:50:54.696196 Epoch [30/50], Step[4000/8080], Loss: 3.6570, Perplexity: 38.74\n",
      "2019-11-26 18:58:14.851386 Epoch [30/50], Step[5000/8080], Loss: 4.2754, Perplexity: 71.91\n",
      "2019-11-26 19:05:42.237338 Epoch [30/50], Step[6000/8080], Loss: 3.8203, Perplexity: 45.62\n",
      "2019-11-26 19:12:35.568250 Epoch [30/50], Step[7000/8080], Loss: 3.6006, Perplexity: 36.62\n",
      "2019-11-26 19:19:15.715643 Epoch [30/50], Step[8000/8080], Loss: 3.9392, Perplexity: 51.38\n",
      "2019-11-26 19:19:47.811616 Epoch [31/50], Step[0/8080], Loss: 3.7662, Perplexity: 43.21\n",
      "2019-11-26 19:26:30.394617 Epoch [31/50], Step[1000/8080], Loss: 3.6730, Perplexity: 39.37\n",
      "2019-11-26 19:33:23.736457 Epoch [31/50], Step[2000/8080], Loss: 3.8794, Perplexity: 48.39\n",
      "2019-11-26 19:40:13.351599 Epoch [31/50], Step[3000/8080], Loss: 3.7380, Perplexity: 42.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-26 19:47:01.003340 Epoch [31/50], Step[4000/8080], Loss: 3.6263, Perplexity: 37.58\n",
      "2019-11-26 19:53:45.440191 Epoch [31/50], Step[5000/8080], Loss: 4.2611, Perplexity: 70.89\n",
      "2019-11-26 20:00:33.066453 Epoch [31/50], Step[6000/8080], Loss: 3.8422, Perplexity: 46.63\n",
      "2019-11-26 20:07:20.864144 Epoch [31/50], Step[7000/8080], Loss: 3.6490, Perplexity: 38.44\n",
      "2019-11-26 20:14:08.269034 Epoch [31/50], Step[8000/8080], Loss: 3.9097, Perplexity: 49.88\n",
      "2019-11-26 20:14:40.858352 Epoch [32/50], Step[0/8080], Loss: 3.7812, Perplexity: 43.87\n",
      "2019-11-26 20:21:25.474911 Epoch [32/50], Step[1000/8080], Loss: 3.6765, Perplexity: 39.51\n",
      "2019-11-26 20:28:10.779424 Epoch [32/50], Step[2000/8080], Loss: 3.9093, Perplexity: 49.86\n",
      "2019-11-26 20:34:56.354798 Epoch [32/50], Step[3000/8080], Loss: 3.7224, Perplexity: 41.36\n",
      "2019-11-26 20:41:48.670083 Epoch [32/50], Step[4000/8080], Loss: 3.6395, Perplexity: 38.07\n",
      "2019-11-26 20:48:36.054948 Epoch [32/50], Step[5000/8080], Loss: 4.2555, Perplexity: 70.49\n",
      "2019-11-26 20:55:23.920452 Epoch [32/50], Step[6000/8080], Loss: 3.8333, Perplexity: 46.21\n",
      "2019-11-26 21:02:08.479041 Epoch [32/50], Step[7000/8080], Loss: 3.5909, Perplexity: 36.27\n",
      "2019-11-26 21:08:52.361364 Epoch [32/50], Step[8000/8080], Loss: 3.9216, Perplexity: 50.48\n",
      "2019-11-26 21:09:26.962684 Epoch [33/50], Step[0/8080], Loss: 3.7632, Perplexity: 43.09\n",
      "2019-11-26 21:16:15.212158 Epoch [33/50], Step[1000/8080], Loss: 3.6451, Perplexity: 38.29\n",
      "2019-11-26 21:23:02.482926 Epoch [33/50], Step[2000/8080], Loss: 3.9231, Perplexity: 50.56\n",
      "2019-11-26 21:29:48.548478 Epoch [33/50], Step[3000/8080], Loss: 3.7052, Perplexity: 40.66\n",
      "2019-11-26 21:36:35.319946 Epoch [33/50], Step[4000/8080], Loss: 3.6462, Perplexity: 38.33\n",
      "2019-11-26 21:43:25.824657 Epoch [33/50], Step[5000/8080], Loss: 4.2292, Perplexity: 68.66\n",
      "2019-11-26 21:50:11.992973 Epoch [33/50], Step[6000/8080], Loss: 3.8474, Perplexity: 46.87\n",
      "2019-11-26 21:56:54.996736 Epoch [33/50], Step[7000/8080], Loss: 3.5795, Perplexity: 35.85\n",
      "2019-11-26 22:03:41.927055 Epoch [33/50], Step[8000/8080], Loss: 3.9380, Perplexity: 51.32\n",
      "2019-11-26 22:04:15.107025 Epoch [34/50], Step[0/8080], Loss: 3.7710, Perplexity: 43.42\n",
      "2019-11-26 22:11:01.090383 Epoch [34/50], Step[1000/8080], Loss: 3.6644, Perplexity: 39.03\n",
      "2019-11-26 22:17:49.961481 Epoch [34/50], Step[2000/8080], Loss: 3.8835, Perplexity: 48.59\n",
      "2019-11-26 22:24:42.951801 Epoch [34/50], Step[3000/8080], Loss: 3.7388, Perplexity: 42.05\n",
      "2019-11-26 22:31:34.918769 Epoch [34/50], Step[4000/8080], Loss: 3.6332, Perplexity: 37.83\n",
      "2019-11-26 22:38:23.975109 Epoch [34/50], Step[5000/8080], Loss: 4.2747, Perplexity: 71.86\n",
      "2019-11-26 22:45:16.973312 Epoch [34/50], Step[6000/8080], Loss: 3.8335, Perplexity: 46.22\n",
      "2019-11-26 22:52:04.671630 Epoch [34/50], Step[7000/8080], Loss: 3.6016, Perplexity: 36.66\n",
      "2019-11-26 22:58:49.694047 Epoch [34/50], Step[8000/8080], Loss: 3.9472, Perplexity: 51.79\n",
      "2019-11-26 22:59:22.073566 Epoch [35/50], Step[0/8080], Loss: 3.7774, Perplexity: 43.70\n",
      "2019-11-26 23:06:14.503546 Epoch [35/50], Step[1000/8080], Loss: 3.6494, Perplexity: 38.45\n",
      "2019-11-26 23:13:04.859664 Epoch [35/50], Step[2000/8080], Loss: 3.8755, Perplexity: 48.21\n",
      "2019-11-26 23:19:53.466829 Epoch [35/50], Step[3000/8080], Loss: 3.7479, Perplexity: 42.43\n",
      "2019-11-26 23:26:45.211975 Epoch [35/50], Step[4000/8080], Loss: 3.6356, Perplexity: 37.92\n",
      "2019-11-26 23:33:34.293418 Epoch [35/50], Step[5000/8080], Loss: 4.2321, Perplexity: 68.86\n",
      "2019-11-26 23:40:21.915652 Epoch [35/50], Step[6000/8080], Loss: 3.8420, Perplexity: 46.62\n",
      "2019-11-26 23:47:13.615267 Epoch [35/50], Step[7000/8080], Loss: 3.6097, Perplexity: 36.95\n",
      "2019-11-26 23:53:57.938124 Epoch [35/50], Step[8000/8080], Loss: 3.9135, Perplexity: 50.08\n",
      "2019-11-26 23:54:30.199812 Epoch [36/50], Step[0/8080], Loss: 3.7818, Perplexity: 43.90\n",
      "2019-11-27 00:01:17.181011 Epoch [36/50], Step[1000/8080], Loss: 3.6415, Perplexity: 38.15\n",
      "2019-11-27 00:07:59.579598 Epoch [36/50], Step[2000/8080], Loss: 3.8970, Perplexity: 49.25\n",
      "2019-11-27 00:14:46.865692 Epoch [36/50], Step[3000/8080], Loss: 3.7204, Perplexity: 41.28\n",
      "2019-11-27 00:21:39.421688 Epoch [36/50], Step[4000/8080], Loss: 3.6687, Perplexity: 39.20\n",
      "2019-11-27 00:28:26.918226 Epoch [36/50], Step[5000/8080], Loss: 4.2340, Perplexity: 68.99\n",
      "2019-11-27 00:35:13.642006 Epoch [36/50], Step[6000/8080], Loss: 3.8528, Perplexity: 47.12\n",
      "2019-11-27 00:42:04.621437 Epoch [36/50], Step[7000/8080], Loss: 3.6035, Perplexity: 36.73\n",
      "2019-11-27 00:48:51.977327 Epoch [36/50], Step[8000/8080], Loss: 3.8924, Perplexity: 49.03\n",
      "2019-11-27 00:49:24.792256 Epoch [37/50], Step[0/8080], Loss: 3.7636, Perplexity: 43.10\n",
      "2019-11-27 00:56:13.957500 Epoch [37/50], Step[1000/8080], Loss: 3.6767, Perplexity: 39.51\n",
      "2019-11-27 01:03:05.665334 Epoch [37/50], Step[2000/8080], Loss: 3.8792, Perplexity: 48.38\n",
      "2019-11-27 01:09:56.859708 Epoch [37/50], Step[3000/8080], Loss: 3.7071, Perplexity: 40.74\n",
      "2019-11-27 01:16:43.300853 Epoch [37/50], Step[4000/8080], Loss: 3.6421, Perplexity: 38.17\n",
      "2019-11-27 01:23:32.688447 Epoch [37/50], Step[5000/8080], Loss: 4.2013, Perplexity: 66.77\n",
      "2019-11-27 01:30:18.782579 Epoch [37/50], Step[6000/8080], Loss: 3.8271, Perplexity: 45.93\n",
      "2019-11-27 01:37:08.952372 Epoch [37/50], Step[7000/8080], Loss: 3.6090, Perplexity: 36.93\n",
      "2019-11-27 01:43:57.464310 Epoch [37/50], Step[8000/8080], Loss: 3.9046, Perplexity: 49.63\n",
      "2019-11-27 01:44:30.341448 Epoch [38/50], Step[0/8080], Loss: 3.7905, Perplexity: 44.28\n",
      "2019-11-27 01:51:17.061536 Epoch [38/50], Step[1000/8080], Loss: 3.6258, Perplexity: 37.56\n",
      "2019-11-27 01:58:06.873317 Epoch [38/50], Step[2000/8080], Loss: 3.8791, Perplexity: 48.38\n",
      "2019-11-27 02:04:54.975884 Epoch [38/50], Step[3000/8080], Loss: 3.7205, Perplexity: 41.29\n",
      "2019-11-27 02:11:43.248039 Epoch [38/50], Step[4000/8080], Loss: 3.6787, Perplexity: 39.60\n",
      "2019-11-27 02:18:35.550118 Epoch [38/50], Step[5000/8080], Loss: 4.2308, Perplexity: 68.77\n",
      "2019-11-27 02:25:24.133154 Epoch [38/50], Step[6000/8080], Loss: 3.8332, Perplexity: 46.21\n",
      "2019-11-27 02:32:12.514057 Epoch [38/50], Step[7000/8080], Loss: 3.5637, Perplexity: 35.29\n",
      "2019-11-27 02:38:59.172505 Epoch [38/50], Step[8000/8080], Loss: 3.8819, Perplexity: 48.51\n",
      "2019-11-27 02:39:31.727707 Epoch [39/50], Step[0/8080], Loss: 3.7818, Perplexity: 43.89\n",
      "2019-11-27 02:46:21.483026 Epoch [39/50], Step[1000/8080], Loss: 3.6389, Perplexity: 38.05\n",
      "2019-11-27 02:53:07.859005 Epoch [39/50], Step[2000/8080], Loss: 3.8690, Perplexity: 47.90\n",
      "2019-11-27 02:59:52.036384 Epoch [39/50], Step[3000/8080], Loss: 3.7076, Perplexity: 40.76\n",
      "2019-11-27 03:06:41.413909 Epoch [39/50], Step[4000/8080], Loss: 3.6361, Perplexity: 37.94\n",
      "2019-11-27 03:13:26.737929 Epoch [39/50], Step[5000/8080], Loss: 4.2302, Perplexity: 68.73\n",
      "2019-11-27 03:20:13.446374 Epoch [39/50], Step[6000/8080], Loss: 3.8280, Perplexity: 45.97\n",
      "2019-11-27 03:27:00.786783 Epoch [39/50], Step[7000/8080], Loss: 3.5789, Perplexity: 35.83\n",
      "2019-11-27 03:33:47.372900 Epoch [39/50], Step[8000/8080], Loss: 3.9100, Perplexity: 49.90\n",
      "2019-11-27 03:34:19.868695 Epoch [40/50], Step[0/8080], Loss: 3.7786, Perplexity: 43.76\n",
      "2019-11-27 03:41:05.624270 Epoch [40/50], Step[1000/8080], Loss: 3.6277, Perplexity: 37.63\n",
      "2019-11-27 03:47:50.404034 Epoch [40/50], Step[2000/8080], Loss: 3.8742, Perplexity: 48.14\n",
      "2019-11-27 03:54:34.478785 Epoch [40/50], Step[3000/8080], Loss: 3.7212, Perplexity: 41.31\n",
      "2019-11-27 04:01:24.202304 Epoch [40/50], Step[4000/8080], Loss: 3.6275, Perplexity: 37.62\n",
      "2019-11-27 04:08:10.192478 Epoch [40/50], Step[5000/8080], Loss: 4.2282, Perplexity: 68.59\n",
      "2019-11-27 04:14:56.948350 Epoch [40/50], Step[6000/8080], Loss: 3.8907, Perplexity: 48.95\n",
      "2019-11-27 04:21:41.844109 Epoch [40/50], Step[7000/8080], Loss: 3.5724, Perplexity: 35.60\n",
      "2019-11-27 04:28:30.739484 Epoch [40/50], Step[8000/8080], Loss: 3.8918, Perplexity: 49.00\n",
      "2019-11-27 04:29:03.469622 Epoch [41/50], Step[0/8080], Loss: 3.7872, Perplexity: 44.13\n",
      "2019-11-27 04:35:52.335252 Epoch [41/50], Step[1000/8080], Loss: 3.6567, Perplexity: 38.73\n",
      "2019-11-27 04:42:41.392805 Epoch [41/50], Step[2000/8080], Loss: 3.9089, Perplexity: 49.85\n",
      "2019-11-27 04:49:33.697002 Epoch [41/50], Step[3000/8080], Loss: 3.6906, Perplexity: 40.07\n",
      "2019-11-27 04:56:21.774623 Epoch [41/50], Step[4000/8080], Loss: 3.6288, Perplexity: 37.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-27 05:03:13.927036 Epoch [41/50], Step[5000/8080], Loss: 4.2305, Perplexity: 68.75\n",
      "2019-11-27 05:09:58.084271 Epoch [41/50], Step[6000/8080], Loss: 3.8512, Perplexity: 47.05\n",
      "2019-11-27 05:16:33.892985 Epoch [41/50], Step[7000/8080], Loss: 3.5891, Perplexity: 36.20\n",
      "2019-11-27 05:23:08.676334 Epoch [41/50], Step[8000/8080], Loss: 3.9186, Perplexity: 50.33\n",
      "2019-11-27 05:23:40.199309 Epoch [42/50], Step[0/8080], Loss: 3.7590, Perplexity: 42.91\n",
      "2019-11-27 05:30:20.809142 Epoch [42/50], Step[1000/8080], Loss: 3.6503, Perplexity: 38.49\n",
      "2019-11-27 05:37:00.245113 Epoch [42/50], Step[2000/8080], Loss: 3.8934, Perplexity: 49.08\n",
      "2019-11-27 05:43:36.355901 Epoch [42/50], Step[3000/8080], Loss: 3.6952, Perplexity: 40.26\n",
      "2019-11-27 05:50:15.368253 Epoch [42/50], Step[4000/8080], Loss: 3.6649, Perplexity: 39.05\n",
      "2019-11-27 05:56:51.995562 Epoch [42/50], Step[5000/8080], Loss: 4.2587, Perplexity: 70.71\n",
      "2019-11-27 06:03:26.656049 Epoch [42/50], Step[6000/8080], Loss: 3.8684, Perplexity: 47.87\n",
      "2019-11-27 06:10:00.085416 Epoch [42/50], Step[7000/8080], Loss: 3.5848, Perplexity: 36.05\n",
      "2019-11-27 06:16:33.416799 Epoch [42/50], Step[8000/8080], Loss: 3.9019, Perplexity: 49.50\n",
      "2019-11-27 06:17:05.351741 Epoch [43/50], Step[0/8080], Loss: 3.7671, Perplexity: 43.25\n",
      "2019-11-27 06:23:46.381215 Epoch [43/50], Step[1000/8080], Loss: 3.6514, Perplexity: 38.53\n",
      "2019-11-27 06:30:23.520527 Epoch [43/50], Step[2000/8080], Loss: 3.8702, Perplexity: 47.95\n",
      "2019-11-27 06:37:00.579082 Epoch [43/50], Step[3000/8080], Loss: 3.7076, Perplexity: 40.75\n",
      "2019-11-27 06:43:38.764077 Epoch [43/50], Step[4000/8080], Loss: 3.6416, Perplexity: 38.15\n",
      "2019-11-27 06:50:18.843368 Epoch [43/50], Step[5000/8080], Loss: 4.2197, Perplexity: 68.01\n",
      "2019-11-27 06:56:51.791385 Epoch [43/50], Step[6000/8080], Loss: 3.8318, Perplexity: 46.15\n",
      "2019-11-27 07:03:25.389013 Epoch [43/50], Step[7000/8080], Loss: 3.5955, Perplexity: 36.43\n",
      "2019-11-27 07:10:00.886363 Epoch [43/50], Step[8000/8080], Loss: 3.8833, Perplexity: 48.59\n",
      "2019-11-27 07:10:31.646493 Epoch [44/50], Step[0/8080], Loss: 3.7947, Perplexity: 44.46\n",
      "2019-11-27 07:17:08.532985 Epoch [44/50], Step[1000/8080], Loss: 3.6424, Perplexity: 38.18\n",
      "2019-11-27 07:23:46.887578 Epoch [44/50], Step[2000/8080], Loss: 3.8686, Perplexity: 47.88\n",
      "2019-11-27 07:30:23.238274 Epoch [44/50], Step[3000/8080], Loss: 3.7271, Perplexity: 41.56\n",
      "2019-11-27 07:37:00.541607 Epoch [44/50], Step[4000/8080], Loss: 3.6381, Perplexity: 38.02\n",
      "2019-11-27 07:43:37.363315 Epoch [44/50], Step[5000/8080], Loss: 4.1960, Perplexity: 66.42\n",
      "2019-11-27 07:50:12.541648 Epoch [44/50], Step[6000/8080], Loss: 3.8152, Perplexity: 45.38\n",
      "2019-11-27 07:56:50.030559 Epoch [44/50], Step[7000/8080], Loss: 3.5617, Perplexity: 35.22\n",
      "2019-11-27 08:03:29.073303 Epoch [44/50], Step[8000/8080], Loss: 3.9231, Perplexity: 50.56\n",
      "2019-11-27 08:04:01.039335 Epoch [45/50], Step[0/8080], Loss: 3.7958, Perplexity: 44.52\n",
      "2019-11-27 08:10:34.310397 Epoch [45/50], Step[1000/8080], Loss: 3.6386, Perplexity: 38.04\n",
      "2019-11-27 08:17:10.234126 Epoch [45/50], Step[2000/8080], Loss: 3.8733, Perplexity: 48.10\n",
      "2019-11-27 08:23:47.985031 Epoch [45/50], Step[3000/8080], Loss: 3.7223, Perplexity: 41.36\n",
      "2019-11-27 08:30:22.525414 Epoch [45/50], Step[4000/8080], Loss: 3.6302, Perplexity: 37.72\n",
      "2019-11-27 08:36:56.256214 Epoch [45/50], Step[5000/8080], Loss: 4.2506, Perplexity: 70.15\n",
      "2019-11-27 08:43:33.230920 Epoch [45/50], Step[6000/8080], Loss: 3.8258, Perplexity: 45.87\n",
      "2019-11-27 08:50:10.309219 Epoch [45/50], Step[7000/8080], Loss: 3.5937, Perplexity: 36.37\n",
      "2019-11-27 08:56:51.421935 Epoch [45/50], Step[8000/8080], Loss: 3.8914, Perplexity: 48.98\n",
      "2019-11-27 08:57:23.450179 Epoch [46/50], Step[0/8080], Loss: 3.7908, Perplexity: 44.29\n",
      "2019-11-27 09:04:07.971900 Epoch [46/50], Step[1000/8080], Loss: 3.6318, Perplexity: 37.78\n",
      "2019-11-27 09:10:42.904312 Epoch [46/50], Step[2000/8080], Loss: 3.8847, Perplexity: 48.65\n",
      "2019-11-27 09:17:22.874376 Epoch [46/50], Step[3000/8080], Loss: 3.7324, Perplexity: 41.78\n",
      "2019-11-27 09:24:01.414405 Epoch [46/50], Step[4000/8080], Loss: 3.6395, Perplexity: 38.07\n",
      "2019-11-27 09:30:36.519415 Epoch [46/50], Step[5000/8080], Loss: 4.2368, Perplexity: 69.19\n",
      "2019-11-27 09:37:14.719544 Epoch [46/50], Step[6000/8080], Loss: 3.8266, Perplexity: 45.91\n",
      "2019-11-27 09:43:52.461625 Epoch [46/50], Step[7000/8080], Loss: 3.5553, Perplexity: 35.00\n",
      "2019-11-27 09:50:32.420909 Epoch [46/50], Step[8000/8080], Loss: 3.8955, Perplexity: 49.18\n",
      "2019-11-27 09:51:04.011045 Epoch [47/50], Step[0/8080], Loss: 3.7921, Perplexity: 44.35\n",
      "2019-11-27 09:57:41.277262 Epoch [47/50], Step[1000/8080], Loss: 3.6282, Perplexity: 37.65\n",
      "2019-11-27 10:04:19.083550 Epoch [47/50], Step[2000/8080], Loss: 3.8607, Perplexity: 47.50\n",
      "2019-11-27 10:10:52.570813 Epoch [47/50], Step[3000/8080], Loss: 3.7085, Perplexity: 40.79\n",
      "2019-11-27 10:17:34.164734 Epoch [47/50], Step[4000/8080], Loss: 3.6189, Perplexity: 37.30\n",
      "2019-11-27 10:24:07.748464 Epoch [47/50], Step[5000/8080], Loss: 4.2287, Perplexity: 68.63\n",
      "2019-11-27 10:30:42.426262 Epoch [47/50], Step[6000/8080], Loss: 3.8380, Perplexity: 46.43\n",
      "2019-11-27 10:37:20.596849 Epoch [47/50], Step[7000/8080], Loss: 3.5967, Perplexity: 36.48\n",
      "2019-11-27 10:43:55.310710 Epoch [47/50], Step[8000/8080], Loss: 3.9027, Perplexity: 49.54\n",
      "2019-11-27 10:44:28.234342 Epoch [48/50], Step[0/8080], Loss: 3.8010, Perplexity: 44.74\n",
      "2019-11-27 10:51:08.496672 Epoch [48/50], Step[1000/8080], Loss: 3.6457, Perplexity: 38.31\n",
      "2019-11-27 10:57:46.521709 Epoch [48/50], Step[2000/8080], Loss: 3.8396, Perplexity: 46.51\n",
      "2019-11-27 11:04:23.350777 Epoch [48/50], Step[3000/8080], Loss: 3.6973, Perplexity: 40.34\n",
      "2019-11-27 11:11:04.026649 Epoch [48/50], Step[4000/8080], Loss: 3.5919, Perplexity: 36.30\n",
      "2019-11-27 11:17:44.011790 Epoch [48/50], Step[5000/8080], Loss: 4.2127, Perplexity: 67.54\n",
      "2019-11-27 11:24:21.808664 Epoch [48/50], Step[6000/8080], Loss: 3.8386, Perplexity: 46.46\n",
      "2019-11-27 11:30:59.298245 Epoch [48/50], Step[7000/8080], Loss: 3.5624, Perplexity: 35.25\n",
      "2019-11-27 11:37:37.521049 Epoch [48/50], Step[8000/8080], Loss: 3.9073, Perplexity: 49.76\n",
      "2019-11-27 11:38:08.198182 Epoch [49/50], Step[0/8080], Loss: 3.7821, Perplexity: 43.91\n",
      "2019-11-27 11:44:46.726268 Epoch [49/50], Step[1000/8080], Loss: 3.6235, Perplexity: 37.47\n",
      "2019-11-27 11:51:23.665049 Epoch [49/50], Step[2000/8080], Loss: 3.8647, Perplexity: 47.69\n",
      "2019-11-27 11:57:58.214308 Epoch [49/50], Step[3000/8080], Loss: 3.7035, Perplexity: 40.59\n",
      "2019-11-27 12:04:35.821490 Epoch [49/50], Step[4000/8080], Loss: 3.6023, Perplexity: 36.68\n",
      "2019-11-27 12:11:11.136093 Epoch [49/50], Step[5000/8080], Loss: 4.2080, Perplexity: 67.22\n",
      "2019-11-27 12:17:49.637670 Epoch [49/50], Step[6000/8080], Loss: 3.8470, Perplexity: 46.85\n",
      "2019-11-27 12:24:27.974270 Epoch [49/50], Step[7000/8080], Loss: 3.5675, Perplexity: 35.43\n",
      "2019-11-27 12:31:00.672981 Epoch [49/50], Step[8000/8080], Loss: 3.8657, Perplexity: 47.74\n",
      "2019-11-27 12:31:32.553801 Epoch [50/50], Step[0/8080], Loss: 3.7905, Perplexity: 44.28\n",
      "2019-11-27 12:38:07.282668 Epoch [50/50], Step[1000/8080], Loss: 3.6611, Perplexity: 38.90\n",
      "2019-11-27 12:44:44.969652 Epoch [50/50], Step[2000/8080], Loss: 3.8529, Perplexity: 47.13\n",
      "2019-11-27 12:51:17.036056 Epoch [50/50], Step[3000/8080], Loss: 3.7157, Perplexity: 41.09\n",
      "2019-11-27 12:57:55.096286 Epoch [50/50], Step[4000/8080], Loss: 3.5957, Perplexity: 36.44\n",
      "2019-11-27 13:04:32.606968 Epoch [50/50], Step[5000/8080], Loss: 4.2011, Perplexity: 66.76\n",
      "2019-11-27 13:11:04.428475 Epoch [50/50], Step[6000/8080], Loss: 3.8194, Perplexity: 45.58\n",
      "2019-11-27 13:17:40.716956 Epoch [50/50], Step[7000/8080], Loss: 3.5750, Perplexity: 35.70\n",
      "2019-11-27 13:24:18.596099 Epoch [50/50], Step[8000/8080], Loss: 3.9188, Perplexity: 50.34\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    state = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Get images\n",
    "        images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "        for b, seq in enumerate(ids):\n",
    "            for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "                images[b, s] = np.load(f'char_img/noto_CJK/msr3/{idx}.npy').reshape(char_size,char_size,1)\n",
    "        images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "        images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "        images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "        \n",
    "        # Get encoded images\n",
    "        cnn_o = cnn_encoder(images)\n",
    "        cnn_o = cnn_o.view(inputs.size(0), inputs.size(1), -1)\n",
    "               \n",
    "        # Forward pass\n",
    "        state = detach(state)\n",
    "        outputs, state = model(inputs, cnn_o, state)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        cnn_encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(params, 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 1000 == 0:\n",
    "            print ('{} Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(datetime.now(), epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perplexity: 43.938697216289434\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "cnn_encoder.eval()\n",
    "\n",
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = ids[:, i:i+seq_length].to(device)\n",
    "    targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Get images\n",
    "    images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "    for b, seq in enumerate(ids):\n",
    "        for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "            images[b, s] = np.load(f'char_img/noto_CJK/msr3/{idx}.npy').reshape(char_size,char_size,1)\n",
    "    images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "    images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "    images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "\n",
    "    # Get encoded images\n",
    "    cnn_o = cnn_encoder(images)\n",
    "    cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(inputs, cnn_o, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "    \n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Train Perplexity: {perplexity / num_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = corpus.get_data('icwb2-data/testing/msr_test.utf8', batch_size)\n",
    "# filter out unknown character\n",
    "test_ids = test_ids.view(-1)\n",
    "mask = test_ids < vocab_size\n",
    "test_ids = test_ids[mask]\n",
    "num_batches = test_ids.size(0) // batch_size\n",
    "test_ids = test_ids[:num_batches*batch_size]\n",
    "test_ids = test_ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 58.1385755453622\n"
     ]
    }
   ],
   "source": [
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = test_ids[:, i:i+seq_length].to(device)\n",
    "    targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Get images\n",
    "    images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "    for b, seq in enumerate(test_ids):\n",
    "        for s, idx in enumerate(test_ids[b][i:i+seq_length]):\n",
    "            images[b, s] = np.load(f'char_img/noto_CJK/msr3/{idx}.npy').reshape(char_size,char_size,1)\n",
    "    images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "    images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "    images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "\n",
    "    # Get encoded images\n",
    "    cnn_o = cnn_encoder(images)\n",
    "    cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(inputs, cnn_o, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Test Perplexity: {perplexity / num_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch1.2)",
   "language": "python",
   "name": "pytorch1.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
