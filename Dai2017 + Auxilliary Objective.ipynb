{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renderer\n",
    "from PIL import ImageFont\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "char_size = 24\n",
    "# char render\n",
    "def render(text, font=None):\n",
    "    if font is None:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\", char_size)\n",
    "    mask = font.getmask(text)\n",
    "    size = mask.size[::-1]\n",
    "    a = np.asarray(mask).reshape(size) / 255\n",
    "    res = cv2.resize(a, dsize=(char_size, char_size), interpolation=cv2.INTER_CUBIC)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model/data_utils.py\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, max_size=None):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 1\n",
    "        self.word2idx['⸘'] = 0 # as unk\n",
    "        self.idx2word[0] = '⸘'\n",
    "        self.max_size = max_size + 1\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx and self.idx < self.max_size:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, max_size=None):\n",
    "        self.dictionary = Dictionary(max_size=max_size)\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "#         with open(path, 'r') as f:\n",
    "#             tokens = 0\n",
    "#             for line in f:\n",
    "#                 words = line.split() + ['<eos>']\n",
    "#                 tokens += len(words)\n",
    "#                 for word in words: \n",
    "#                     self.dictionary.add_word(word)  \n",
    "\n",
    "        # split words to char and add to dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                tokens += len(chars)\n",
    "                for char in chars:\n",
    "                    self.dictionary.add_word(char)\n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                for char in chars:\n",
    "                    if char in self.dictionary.word2idx:\n",
    "                        ids[token] = self.dictionary.word2idx[char]\n",
    "                        token += 1\n",
    "                    else:\n",
    "                        ids[token] = self.dictionary.word2idx['⸘']\n",
    "                        token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, char_id, char_cnn_o, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(char_id) + char_cnn_o\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, h = self.gru(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)\n",
    "\"\"\"\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    if type(stride) is not tuple:\n",
    "        stride = (stride, stride)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride[0]) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride[1]) + 1)\n",
    "    return h, w\n",
    "\n",
    "# Dai et al. 's CNN glyph encoder\n",
    "class Dai_CNN(nn.Module):\n",
    "    def __init__(self, embed_size, input_size=(32, 32)):\n",
    "        super(Dai_CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, (7, 7), stride=(2,2))\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.zeros_(self.conv1.bias)\n",
    "        h, w = conv_output_shape(input_size, (7, 7), (2, 2))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 16, (5, 5), stride=(2,2))\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        torch.nn.init.zeros_(self.conv2.bias)\n",
    "        h, w = conv_output_shape((h, w), (5, 5), (2, 2))\n",
    "                \n",
    "        self.fc = nn.Linear(16*h*w, embed_size)\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "        torch.nn.init.zeros_(self.fc.bias)\n",
    "        \n",
    "        self.h, self.w = h, w\n",
    "        \n",
    "    def forward(self, char_img):\n",
    "        b = char_img.size(0)\n",
    "        x = self.conv1(char_img)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = x.view(-1, 16*self.h*self.w)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassification(nn.Module):\n",
    "    def __init__(self, hidden_size, classes):\n",
    "        super(ImageClassification, self).__init__()\n",
    "        self.hidden = nn.Linear(hidden_size, classes)\n",
    "    \n",
    "    def forward(self, cnn_o):\n",
    "        return self.hidden(cnn_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "embed_size = 300\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "seq_length = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Load dataset\n",
    "corpus = Corpus(max_size=4000)\n",
    "ids = corpus.get_data('icwb2-data/training/msr_training.utf8', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save char images for reference\n",
    "for char, idx in corpus.dictionary.word2idx.items():\n",
    "    np.save(f'char_img/noto_CJK/msr/{idx}.npy', render(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "cnn_encoder = Dai_CNN(embed_size, (char_size, char_size)).to(device)\n",
    "clf = ImageClassification(embed_size, vocab_size).to(device)\n",
    "model.train()\n",
    "cnn_encoder.train()\n",
    "clf.train()\n",
    "params = list(model.parameters())+list(cnn_encoder.parameters()) + list(clf.parameters())\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# Truncated backpropagation\n",
    "def detach(state):\n",
    "    return state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-27 18:12:21.153890 Epoch [1/50], Step[0/8080], Loss: 8.3136, Perplexity: 4078.90\n",
      "2019-11-27 18:15:18.997133 Epoch [1/50], Step[1000/8080], Loss: 5.0811, Perplexity: 160.96\n",
      "2019-11-27 18:18:17.908673 Epoch [1/50], Step[2000/8080], Loss: 4.6229, Perplexity: 101.79\n",
      "2019-11-27 18:21:15.196696 Epoch [1/50], Step[3000/8080], Loss: 4.4064, Perplexity: 81.98\n",
      "2019-11-27 18:24:13.361497 Epoch [1/50], Step[4000/8080], Loss: 4.2224, Perplexity: 68.20\n",
      "2019-11-27 18:27:06.658774 Epoch [1/50], Step[5000/8080], Loss: 4.6678, Perplexity: 106.46\n",
      "2019-11-27 18:29:59.347630 Epoch [1/50], Step[6000/8080], Loss: 4.6531, Perplexity: 104.91\n",
      "2019-11-27 18:32:52.768386 Epoch [1/50], Step[7000/8080], Loss: 4.1191, Perplexity: 61.51\n",
      "2019-11-27 18:35:41.835422 Epoch [1/50], Step[8000/8080], Loss: 4.3034, Perplexity: 73.95\n",
      "2019-11-27 18:35:55.704732 Epoch [2/50], Step[0/8080], Loss: 4.2709, Perplexity: 71.58\n",
      "2019-11-27 18:38:41.992344 Epoch [2/50], Step[1000/8080], Loss: 4.1539, Perplexity: 63.68\n",
      "2019-11-27 18:41:32.605801 Epoch [2/50], Step[2000/8080], Loss: 4.1588, Perplexity: 63.99\n",
      "2019-11-27 18:44:21.868486 Epoch [2/50], Step[3000/8080], Loss: 4.0529, Perplexity: 57.56\n",
      "2019-11-27 18:47:14.294453 Epoch [2/50], Step[4000/8080], Loss: 3.9378, Perplexity: 51.31\n",
      "2019-11-27 18:50:02.210265 Epoch [2/50], Step[5000/8080], Loss: 4.4936, Perplexity: 89.44\n",
      "2019-11-27 18:52:54.716349 Epoch [2/50], Step[6000/8080], Loss: 4.3916, Perplexity: 80.77\n",
      "2019-11-27 18:55:49.693531 Epoch [2/50], Step[7000/8080], Loss: 3.8963, Perplexity: 49.22\n",
      "2019-11-27 18:58:44.635194 Epoch [2/50], Step[8000/8080], Loss: 4.1387, Perplexity: 62.72\n",
      "2019-11-27 18:58:58.465665 Epoch [3/50], Step[0/8080], Loss: 4.0880, Perplexity: 59.62\n",
      "2019-11-27 19:01:56.470971 Epoch [3/50], Step[1000/8080], Loss: 3.9791, Perplexity: 53.47\n",
      "2019-11-27 19:04:53.558473 Epoch [3/50], Step[2000/8080], Loss: 4.0344, Perplexity: 56.51\n",
      "2019-11-27 19:07:49.661472 Epoch [3/50], Step[3000/8080], Loss: 3.9671, Perplexity: 52.83\n",
      "2019-11-27 19:10:41.174543 Epoch [3/50], Step[4000/8080], Loss: 3.8738, Perplexity: 48.12\n",
      "2019-11-27 19:13:27.432051 Epoch [3/50], Step[5000/8080], Loss: 4.4144, Perplexity: 82.63\n",
      "2019-11-27 19:16:13.155493 Epoch [3/50], Step[6000/8080], Loss: 4.2717, Perplexity: 71.64\n",
      "2019-11-27 19:19:04.812817 Epoch [3/50], Step[7000/8080], Loss: 3.8494, Perplexity: 46.96\n",
      "2019-11-27 19:21:57.651922 Epoch [3/50], Step[8000/8080], Loss: 4.0655, Perplexity: 58.29\n",
      "2019-11-27 19:22:11.321455 Epoch [4/50], Step[0/8080], Loss: 4.0083, Perplexity: 55.05\n",
      "2019-11-27 19:24:59.115183 Epoch [4/50], Step[1000/8080], Loss: 3.8875, Perplexity: 48.79\n",
      "2019-11-27 19:27:46.658350 Epoch [4/50], Step[2000/8080], Loss: 3.9952, Perplexity: 54.34\n",
      "2019-11-27 19:30:37.433270 Epoch [4/50], Step[3000/8080], Loss: 3.9243, Perplexity: 50.62\n",
      "2019-11-27 19:33:23.853210 Epoch [4/50], Step[4000/8080], Loss: 3.8424, Perplexity: 46.64\n",
      "2019-11-27 19:36:12.565692 Epoch [4/50], Step[5000/8080], Loss: 4.3989, Perplexity: 81.36\n",
      "2019-11-27 19:39:00.922651 Epoch [4/50], Step[6000/8080], Loss: 4.1977, Perplexity: 66.53\n",
      "2019-11-27 19:41:51.520872 Epoch [4/50], Step[7000/8080], Loss: 3.7939, Perplexity: 44.43\n",
      "2019-11-27 19:44:38.527524 Epoch [4/50], Step[8000/8080], Loss: 4.0357, Perplexity: 56.58\n",
      "2019-11-27 19:44:52.340259 Epoch [5/50], Step[0/8080], Loss: 3.9743, Perplexity: 53.21\n",
      "2019-11-27 19:47:40.651118 Epoch [5/50], Step[1000/8080], Loss: 3.8435, Perplexity: 46.69\n",
      "2019-11-27 19:50:33.214538 Epoch [5/50], Step[2000/8080], Loss: 3.9707, Perplexity: 53.02\n",
      "2019-11-27 19:53:27.893257 Epoch [5/50], Step[3000/8080], Loss: 3.8938, Perplexity: 49.10\n",
      "2019-11-27 19:56:15.440015 Epoch [5/50], Step[4000/8080], Loss: 3.8092, Perplexity: 45.11\n",
      "2019-11-27 19:59:06.248840 Epoch [5/50], Step[5000/8080], Loss: 4.3634, Perplexity: 78.53\n",
      "2019-11-27 20:01:59.564696 Epoch [5/50], Step[6000/8080], Loss: 4.1176, Perplexity: 61.41\n",
      "2019-11-27 20:04:48.755400 Epoch [5/50], Step[7000/8080], Loss: 3.7844, Perplexity: 44.01\n",
      "2019-11-27 20:07:37.164888 Epoch [5/50], Step[8000/8080], Loss: 3.9982, Perplexity: 54.50\n",
      "2019-11-27 20:07:50.667001 Epoch [6/50], Step[0/8080], Loss: 3.9404, Perplexity: 51.44\n",
      "2019-11-27 20:10:38.967186 Epoch [6/50], Step[1000/8080], Loss: 3.7824, Perplexity: 43.92\n",
      "2019-11-27 20:13:24.162751 Epoch [6/50], Step[2000/8080], Loss: 3.9729, Perplexity: 53.14\n",
      "2019-11-27 20:16:11.350520 Epoch [6/50], Step[3000/8080], Loss: 3.8431, Perplexity: 46.67\n",
      "2019-11-27 20:19:01.271695 Epoch [6/50], Step[4000/8080], Loss: 3.7904, Perplexity: 44.28\n",
      "2019-11-27 20:21:51.425846 Epoch [6/50], Step[5000/8080], Loss: 4.3431, Perplexity: 76.94\n",
      "2019-11-27 20:24:40.288225 Epoch [6/50], Step[6000/8080], Loss: 4.0940, Perplexity: 59.98\n",
      "2019-11-27 20:27:40.116139 Epoch [6/50], Step[7000/8080], Loss: 3.7263, Perplexity: 41.53\n",
      "2019-11-27 20:30:42.533875 Epoch [6/50], Step[8000/8080], Loss: 3.9723, Perplexity: 53.10\n",
      "2019-11-27 20:30:57.062001 Epoch [7/50], Step[0/8080], Loss: 3.9223, Perplexity: 50.52\n",
      "2019-11-27 20:33:55.568367 Epoch [7/50], Step[1000/8080], Loss: 3.7484, Perplexity: 42.45\n",
      "2019-11-27 20:36:46.965751 Epoch [7/50], Step[2000/8080], Loss: 3.9417, Perplexity: 51.51\n",
      "2019-11-27 20:39:41.630821 Epoch [7/50], Step[3000/8080], Loss: 3.7971, Perplexity: 44.57\n",
      "2019-11-27 20:42:38.335445 Epoch [7/50], Step[4000/8080], Loss: 3.7562, Perplexity: 42.79\n",
      "2019-11-27 20:45:36.950752 Epoch [7/50], Step[5000/8080], Loss: 4.3326, Perplexity: 76.14\n",
      "2019-11-27 20:48:27.958313 Epoch [7/50], Step[6000/8080], Loss: 4.0702, Perplexity: 58.57\n",
      "2019-11-27 20:51:17.063758 Epoch [7/50], Step[7000/8080], Loss: 3.7362, Perplexity: 41.94\n",
      "2019-11-27 20:54:05.748913 Epoch [7/50], Step[8000/8080], Loss: 3.9648, Perplexity: 52.71\n",
      "2019-11-27 20:54:19.667827 Epoch [8/50], Step[0/8080], Loss: 3.8876, Perplexity: 48.79\n",
      "2019-11-27 20:57:08.207566 Epoch [8/50], Step[1000/8080], Loss: 3.7311, Perplexity: 41.73\n",
      "2019-11-27 21:00:02.132407 Epoch [8/50], Step[2000/8080], Loss: 3.9489, Perplexity: 51.88\n",
      "2019-11-27 21:02:51.635351 Epoch [8/50], Step[3000/8080], Loss: 3.7947, Perplexity: 44.47\n",
      "2019-11-27 21:05:46.044027 Epoch [8/50], Step[4000/8080], Loss: 3.7486, Perplexity: 42.46\n",
      "2019-11-27 21:08:41.733783 Epoch [8/50], Step[5000/8080], Loss: 4.3166, Perplexity: 74.93\n",
      "2019-11-27 21:11:38.330773 Epoch [8/50], Step[6000/8080], Loss: 4.0355, Perplexity: 56.57\n",
      "2019-11-27 21:14:36.883935 Epoch [8/50], Step[7000/8080], Loss: 3.7304, Perplexity: 41.70\n",
      "2019-11-27 21:17:30.443736 Epoch [8/50], Step[8000/8080], Loss: 3.9624, Perplexity: 52.58\n",
      "2019-11-27 21:17:44.573393 Epoch [9/50], Step[0/8080], Loss: 3.8646, Perplexity: 47.68\n",
      "2019-11-27 21:20:43.363659 Epoch [9/50], Step[1000/8080], Loss: 3.7262, Perplexity: 41.52\n",
      "2019-11-27 21:23:41.284326 Epoch [9/50], Step[2000/8080], Loss: 3.9087, Perplexity: 49.84\n",
      "2019-11-27 21:26:40.805309 Epoch [9/50], Step[3000/8080], Loss: 3.7835, Perplexity: 43.97\n",
      "2019-11-27 21:29:41.488772 Epoch [9/50], Step[4000/8080], Loss: 3.7400, Perplexity: 42.10\n",
      "2019-11-27 21:32:41.719002 Epoch [9/50], Step[5000/8080], Loss: 4.2869, Perplexity: 72.74\n",
      "2019-11-27 21:35:41.554569 Epoch [9/50], Step[6000/8080], Loss: 3.9924, Perplexity: 54.18\n",
      "2019-11-27 21:38:38.370416 Epoch [9/50], Step[7000/8080], Loss: 3.7273, Perplexity: 41.57\n",
      "2019-11-27 21:41:29.556096 Epoch [9/50], Step[8000/8080], Loss: 3.9519, Perplexity: 52.03\n",
      "2019-11-27 21:41:43.161496 Epoch [10/50], Step[0/8080], Loss: 3.8530, Perplexity: 47.13\n",
      "2019-11-27 21:44:32.358566 Epoch [10/50], Step[1000/8080], Loss: 3.7053, Perplexity: 40.66\n",
      "2019-11-27 21:47:24.652812 Epoch [10/50], Step[2000/8080], Loss: 3.8972, Perplexity: 49.26\n",
      "2019-11-27 21:50:11.882948 Epoch [10/50], Step[3000/8080], Loss: 3.7818, Perplexity: 43.90\n",
      "2019-11-27 21:52:57.204773 Epoch [10/50], Step[4000/8080], Loss: 3.7394, Perplexity: 42.07\n",
      "2019-11-27 21:55:44.551153 Epoch [10/50], Step[5000/8080], Loss: 4.2739, Perplexity: 71.80\n",
      "2019-11-27 21:58:32.905640 Epoch [10/50], Step[6000/8080], Loss: 3.9725, Perplexity: 53.12\n",
      "2019-11-27 22:01:20.486234 Epoch [10/50], Step[7000/8080], Loss: 3.7132, Perplexity: 40.98\n",
      "2019-11-27 22:04:09.438256 Epoch [10/50], Step[8000/8080], Loss: 3.9353, Perplexity: 51.18\n",
      "2019-11-27 22:04:22.781067 Epoch [11/50], Step[0/8080], Loss: 3.8439, Perplexity: 46.71\n",
      "2019-11-27 22:07:13.456048 Epoch [11/50], Step[1000/8080], Loss: 3.6969, Perplexity: 40.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-27 22:10:04.335536 Epoch [11/50], Step[2000/8080], Loss: 3.8856, Perplexity: 48.70\n",
      "2019-11-27 22:12:52.465489 Epoch [11/50], Step[3000/8080], Loss: 3.7704, Perplexity: 43.40\n",
      "2019-11-27 22:15:41.765332 Epoch [11/50], Step[4000/8080], Loss: 3.7356, Perplexity: 41.91\n",
      "2019-11-27 22:18:30.128154 Epoch [11/50], Step[5000/8080], Loss: 4.2791, Perplexity: 72.18\n",
      "2019-11-27 22:21:20.376426 Epoch [11/50], Step[6000/8080], Loss: 3.9506, Perplexity: 51.97\n",
      "2019-11-27 22:24:06.080005 Epoch [11/50], Step[7000/8080], Loss: 3.6808, Perplexity: 39.68\n",
      "2019-11-27 22:26:54.139484 Epoch [11/50], Step[8000/8080], Loss: 3.9245, Perplexity: 50.63\n",
      "2019-11-27 22:27:07.374179 Epoch [12/50], Step[0/8080], Loss: 3.8492, Perplexity: 46.95\n",
      "2019-11-27 22:29:56.402528 Epoch [12/50], Step[1000/8080], Loss: 3.6561, Perplexity: 38.71\n",
      "2019-11-27 22:32:43.151774 Epoch [12/50], Step[2000/8080], Loss: 3.8737, Perplexity: 48.12\n",
      "2019-11-27 22:35:32.760514 Epoch [12/50], Step[3000/8080], Loss: 3.7576, Perplexity: 42.84\n",
      "2019-11-27 22:38:20.607728 Epoch [12/50], Step[4000/8080], Loss: 3.7298, Perplexity: 41.67\n",
      "2019-11-27 22:41:09.075405 Epoch [12/50], Step[5000/8080], Loss: 4.2748, Perplexity: 71.86\n",
      "2019-11-27 22:43:59.097800 Epoch [12/50], Step[6000/8080], Loss: 3.9379, Perplexity: 51.31\n",
      "2019-11-27 22:46:48.430852 Epoch [12/50], Step[7000/8080], Loss: 3.7170, Perplexity: 41.14\n",
      "2019-11-27 22:49:39.465314 Epoch [12/50], Step[8000/8080], Loss: 3.9372, Perplexity: 51.27\n",
      "2019-11-27 22:49:53.594893 Epoch [13/50], Step[0/8080], Loss: 3.8660, Perplexity: 47.75\n",
      "2019-11-27 22:52:45.953107 Epoch [13/50], Step[1000/8080], Loss: 3.6669, Perplexity: 39.13\n",
      "2019-11-27 22:55:33.964941 Epoch [13/50], Step[2000/8080], Loss: 3.8599, Perplexity: 47.46\n",
      "2019-11-27 22:58:21.474630 Epoch [13/50], Step[3000/8080], Loss: 3.7465, Perplexity: 42.37\n",
      "2019-11-27 23:01:11.528006 Epoch [13/50], Step[4000/8080], Loss: 3.6887, Perplexity: 39.99\n",
      "2019-11-27 23:03:58.472248 Epoch [13/50], Step[5000/8080], Loss: 4.2626, Perplexity: 71.00\n",
      "2019-11-27 23:06:49.372349 Epoch [13/50], Step[6000/8080], Loss: 3.9517, Perplexity: 52.02\n",
      "2019-11-27 23:09:38.658956 Epoch [13/50], Step[7000/8080], Loss: 3.7161, Perplexity: 41.10\n",
      "2019-11-27 23:12:30.675209 Epoch [13/50], Step[8000/8080], Loss: 3.9490, Perplexity: 51.88\n",
      "2019-11-27 23:12:43.926951 Epoch [14/50], Step[0/8080], Loss: 3.8446, Perplexity: 46.74\n",
      "2019-11-27 23:15:33.235995 Epoch [14/50], Step[1000/8080], Loss: 3.6447, Perplexity: 38.27\n",
      "2019-11-27 23:18:20.910212 Epoch [14/50], Step[2000/8080], Loss: 3.8441, Perplexity: 46.72\n",
      "2019-11-27 23:21:10.378329 Epoch [14/50], Step[3000/8080], Loss: 3.7343, Perplexity: 41.86\n",
      "2019-11-27 23:24:00.295003 Epoch [14/50], Step[4000/8080], Loss: 3.6968, Perplexity: 40.32\n",
      "2019-11-27 23:26:50.354866 Epoch [14/50], Step[5000/8080], Loss: 4.2592, Perplexity: 70.75\n",
      "2019-11-27 23:29:39.110556 Epoch [14/50], Step[6000/8080], Loss: 3.9260, Perplexity: 50.71\n",
      "2019-11-27 23:32:27.958352 Epoch [14/50], Step[7000/8080], Loss: 3.6810, Perplexity: 39.69\n",
      "2019-11-27 23:35:18.184787 Epoch [14/50], Step[8000/8080], Loss: 3.9306, Perplexity: 50.94\n",
      "2019-11-27 23:35:31.924666 Epoch [15/50], Step[0/8080], Loss: 3.8428, Perplexity: 46.66\n",
      "2019-11-27 23:38:20.217967 Epoch [15/50], Step[1000/8080], Loss: 3.6531, Perplexity: 38.59\n",
      "2019-11-27 23:41:09.628862 Epoch [15/50], Step[2000/8080], Loss: 3.8710, Perplexity: 47.99\n",
      "2019-11-27 23:43:58.161017 Epoch [15/50], Step[3000/8080], Loss: 3.7466, Perplexity: 42.37\n",
      "2019-11-27 23:46:44.055389 Epoch [15/50], Step[4000/8080], Loss: 3.6763, Perplexity: 39.50\n",
      "2019-11-27 23:49:32.641363 Epoch [15/50], Step[5000/8080], Loss: 4.2949, Perplexity: 73.32\n",
      "2019-11-27 23:52:16.119121 Epoch [15/50], Step[6000/8080], Loss: 3.9050, Perplexity: 49.65\n",
      "2019-11-27 23:55:04.368641 Epoch [15/50], Step[7000/8080], Loss: 3.6787, Perplexity: 39.59\n",
      "2019-11-27 23:57:53.895822 Epoch [15/50], Step[8000/8080], Loss: 3.9148, Perplexity: 50.14\n",
      "2019-11-27 23:58:07.806577 Epoch [16/50], Step[0/8080], Loss: 3.8587, Perplexity: 47.40\n",
      "2019-11-28 00:00:56.262130 Epoch [16/50], Step[1000/8080], Loss: 3.6689, Perplexity: 39.21\n",
      "2019-11-28 00:03:43.642514 Epoch [16/50], Step[2000/8080], Loss: 3.8826, Perplexity: 48.55\n",
      "2019-11-28 00:06:33.263816 Epoch [16/50], Step[3000/8080], Loss: 3.7010, Perplexity: 40.49\n",
      "2019-11-28 00:09:24.548537 Epoch [16/50], Step[4000/8080], Loss: 3.6724, Perplexity: 39.35\n",
      "2019-11-28 00:12:14.782775 Epoch [16/50], Step[5000/8080], Loss: 4.2811, Perplexity: 72.32\n",
      "2019-11-28 00:15:04.150048 Epoch [16/50], Step[6000/8080], Loss: 3.8953, Perplexity: 49.17\n",
      "2019-11-28 00:17:54.600948 Epoch [16/50], Step[7000/8080], Loss: 3.6705, Perplexity: 39.27\n",
      "2019-11-28 00:20:47.750281 Epoch [16/50], Step[8000/8080], Loss: 3.9253, Perplexity: 50.67\n",
      "2019-11-28 00:21:01.262318 Epoch [17/50], Step[0/8080], Loss: 3.8614, Perplexity: 47.53\n",
      "2019-11-28 00:23:49.995016 Epoch [17/50], Step[1000/8080], Loss: 3.6336, Perplexity: 37.85\n",
      "2019-11-28 00:26:40.804806 Epoch [17/50], Step[2000/8080], Loss: 3.8834, Perplexity: 48.59\n",
      "2019-11-28 00:29:31.888956 Epoch [17/50], Step[3000/8080], Loss: 3.7251, Perplexity: 41.48\n",
      "2019-11-28 00:32:18.587874 Epoch [17/50], Step[4000/8080], Loss: 3.6858, Perplexity: 39.88\n",
      "2019-11-28 00:35:06.605868 Epoch [17/50], Step[5000/8080], Loss: 4.2782, Perplexity: 72.11\n",
      "2019-11-28 00:37:56.646404 Epoch [17/50], Step[6000/8080], Loss: 3.9350, Perplexity: 51.16\n",
      "2019-11-28 00:40:44.886107 Epoch [17/50], Step[7000/8080], Loss: 3.6375, Perplexity: 38.00\n",
      "2019-11-28 00:43:34.089194 Epoch [17/50], Step[8000/8080], Loss: 3.9247, Perplexity: 50.64\n",
      "2019-11-28 00:43:47.869537 Epoch [18/50], Step[0/8080], Loss: 3.8573, Perplexity: 47.34\n",
      "2019-11-28 00:46:38.733169 Epoch [18/50], Step[1000/8080], Loss: 3.6155, Perplexity: 37.17\n",
      "2019-11-28 00:49:26.529996 Epoch [18/50], Step[2000/8080], Loss: 3.8654, Perplexity: 47.72\n",
      "2019-11-28 00:52:14.533892 Epoch [18/50], Step[3000/8080], Loss: 3.7283, Perplexity: 41.61\n",
      "2019-11-28 00:55:04.601954 Epoch [18/50], Step[4000/8080], Loss: 3.6760, Perplexity: 39.49\n",
      "2019-11-28 00:57:53.781384 Epoch [18/50], Step[5000/8080], Loss: 4.2493, Perplexity: 70.06\n",
      "2019-11-28 01:00:42.559120 Epoch [18/50], Step[6000/8080], Loss: 3.9192, Perplexity: 50.36\n",
      "2019-11-28 01:03:31.302949 Epoch [18/50], Step[7000/8080], Loss: 3.6277, Perplexity: 37.63\n",
      "2019-11-28 01:06:19.071697 Epoch [18/50], Step[8000/8080], Loss: 3.9295, Perplexity: 50.88\n",
      "2019-11-28 01:06:32.777162 Epoch [19/50], Step[0/8080], Loss: 3.8533, Perplexity: 47.15\n",
      "2019-11-28 01:09:24.088244 Epoch [19/50], Step[1000/8080], Loss: 3.5955, Perplexity: 36.44\n",
      "2019-11-28 01:12:11.638125 Epoch [19/50], Step[2000/8080], Loss: 3.8714, Perplexity: 48.01\n",
      "2019-11-28 01:15:03.282411 Epoch [19/50], Step[3000/8080], Loss: 3.7216, Perplexity: 41.33\n",
      "2019-11-28 01:17:48.997759 Epoch [19/50], Step[4000/8080], Loss: 3.6473, Perplexity: 38.37\n",
      "2019-11-28 01:20:37.361481 Epoch [19/50], Step[5000/8080], Loss: 4.2519, Perplexity: 70.24\n",
      "2019-11-28 01:23:27.052026 Epoch [19/50], Step[6000/8080], Loss: 3.9169, Perplexity: 50.24\n",
      "2019-11-28 01:26:16.102833 Epoch [19/50], Step[7000/8080], Loss: 3.6326, Perplexity: 37.81\n",
      "2019-11-28 01:29:05.621746 Epoch [19/50], Step[8000/8080], Loss: 3.9247, Perplexity: 50.64\n",
      "2019-11-28 01:29:18.585420 Epoch [20/50], Step[0/8080], Loss: 3.8365, Perplexity: 46.36\n",
      "2019-11-28 01:32:08.751801 Epoch [20/50], Step[1000/8080], Loss: 3.6398, Perplexity: 38.09\n",
      "2019-11-28 01:34:56.020791 Epoch [20/50], Step[2000/8080], Loss: 3.8382, Perplexity: 46.44\n",
      "2019-11-28 01:37:46.093899 Epoch [20/50], Step[3000/8080], Loss: 3.7052, Perplexity: 40.66\n",
      "2019-11-28 01:40:33.003529 Epoch [20/50], Step[4000/8080], Loss: 3.6478, Perplexity: 38.39\n",
      "2019-11-28 01:43:22.702262 Epoch [20/50], Step[5000/8080], Loss: 4.2620, Perplexity: 70.95\n",
      "2019-11-28 01:46:11.095737 Epoch [20/50], Step[6000/8080], Loss: 3.9134, Perplexity: 50.07\n",
      "2019-11-28 01:48:57.583283 Epoch [20/50], Step[7000/8080], Loss: 3.6408, Perplexity: 38.12\n",
      "2019-11-28 01:51:43.943814 Epoch [20/50], Step[8000/8080], Loss: 3.9179, Perplexity: 50.29\n",
      "2019-11-28 01:51:57.221544 Epoch [21/50], Step[0/8080], Loss: 3.8412, Perplexity: 46.58\n",
      "2019-11-28 01:54:48.893377 Epoch [21/50], Step[1000/8080], Loss: 3.6292, Perplexity: 37.68\n",
      "2019-11-28 01:57:35.678253 Epoch [21/50], Step[2000/8080], Loss: 3.8286, Perplexity: 46.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-28 02:00:27.454595 Epoch [21/50], Step[3000/8080], Loss: 3.6987, Perplexity: 40.40\n",
      "2019-11-28 02:03:16.511480 Epoch [21/50], Step[4000/8080], Loss: 3.6543, Perplexity: 38.64\n",
      "2019-11-28 02:06:06.200249 Epoch [21/50], Step[5000/8080], Loss: 4.2183, Perplexity: 67.92\n",
      "2019-11-28 02:08:55.838235 Epoch [21/50], Step[6000/8080], Loss: 3.8776, Perplexity: 48.31\n",
      "2019-11-28 02:11:42.669574 Epoch [21/50], Step[7000/8080], Loss: 3.6566, Perplexity: 38.73\n",
      "2019-11-28 02:14:30.806274 Epoch [21/50], Step[8000/8080], Loss: 3.9125, Perplexity: 50.02\n",
      "2019-11-28 02:14:44.338418 Epoch [22/50], Step[0/8080], Loss: 3.8214, Perplexity: 45.67\n",
      "2019-11-28 02:17:33.287894 Epoch [22/50], Step[1000/8080], Loss: 3.6511, Perplexity: 38.52\n",
      "2019-11-28 02:20:22.926488 Epoch [22/50], Step[2000/8080], Loss: 3.8379, Perplexity: 46.43\n",
      "2019-11-28 02:23:14.178356 Epoch [22/50], Step[3000/8080], Loss: 3.7279, Perplexity: 41.59\n",
      "2019-11-28 02:26:04.720046 Epoch [22/50], Step[4000/8080], Loss: 3.6438, Perplexity: 38.24\n",
      "2019-11-28 02:28:55.460038 Epoch [22/50], Step[5000/8080], Loss: 4.2383, Perplexity: 69.29\n",
      "2019-11-28 02:31:44.028806 Epoch [22/50], Step[6000/8080], Loss: 3.9005, Perplexity: 49.43\n",
      "2019-11-28 02:34:32.604951 Epoch [22/50], Step[7000/8080], Loss: 3.6194, Perplexity: 37.32\n",
      "2019-11-28 02:37:23.664366 Epoch [22/50], Step[8000/8080], Loss: 3.8762, Perplexity: 48.24\n",
      "2019-11-28 02:37:37.111055 Epoch [23/50], Step[0/8080], Loss: 3.8363, Perplexity: 46.35\n",
      "2019-11-28 02:40:26.274887 Epoch [23/50], Step[1000/8080], Loss: 3.6076, Perplexity: 36.88\n",
      "2019-11-28 02:43:15.646625 Epoch [23/50], Step[2000/8080], Loss: 3.8768, Perplexity: 48.27\n",
      "2019-11-28 02:46:03.749308 Epoch [23/50], Step[3000/8080], Loss: 3.6974, Perplexity: 40.34\n",
      "2019-11-28 02:48:50.819722 Epoch [23/50], Step[4000/8080], Loss: 3.6335, Perplexity: 37.85\n",
      "2019-11-28 02:51:41.327042 Epoch [23/50], Step[5000/8080], Loss: 4.2217, Perplexity: 68.15\n",
      "2019-11-28 02:54:30.413825 Epoch [23/50], Step[6000/8080], Loss: 3.8657, Perplexity: 47.73\n",
      "2019-11-28 02:57:21.475589 Epoch [23/50], Step[7000/8080], Loss: 3.6288, Perplexity: 37.67\n",
      "2019-11-28 03:00:12.050182 Epoch [23/50], Step[8000/8080], Loss: 3.9040, Perplexity: 49.60\n",
      "2019-11-28 03:00:25.788867 Epoch [24/50], Step[0/8080], Loss: 3.8434, Perplexity: 46.68\n",
      "2019-11-28 03:03:14.418350 Epoch [24/50], Step[1000/8080], Loss: 3.6330, Perplexity: 37.83\n",
      "2019-11-28 03:06:04.164411 Epoch [24/50], Step[2000/8080], Loss: 3.8628, Perplexity: 47.60\n",
      "2019-11-28 03:08:51.864873 Epoch [24/50], Step[3000/8080], Loss: 3.7063, Perplexity: 40.70\n",
      "2019-11-28 03:11:40.899699 Epoch [24/50], Step[4000/8080], Loss: 3.6521, Perplexity: 38.56\n",
      "2019-11-28 03:14:30.751048 Epoch [24/50], Step[5000/8080], Loss: 4.2413, Perplexity: 69.50\n",
      "2019-11-28 03:17:16.325000 Epoch [24/50], Step[6000/8080], Loss: 3.8711, Perplexity: 47.99\n",
      "2019-11-28 03:20:03.561900 Epoch [24/50], Step[7000/8080], Loss: 3.5994, Perplexity: 36.57\n",
      "2019-11-28 03:22:53.378828 Epoch [24/50], Step[8000/8080], Loss: 3.9361, Perplexity: 51.22\n",
      "2019-11-28 03:23:07.122575 Epoch [25/50], Step[0/8080], Loss: 3.8397, Perplexity: 46.51\n",
      "2019-11-28 03:25:55.755669 Epoch [25/50], Step[1000/8080], Loss: 3.6437, Perplexity: 38.23\n",
      "2019-11-28 03:28:47.248775 Epoch [25/50], Step[2000/8080], Loss: 3.8583, Perplexity: 47.39\n",
      "2019-11-28 03:31:38.556530 Epoch [25/50], Step[3000/8080], Loss: 3.7155, Perplexity: 41.08\n",
      "2019-11-28 03:34:27.119662 Epoch [25/50], Step[4000/8080], Loss: 3.6792, Perplexity: 39.61\n",
      "2019-11-28 03:37:14.603445 Epoch [25/50], Step[5000/8080], Loss: 4.2389, Perplexity: 69.33\n",
      "2019-11-28 03:40:04.060111 Epoch [25/50], Step[6000/8080], Loss: 3.8538, Perplexity: 47.17\n",
      "2019-11-28 03:42:53.434929 Epoch [25/50], Step[7000/8080], Loss: 3.6179, Perplexity: 37.26\n",
      "2019-11-28 03:45:42.125974 Epoch [25/50], Step[8000/8080], Loss: 3.9288, Perplexity: 50.85\n",
      "2019-11-28 03:45:55.576333 Epoch [26/50], Step[0/8080], Loss: 3.8289, Perplexity: 46.01\n",
      "2019-11-28 03:48:41.613103 Epoch [26/50], Step[1000/8080], Loss: 3.6300, Perplexity: 37.71\n",
      "2019-11-28 03:51:27.825493 Epoch [26/50], Step[2000/8080], Loss: 3.8456, Perplexity: 46.79\n",
      "2019-11-28 03:54:16.819560 Epoch [26/50], Step[3000/8080], Loss: 3.7027, Perplexity: 40.56\n",
      "2019-11-28 03:57:05.924508 Epoch [26/50], Step[4000/8080], Loss: 3.6659, Perplexity: 39.09\n",
      "2019-11-28 03:59:52.515601 Epoch [26/50], Step[5000/8080], Loss: 4.2043, Perplexity: 66.98\n",
      "2019-11-28 04:02:42.226470 Epoch [26/50], Step[6000/8080], Loss: 3.8532, Perplexity: 47.14\n",
      "2019-11-28 04:05:31.278493 Epoch [26/50], Step[7000/8080], Loss: 3.6147, Perplexity: 37.14\n",
      "2019-11-28 04:08:19.137571 Epoch [26/50], Step[8000/8080], Loss: 3.9445, Perplexity: 51.65\n",
      "2019-11-28 04:08:32.390311 Epoch [27/50], Step[0/8080], Loss: 3.8130, Perplexity: 45.29\n",
      "2019-11-28 04:11:18.958535 Epoch [27/50], Step[1000/8080], Loss: 3.6228, Perplexity: 37.44\n",
      "2019-11-28 04:14:06.069572 Epoch [27/50], Step[2000/8080], Loss: 3.8382, Perplexity: 46.44\n",
      "2019-11-28 04:16:55.582264 Epoch [27/50], Step[3000/8080], Loss: 3.7003, Perplexity: 40.46\n",
      "2019-11-28 04:19:43.712964 Epoch [27/50], Step[4000/8080], Loss: 3.6728, Perplexity: 39.36\n",
      "2019-11-28 04:22:31.949457 Epoch [27/50], Step[5000/8080], Loss: 4.2281, Perplexity: 68.59\n",
      "2019-11-28 04:25:20.874677 Epoch [27/50], Step[6000/8080], Loss: 3.8251, Perplexity: 45.84\n",
      "2019-11-28 04:28:09.282178 Epoch [27/50], Step[7000/8080], Loss: 3.5979, Perplexity: 36.52\n",
      "2019-11-28 04:31:00.157961 Epoch [27/50], Step[8000/8080], Loss: 3.8906, Perplexity: 48.94\n",
      "2019-11-28 04:31:13.142002 Epoch [28/50], Step[0/8080], Loss: 3.8231, Perplexity: 45.75\n",
      "2019-11-28 04:33:58.864903 Epoch [28/50], Step[1000/8080], Loss: 3.6476, Perplexity: 38.38\n",
      "2019-11-28 04:36:47.395207 Epoch [28/50], Step[2000/8080], Loss: 3.8409, Perplexity: 46.57\n",
      "2019-11-28 04:39:31.989306 Epoch [28/50], Step[3000/8080], Loss: 3.6816, Perplexity: 39.71\n",
      "2019-11-28 04:42:21.778612 Epoch [28/50], Step[4000/8080], Loss: 3.6683, Perplexity: 39.19\n",
      "2019-11-28 04:45:11.778130 Epoch [28/50], Step[5000/8080], Loss: 4.2169, Perplexity: 67.82\n",
      "2019-11-28 04:47:59.254463 Epoch [28/50], Step[6000/8080], Loss: 3.8524, Perplexity: 47.11\n",
      "2019-11-28 04:50:48.134024 Epoch [28/50], Step[7000/8080], Loss: 3.6125, Perplexity: 37.06\n",
      "2019-11-28 04:53:34.472430 Epoch [28/50], Step[8000/8080], Loss: 3.9371, Perplexity: 51.27\n",
      "2019-11-28 04:53:48.047500 Epoch [29/50], Step[0/8080], Loss: 3.8307, Perplexity: 46.09\n",
      "2019-11-28 04:56:39.564647 Epoch [29/50], Step[1000/8080], Loss: 3.6054, Perplexity: 36.80\n",
      "2019-11-28 04:59:31.309108 Epoch [29/50], Step[2000/8080], Loss: 3.8507, Perplexity: 47.03\n",
      "2019-11-28 05:02:19.900121 Epoch [29/50], Step[3000/8080], Loss: 3.6837, Perplexity: 39.80\n",
      "2019-11-28 05:05:08.877476 Epoch [29/50], Step[4000/8080], Loss: 3.6691, Perplexity: 39.22\n",
      "2019-11-28 05:08:00.911801 Epoch [29/50], Step[5000/8080], Loss: 4.2317, Perplexity: 68.84\n",
      "2019-11-28 05:10:49.899872 Epoch [29/50], Step[6000/8080], Loss: 3.8329, Perplexity: 46.19\n",
      "2019-11-28 05:13:38.575267 Epoch [29/50], Step[7000/8080], Loss: 3.6338, Perplexity: 37.86\n",
      "2019-11-28 05:16:28.366904 Epoch [29/50], Step[8000/8080], Loss: 3.9169, Perplexity: 50.24\n",
      "2019-11-28 05:16:41.859657 Epoch [30/50], Step[0/8080], Loss: 3.8263, Perplexity: 45.89\n",
      "2019-11-28 05:19:33.705906 Epoch [30/50], Step[1000/8080], Loss: 3.6168, Perplexity: 37.22\n",
      "2019-11-28 05:22:23.479385 Epoch [30/50], Step[2000/8080], Loss: 3.8196, Perplexity: 45.59\n",
      "2019-11-28 05:25:14.557369 Epoch [30/50], Step[3000/8080], Loss: 3.6981, Perplexity: 40.37\n",
      "2019-11-28 05:28:04.048279 Epoch [30/50], Step[4000/8080], Loss: 3.6352, Perplexity: 37.91\n",
      "2019-11-28 05:30:52.237735 Epoch [30/50], Step[5000/8080], Loss: 4.1925, Perplexity: 66.19\n",
      "2019-11-28 05:33:41.431035 Epoch [30/50], Step[6000/8080], Loss: 3.8263, Perplexity: 45.89\n",
      "2019-11-28 05:36:30.775386 Epoch [30/50], Step[7000/8080], Loss: 3.6321, Perplexity: 37.79\n",
      "2019-11-28 05:39:18.713224 Epoch [30/50], Step[8000/8080], Loss: 3.9365, Perplexity: 51.24\n",
      "2019-11-28 05:39:32.801043 Epoch [31/50], Step[0/8080], Loss: 3.7953, Perplexity: 44.49\n",
      "2019-11-28 05:42:22.917956 Epoch [31/50], Step[1000/8080], Loss: 3.5995, Perplexity: 36.58\n",
      "2019-11-28 05:45:10.674167 Epoch [31/50], Step[2000/8080], Loss: 3.8394, Perplexity: 46.50\n",
      "2019-11-28 05:47:59.101187 Epoch [31/50], Step[3000/8080], Loss: 3.6916, Perplexity: 40.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-28 05:50:47.819285 Epoch [31/50], Step[4000/8080], Loss: 3.6332, Perplexity: 37.83\n",
      "2019-11-28 05:53:37.346235 Epoch [31/50], Step[5000/8080], Loss: 4.1803, Perplexity: 65.38\n",
      "2019-11-28 05:56:25.587306 Epoch [31/50], Step[6000/8080], Loss: 3.8362, Perplexity: 46.35\n",
      "2019-11-28 05:59:15.551742 Epoch [31/50], Step[7000/8080], Loss: 3.6280, Perplexity: 37.64\n",
      "2019-11-28 06:02:05.515473 Epoch [31/50], Step[8000/8080], Loss: 3.9082, Perplexity: 49.81\n",
      "2019-11-28 06:02:19.152231 Epoch [32/50], Step[0/8080], Loss: 3.8338, Perplexity: 46.24\n",
      "2019-11-28 06:05:05.011440 Epoch [32/50], Step[1000/8080], Loss: 3.6028, Perplexity: 36.70\n",
      "2019-11-28 06:07:55.254631 Epoch [32/50], Step[2000/8080], Loss: 3.8580, Perplexity: 47.37\n",
      "2019-11-28 06:10:46.133183 Epoch [32/50], Step[3000/8080], Loss: 3.6912, Perplexity: 40.09\n",
      "2019-11-28 06:13:35.133753 Epoch [32/50], Step[4000/8080], Loss: 3.6500, Perplexity: 38.48\n",
      "2019-11-28 06:16:26.970008 Epoch [32/50], Step[5000/8080], Loss: 4.1808, Perplexity: 65.42\n",
      "2019-11-28 06:19:16.592360 Epoch [32/50], Step[6000/8080], Loss: 3.8280, Perplexity: 45.97\n",
      "2019-11-28 06:22:06.229812 Epoch [32/50], Step[7000/8080], Loss: 3.6199, Perplexity: 37.33\n",
      "2019-11-28 06:24:56.560520 Epoch [32/50], Step[8000/8080], Loss: 3.9082, Perplexity: 49.81\n",
      "2019-11-28 06:25:10.086101 Epoch [33/50], Step[0/8080], Loss: 3.8451, Perplexity: 46.76\n",
      "2019-11-28 06:27:58.406022 Epoch [33/50], Step[1000/8080], Loss: 3.6331, Perplexity: 37.83\n",
      "2019-11-28 06:30:50.867221 Epoch [33/50], Step[2000/8080], Loss: 3.8104, Perplexity: 45.17\n",
      "2019-11-28 06:33:42.080930 Epoch [33/50], Step[3000/8080], Loss: 3.6824, Perplexity: 39.74\n",
      "2019-11-28 06:36:31.081247 Epoch [33/50], Step[4000/8080], Loss: 3.6341, Perplexity: 37.87\n",
      "2019-11-28 06:39:21.197985 Epoch [33/50], Step[5000/8080], Loss: 4.1743, Perplexity: 64.99\n",
      "2019-11-28 06:42:11.088371 Epoch [33/50], Step[6000/8080], Loss: 3.8400, Perplexity: 46.52\n",
      "2019-11-28 06:44:58.117797 Epoch [33/50], Step[7000/8080], Loss: 3.5866, Perplexity: 36.11\n",
      "2019-11-28 06:47:43.844635 Epoch [33/50], Step[8000/8080], Loss: 3.9413, Perplexity: 51.49\n",
      "2019-11-28 06:47:56.743552 Epoch [34/50], Step[0/8080], Loss: 3.8438, Perplexity: 46.70\n",
      "2019-11-28 06:50:48.287125 Epoch [34/50], Step[1000/8080], Loss: 3.5751, Perplexity: 35.70\n",
      "2019-11-28 06:53:38.068012 Epoch [34/50], Step[2000/8080], Loss: 3.8165, Perplexity: 45.45\n",
      "2019-11-28 06:56:26.999368 Epoch [34/50], Step[3000/8080], Loss: 3.6712, Perplexity: 39.30\n",
      "2019-11-28 06:59:14.178069 Epoch [34/50], Step[4000/8080], Loss: 3.6353, Perplexity: 37.91\n",
      "2019-11-28 07:02:04.721987 Epoch [34/50], Step[5000/8080], Loss: 4.1968, Perplexity: 66.47\n",
      "2019-11-28 07:04:52.216854 Epoch [34/50], Step[6000/8080], Loss: 3.8129, Perplexity: 45.28\n",
      "2019-11-28 07:07:40.092023 Epoch [34/50], Step[7000/8080], Loss: 3.5807, Perplexity: 35.90\n",
      "2019-11-28 07:10:26.457406 Epoch [34/50], Step[8000/8080], Loss: 3.9147, Perplexity: 50.13\n",
      "2019-11-28 07:10:39.738728 Epoch [35/50], Step[0/8080], Loss: 3.8525, Perplexity: 47.11\n",
      "2019-11-28 07:13:28.342112 Epoch [35/50], Step[1000/8080], Loss: 3.6112, Perplexity: 37.01\n",
      "2019-11-28 07:16:16.990760 Epoch [35/50], Step[2000/8080], Loss: 3.8680, Perplexity: 47.84\n",
      "2019-11-28 07:19:04.742478 Epoch [35/50], Step[3000/8080], Loss: 3.6837, Perplexity: 39.79\n",
      "2019-11-28 07:21:54.076093 Epoch [35/50], Step[4000/8080], Loss: 3.6111, Perplexity: 37.01\n",
      "2019-11-28 07:24:39.591532 Epoch [35/50], Step[5000/8080], Loss: 4.1847, Perplexity: 65.67\n",
      "2019-11-28 07:27:29.354283 Epoch [35/50], Step[6000/8080], Loss: 3.8454, Perplexity: 46.78\n",
      "2019-11-28 07:30:18.290820 Epoch [35/50], Step[7000/8080], Loss: 3.6028, Perplexity: 36.70\n",
      "2019-11-28 07:33:09.250259 Epoch [35/50], Step[8000/8080], Loss: 3.9028, Perplexity: 49.54\n",
      "2019-11-28 07:33:23.189884 Epoch [36/50], Step[0/8080], Loss: 3.8250, Perplexity: 45.83\n",
      "2019-11-28 07:36:14.900057 Epoch [36/50], Step[1000/8080], Loss: 3.5966, Perplexity: 36.47\n",
      "2019-11-28 07:39:02.261180 Epoch [36/50], Step[2000/8080], Loss: 3.8514, Perplexity: 47.06\n",
      "2019-11-28 07:41:55.212052 Epoch [36/50], Step[3000/8080], Loss: 3.6560, Perplexity: 38.71\n",
      "2019-11-28 07:44:44.176856 Epoch [36/50], Step[4000/8080], Loss: 3.6446, Perplexity: 38.27\n",
      "2019-11-28 07:47:32.367475 Epoch [36/50], Step[5000/8080], Loss: 4.1900, Perplexity: 66.02\n",
      "2019-11-28 07:50:20.754290 Epoch [36/50], Step[6000/8080], Loss: 3.8491, Perplexity: 46.95\n",
      "2019-11-28 07:53:13.999207 Epoch [36/50], Step[7000/8080], Loss: 3.6129, Perplexity: 37.08\n",
      "2019-11-28 07:56:03.482419 Epoch [36/50], Step[8000/8080], Loss: 3.9304, Perplexity: 50.93\n",
      "2019-11-28 07:56:17.252564 Epoch [37/50], Step[0/8080], Loss: 3.8511, Perplexity: 47.05\n",
      "2019-11-28 07:59:05.914231 Epoch [37/50], Step[1000/8080], Loss: 3.5945, Perplexity: 36.40\n",
      "2019-11-28 08:01:55.376969 Epoch [37/50], Step[2000/8080], Loss: 3.8702, Perplexity: 47.95\n",
      "2019-11-28 08:04:46.883712 Epoch [37/50], Step[3000/8080], Loss: 3.7055, Perplexity: 40.67\n",
      "2019-11-28 08:07:37.568991 Epoch [37/50], Step[4000/8080], Loss: 3.6520, Perplexity: 38.55\n",
      "2019-11-28 08:10:25.732539 Epoch [37/50], Step[5000/8080], Loss: 4.1819, Perplexity: 65.49\n",
      "2019-11-28 08:13:13.513542 Epoch [37/50], Step[6000/8080], Loss: 3.7886, Perplexity: 44.20\n",
      "2019-11-28 08:16:00.582805 Epoch [37/50], Step[7000/8080], Loss: 3.5936, Perplexity: 36.36\n",
      "2019-11-28 08:18:47.815382 Epoch [37/50], Step[8000/8080], Loss: 3.9168, Perplexity: 50.24\n",
      "2019-11-28 08:19:00.826603 Epoch [38/50], Step[0/8080], Loss: 3.8592, Perplexity: 47.43\n",
      "2019-11-28 08:21:49.533748 Epoch [38/50], Step[1000/8080], Loss: 3.6096, Perplexity: 36.95\n",
      "2019-11-28 08:24:39.458316 Epoch [38/50], Step[2000/8080], Loss: 3.8712, Perplexity: 48.00\n",
      "2019-11-28 08:27:28.353443 Epoch [38/50], Step[3000/8080], Loss: 3.6773, Perplexity: 39.54\n",
      "2019-11-28 08:30:19.168724 Epoch [38/50], Step[4000/8080], Loss: 3.6411, Perplexity: 38.13\n",
      "2019-11-28 08:33:06.312427 Epoch [38/50], Step[5000/8080], Loss: 4.1951, Perplexity: 66.36\n",
      "2019-11-28 08:35:53.451189 Epoch [38/50], Step[6000/8080], Loss: 3.7891, Perplexity: 44.22\n",
      "2019-11-28 08:38:38.224977 Epoch [38/50], Step[7000/8080], Loss: 3.6148, Perplexity: 37.14\n",
      "2019-11-28 08:41:28.495860 Epoch [38/50], Step[8000/8080], Loss: 3.9309, Perplexity: 50.95\n",
      "2019-11-28 08:41:41.935361 Epoch [39/50], Step[0/8080], Loss: 3.8310, Perplexity: 46.11\n",
      "2019-11-28 08:44:26.915055 Epoch [39/50], Step[1000/8080], Loss: 3.6448, Perplexity: 38.28\n",
      "2019-11-28 08:47:12.884948 Epoch [39/50], Step[2000/8080], Loss: 3.8405, Perplexity: 46.55\n",
      "2019-11-28 08:49:59.197788 Epoch [39/50], Step[3000/8080], Loss: 3.7191, Perplexity: 41.23\n",
      "2019-11-28 08:52:48.342434 Epoch [39/50], Step[4000/8080], Loss: 3.6455, Perplexity: 38.30\n",
      "2019-11-28 08:55:37.919996 Epoch [39/50], Step[5000/8080], Loss: 4.2061, Perplexity: 67.09\n",
      "2019-11-28 08:58:29.135137 Epoch [39/50], Step[6000/8080], Loss: 3.8207, Perplexity: 45.64\n",
      "2019-11-28 09:01:17.594329 Epoch [39/50], Step[7000/8080], Loss: 3.5877, Perplexity: 36.15\n",
      "2019-11-28 09:04:03.640354 Epoch [39/50], Step[8000/8080], Loss: 3.9302, Perplexity: 50.92\n",
      "2019-11-28 09:04:17.135584 Epoch [40/50], Step[0/8080], Loss: 3.8046, Perplexity: 44.91\n",
      "2019-11-28 09:07:06.050712 Epoch [40/50], Step[1000/8080], Loss: 3.6156, Perplexity: 37.17\n",
      "2019-11-28 09:09:53.725708 Epoch [40/50], Step[2000/8080], Loss: 3.8594, Perplexity: 47.44\n",
      "2019-11-28 09:12:42.143227 Epoch [40/50], Step[3000/8080], Loss: 3.6939, Perplexity: 40.20\n",
      "2019-11-28 09:15:32.178242 Epoch [40/50], Step[4000/8080], Loss: 3.6318, Perplexity: 37.78\n",
      "2019-11-28 09:18:18.085505 Epoch [40/50], Step[5000/8080], Loss: 4.1490, Perplexity: 63.37\n",
      "2019-11-28 09:21:06.702176 Epoch [40/50], Step[6000/8080], Loss: 3.8328, Perplexity: 46.19\n",
      "2019-11-28 09:23:54.877595 Epoch [40/50], Step[7000/8080], Loss: 3.6023, Perplexity: 36.68\n",
      "2019-11-28 09:26:44.879802 Epoch [40/50], Step[8000/8080], Loss: 3.9250, Perplexity: 50.65\n",
      "2019-11-28 09:26:58.314316 Epoch [41/50], Step[0/8080], Loss: 3.8205, Perplexity: 45.63\n",
      "2019-11-28 09:29:48.137928 Epoch [41/50], Step[1000/8080], Loss: 3.5693, Perplexity: 35.49\n",
      "2019-11-28 09:32:34.953782 Epoch [41/50], Step[2000/8080], Loss: 3.8637, Perplexity: 47.64\n",
      "2019-11-28 09:35:25.549341 Epoch [41/50], Step[3000/8080], Loss: 3.6786, Perplexity: 39.59\n",
      "2019-11-28 09:38:16.368358 Epoch [41/50], Step[4000/8080], Loss: 3.6725, Perplexity: 39.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-28 09:41:06.942738 Epoch [41/50], Step[5000/8080], Loss: 4.1936, Perplexity: 66.26\n",
      "2019-11-28 09:43:53.651293 Epoch [41/50], Step[6000/8080], Loss: 3.8024, Perplexity: 44.81\n",
      "2019-11-28 09:46:41.664878 Epoch [41/50], Step[7000/8080], Loss: 3.6004, Perplexity: 36.61\n",
      "2019-11-28 09:49:27.925591 Epoch [41/50], Step[8000/8080], Loss: 3.9094, Perplexity: 49.87\n",
      "2019-11-28 09:49:41.395565 Epoch [42/50], Step[0/8080], Loss: 3.8209, Perplexity: 45.65\n",
      "2019-11-28 09:52:30.326263 Epoch [42/50], Step[1000/8080], Loss: 3.6155, Perplexity: 37.17\n",
      "2019-11-28 09:55:19.303795 Epoch [42/50], Step[2000/8080], Loss: 3.8507, Perplexity: 47.03\n",
      "2019-11-28 09:58:05.324105 Epoch [42/50], Step[3000/8080], Loss: 3.6726, Perplexity: 39.35\n",
      "2019-11-28 10:00:54.998469 Epoch [42/50], Step[4000/8080], Loss: 3.6442, Perplexity: 38.25\n",
      "2019-11-28 10:03:45.506824 Epoch [42/50], Step[5000/8080], Loss: 4.1835, Perplexity: 65.60\n",
      "2019-11-28 10:06:31.240613 Epoch [42/50], Step[6000/8080], Loss: 3.7984, Perplexity: 44.63\n",
      "2019-11-28 10:09:22.032695 Epoch [42/50], Step[7000/8080], Loss: 3.5793, Perplexity: 35.85\n",
      "2019-11-28 10:12:11.567730 Epoch [42/50], Step[8000/8080], Loss: 3.9041, Perplexity: 49.61\n",
      "2019-11-28 10:12:25.159183 Epoch [43/50], Step[0/8080], Loss: 3.8422, Perplexity: 46.63\n",
      "2019-11-28 10:15:10.773630 Epoch [43/50], Step[1000/8080], Loss: 3.5692, Perplexity: 35.49\n",
      "2019-11-28 10:17:57.274745 Epoch [43/50], Step[2000/8080], Loss: 3.8488, Perplexity: 46.94\n",
      "2019-11-28 10:20:44.383245 Epoch [43/50], Step[3000/8080], Loss: 3.6616, Perplexity: 38.92\n",
      "2019-11-28 10:23:32.676641 Epoch [43/50], Step[4000/8080], Loss: 3.6526, Perplexity: 38.58\n",
      "2019-11-28 10:26:22.569923 Epoch [43/50], Step[5000/8080], Loss: 4.1656, Perplexity: 64.43\n",
      "2019-11-28 10:29:12.037068 Epoch [43/50], Step[6000/8080], Loss: 3.7998, Perplexity: 44.69\n",
      "2019-11-28 10:32:01.581497 Epoch [43/50], Step[7000/8080], Loss: 3.5883, Perplexity: 36.17\n",
      "2019-11-28 10:34:51.181208 Epoch [43/50], Step[8000/8080], Loss: 3.9219, Perplexity: 50.49\n",
      "2019-11-28 10:35:04.817964 Epoch [44/50], Step[0/8080], Loss: 3.7934, Perplexity: 44.41\n",
      "2019-11-28 10:37:53.129181 Epoch [44/50], Step[1000/8080], Loss: 3.5705, Perplexity: 35.53\n",
      "2019-11-28 10:40:42.351091 Epoch [44/50], Step[2000/8080], Loss: 3.8525, Perplexity: 47.11\n",
      "2019-11-28 10:43:29.295948 Epoch [44/50], Step[3000/8080], Loss: 3.6636, Perplexity: 39.00\n",
      "2019-11-28 10:46:17.112951 Epoch [44/50], Step[4000/8080], Loss: 3.6572, Perplexity: 38.75\n",
      "2019-11-28 10:49:06.908869 Epoch [44/50], Step[5000/8080], Loss: 4.1744, Perplexity: 65.00\n",
      "2019-11-28 10:51:53.549932 Epoch [44/50], Step[6000/8080], Loss: 3.8100, Perplexity: 45.15\n",
      "2019-11-28 10:54:41.993190 Epoch [44/50], Step[7000/8080], Loss: 3.5922, Perplexity: 36.32\n",
      "2019-11-28 10:57:32.603226 Epoch [44/50], Step[8000/8080], Loss: 3.9226, Perplexity: 50.53\n",
      "2019-11-28 10:57:46.132875 Epoch [45/50], Step[0/8080], Loss: 3.8290, Perplexity: 46.02\n",
      "2019-11-28 11:00:34.523807 Epoch [45/50], Step[1000/8080], Loss: 3.5734, Perplexity: 35.64\n",
      "2019-11-28 11:03:24.208142 Epoch [45/50], Step[2000/8080], Loss: 3.8881, Perplexity: 48.82\n",
      "2019-11-28 11:06:12.370235 Epoch [45/50], Step[3000/8080], Loss: 3.6586, Perplexity: 38.81\n",
      "2019-11-28 11:08:59.313429 Epoch [45/50], Step[4000/8080], Loss: 3.6565, Perplexity: 38.73\n",
      "2019-11-28 11:11:47.438165 Epoch [45/50], Step[5000/8080], Loss: 4.1842, Perplexity: 65.64\n",
      "2019-11-28 11:14:35.027272 Epoch [45/50], Step[6000/8080], Loss: 3.7933, Perplexity: 44.40\n",
      "2019-11-28 11:17:22.393234 Epoch [45/50], Step[7000/8080], Loss: 3.5628, Perplexity: 35.26\n",
      "2019-11-28 11:20:09.109078 Epoch [45/50], Step[8000/8080], Loss: 3.9004, Perplexity: 49.42\n",
      "2019-11-28 11:20:22.357473 Epoch [46/50], Step[0/8080], Loss: 3.7874, Perplexity: 44.14\n",
      "2019-11-28 11:23:11.564923 Epoch [46/50], Step[1000/8080], Loss: 3.5616, Perplexity: 35.22\n",
      "2019-11-28 11:26:01.982153 Epoch [46/50], Step[2000/8080], Loss: 3.8791, Perplexity: 48.38\n",
      "2019-11-28 11:28:50.604014 Epoch [46/50], Step[3000/8080], Loss: 3.6774, Perplexity: 39.54\n",
      "2019-11-28 11:31:35.765360 Epoch [46/50], Step[4000/8080], Loss: 3.6342, Perplexity: 37.87\n",
      "2019-11-28 11:34:26.822932 Epoch [46/50], Step[5000/8080], Loss: 4.1771, Perplexity: 65.18\n",
      "2019-11-28 11:37:16.197869 Epoch [46/50], Step[6000/8080], Loss: 3.8103, Perplexity: 45.16\n",
      "2019-11-28 11:40:05.739805 Epoch [46/50], Step[7000/8080], Loss: 3.5805, Perplexity: 35.89\n",
      "2019-11-28 11:42:52.596342 Epoch [46/50], Step[8000/8080], Loss: 3.9229, Perplexity: 50.55\n",
      "2019-11-28 11:43:06.271689 Epoch [47/50], Step[0/8080], Loss: 3.8277, Perplexity: 45.96\n",
      "2019-11-28 11:45:53.859894 Epoch [47/50], Step[1000/8080], Loss: 3.5897, Perplexity: 36.22\n",
      "2019-11-28 11:48:40.169164 Epoch [47/50], Step[2000/8080], Loss: 3.8614, Perplexity: 47.53\n",
      "2019-11-28 11:51:27.644737 Epoch [47/50], Step[3000/8080], Loss: 3.6577, Perplexity: 38.77\n",
      "2019-11-28 11:54:14.846074 Epoch [47/50], Step[4000/8080], Loss: 3.6358, Perplexity: 37.93\n",
      "2019-11-28 11:57:04.322554 Epoch [47/50], Step[5000/8080], Loss: 4.1819, Perplexity: 65.49\n",
      "2019-11-28 11:59:53.966769 Epoch [47/50], Step[6000/8080], Loss: 3.8393, Perplexity: 46.49\n",
      "2019-11-28 12:02:42.050821 Epoch [47/50], Step[7000/8080], Loss: 3.5659, Perplexity: 35.37\n",
      "2019-11-28 12:05:35.493842 Epoch [47/50], Step[8000/8080], Loss: 3.9172, Perplexity: 50.26\n",
      "2019-11-28 12:05:48.935811 Epoch [48/50], Step[0/8080], Loss: 3.8368, Perplexity: 46.37\n",
      "2019-11-28 12:08:35.513225 Epoch [48/50], Step[1000/8080], Loss: 3.6316, Perplexity: 37.78\n",
      "2019-11-28 12:11:24.870312 Epoch [48/50], Step[2000/8080], Loss: 3.8555, Perplexity: 47.25\n",
      "2019-11-28 12:14:15.968683 Epoch [48/50], Step[3000/8080], Loss: 3.6859, Perplexity: 39.88\n",
      "2019-11-28 12:17:02.965217 Epoch [48/50], Step[4000/8080], Loss: 3.6606, Perplexity: 38.88\n",
      "2019-11-28 12:19:54.171195 Epoch [48/50], Step[5000/8080], Loss: 4.1450, Perplexity: 63.12\n",
      "2019-11-28 12:22:43.417815 Epoch [48/50], Step[6000/8080], Loss: 3.7982, Perplexity: 44.62\n",
      "2019-11-28 12:25:30.821532 Epoch [48/50], Step[7000/8080], Loss: 3.5953, Perplexity: 36.43\n",
      "2019-11-28 12:28:22.615652 Epoch [48/50], Step[8000/8080], Loss: 3.9348, Perplexity: 51.15\n",
      "2019-11-28 12:28:36.527507 Epoch [49/50], Step[0/8080], Loss: 3.8050, Perplexity: 44.93\n",
      "2019-11-28 12:31:26.403002 Epoch [49/50], Step[1000/8080], Loss: 3.5994, Perplexity: 36.58\n",
      "2019-11-28 12:34:13.046283 Epoch [49/50], Step[2000/8080], Loss: 3.8822, Perplexity: 48.53\n",
      "2019-11-28 12:37:03.206959 Epoch [49/50], Step[3000/8080], Loss: 3.6758, Perplexity: 39.48\n",
      "2019-11-28 12:39:54.251285 Epoch [49/50], Step[4000/8080], Loss: 3.6414, Perplexity: 38.15\n",
      "2019-11-28 12:42:43.948128 Epoch [49/50], Step[5000/8080], Loss: 4.1421, Perplexity: 62.94\n",
      "2019-11-28 12:45:33.328683 Epoch [49/50], Step[6000/8080], Loss: 3.7965, Perplexity: 44.55\n",
      "2019-11-28 12:48:23.696600 Epoch [49/50], Step[7000/8080], Loss: 3.5642, Perplexity: 35.31\n",
      "2019-11-28 12:51:13.007311 Epoch [49/50], Step[8000/8080], Loss: 3.8836, Perplexity: 48.60\n",
      "2019-11-28 12:51:26.530856 Epoch [50/50], Step[0/8080], Loss: 3.8384, Perplexity: 46.45\n",
      "2019-11-28 12:54:13.986563 Epoch [50/50], Step[1000/8080], Loss: 3.5861, Perplexity: 36.09\n",
      "2019-11-28 12:57:02.081202 Epoch [50/50], Step[2000/8080], Loss: 3.8344, Perplexity: 46.26\n",
      "2019-11-28 12:59:49.231774 Epoch [50/50], Step[3000/8080], Loss: 3.6738, Perplexity: 39.40\n",
      "2019-11-28 13:02:37.042530 Epoch [50/50], Step[4000/8080], Loss: 3.6351, Perplexity: 37.91\n",
      "2019-11-28 13:05:26.974822 Epoch [50/50], Step[5000/8080], Loss: 4.1476, Perplexity: 63.28\n",
      "2019-11-28 13:08:11.798348 Epoch [50/50], Step[6000/8080], Loss: 3.8011, Perplexity: 44.75\n",
      "2019-11-28 13:11:02.625293 Epoch [50/50], Step[7000/8080], Loss: 3.5909, Perplexity: 36.27\n",
      "2019-11-28 13:13:53.169768 Epoch [50/50], Step[8000/8080], Loss: 3.8984, Perplexity: 49.32\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    global_step = 0 \n",
    "    glyph_warmup = 0\n",
    "    glyph_ratio = 0.1\n",
    "    glyph_decay = 0.1\n",
    "    # Set initial hidden and cell states\n",
    "    state = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "        \n",
    "        # Get images\n",
    "        images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "        for b, seq in enumerate(ids):\n",
    "            for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "                images[b, s] = np.load(f'char_img/noto_CJK/msr/{idx}.npy').reshape(char_size,char_size,1)\n",
    "        images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "        images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "        images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "        \n",
    "        # Get encoded images\n",
    "        cnn_o = cnn_encoder(images)\n",
    "        \n",
    "        # Forward pass of Classification Task\n",
    "        clf_o = clf(cnn_o)\n",
    "        loss_clf = criterion(clf_o, targets.reshape(-1))\n",
    "                      \n",
    "        # Forward pass of RNNLM\n",
    "        cnn_o = cnn_o.view(inputs.size(0), inputs.size(1), -1)\n",
    "        state = detach(state)\n",
    "        outputs, state = model(inputs, cnn_o, state)\n",
    "        loss_lm = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        if global_step < glyph_warmup:\n",
    "            loss = (1 - glyph_ratio) * loss_lm + glyph_ratio * loss_clf\n",
    "        else:\n",
    "            loss_ratio = glyph_ratio * glyph_decay ** (i + 1 + global_step // 5)\n",
    "            loss = (1 - loss_ratio) * loss_lm + loss_ratio * loss_clf\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        cnn_encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(params, 0.5)\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 1000 == 0:\n",
    "            print ('{} Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(datetime.now(), epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perplexity: 44.1288733378575\n",
      "Classification loss: 17.111882503729056\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "cnn_encoder.eval()\n",
    "clf.eval()\n",
    "total_clf_loss = .0\n",
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = ids[:, i:i+seq_length].to(device)\n",
    "    targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Get images\n",
    "    images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "    for b, seq in enumerate(ids):\n",
    "        for s, idx in enumerate(ids[b][i:i+seq_length]):\n",
    "            images[b, s] = np.load(f'char_img/noto_CJK/msr/{idx}.npy').reshape(char_size,char_size,1)\n",
    "    images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "    images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "    images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "\n",
    "    # Forward pass of Classification Task\n",
    "    cnn_o = cnn_encoder(images)\n",
    "    clf_o = clf(cnn_o)\n",
    "    loss_clf = criterion(clf_o, targets.reshape(-1))\n",
    "    \n",
    "    total_clf_loss += loss_clf.item()\n",
    "    \n",
    "    # Get encoded images\n",
    "    cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(inputs, cnn_o, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "    \n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Train Perplexity: {perplexity / num_step}\")\n",
    "print(f\"Classification loss: {total_clf_loss / num_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = corpus.get_data('icwb2-data/testing/msr_test.utf8', batch_size)\n",
    "# filter out unknown character\n",
    "test_ids = test_ids.view(-1)\n",
    "mask = test_ids < vocab_size\n",
    "test_ids = test_ids[mask]\n",
    "num_batches = test_ids.size(0) // batch_size\n",
    "test_ids = test_ids[:num_batches*batch_size]\n",
    "test_ids = test_ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 58.066234992868445\n",
      "Classification loss: 16.723388851176164\n"
     ]
    }
   ],
   "source": [
    "perplexity = .0\n",
    "num_step = 0\n",
    "total_clf_loss = .0\n",
    "for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = test_ids[:, i:i+seq_length].to(device)\n",
    "    targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Get images\n",
    "    images = np.zeros((inputs.size(0), inputs.size(1), char_size, char_size, 1))\n",
    "    for b, seq in enumerate(test_ids):\n",
    "        for s, idx in enumerate(test_ids[b][i:i+seq_length]):\n",
    "            images[b, s] = np.load(f'char_img/noto_CJK/msr/{idx}.npy').reshape(char_size,char_size,1)\n",
    "    images = torch.from_numpy(images).float().to(device) # B N H W C\n",
    "    images = images.view(-1, char_size, char_size, 1) # B*N H W C\n",
    "    images = images.permute(0, 3, 1, 2) # from B*N H W C to B*N C H W\n",
    "    \n",
    "    # Forward pass of Classification Task\n",
    "    cnn_o = cnn_encoder(images)\n",
    "    clf_o = clf(cnn_o)\n",
    "    loss_clf = criterion(clf_o, targets.reshape(-1))\n",
    "    \n",
    "    total_clf_loss += loss_clf.item()\n",
    "\n",
    "    # Get encoded images\n",
    "    cnn_o = torch.reshape(cnn_o, (inputs.size(0), inputs.size(1), -1))\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(inputs, cnn_o, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Test Perplexity: {perplexity / num_step}\")\n",
    "print(f\"Classification loss: {total_clf_loss / num_step}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch1.2)",
   "language": "python",
   "name": "pytorch1.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
