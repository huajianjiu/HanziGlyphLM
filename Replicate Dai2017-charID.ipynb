{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renderer\n",
    "from PIL import ImageFont\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "char_size = 24\n",
    "# char render\n",
    "def render(text, font=None):\n",
    "    if font is None:\n",
    "        font = ImageFont.truetype(\"/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\", char_size)\n",
    "    mask = font.getmask(text)\n",
    "    size = mask.size[::-1]\n",
    "    a = np.asarray(mask).reshape(size) / 255\n",
    "    res = cv2.resize(a, dsize=(char_size, char_size), interpolation=cv2.INTER_CUBIC)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model/data_utils.py\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, max_size=None):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 1\n",
    "        self.word2idx['⸘'] = 0 # as unk\n",
    "        self.idx2word[0] = '⸘'\n",
    "        self.max_size = max_size + 1\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx and self.idx < self.max_size:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, max_size=None):\n",
    "        self.dictionary = Dictionary(max_size=max_size)\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # Add words to the dictionary\n",
    "#         with open(path, 'r') as f:\n",
    "#             tokens = 0\n",
    "#             for line in f:\n",
    "#                 words = line.split() + ['<eos>']\n",
    "#                 tokens += len(words)\n",
    "#                 for word in words: \n",
    "#                     self.dictionary.add_word(word)  \n",
    "\n",
    "        # split words to char and add to dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                tokens += len(chars)\n",
    "                for char in chars:\n",
    "                    self.dictionary.add_word(char)\n",
    "        \n",
    "        # Tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = ' '.join(line) # split words to char\n",
    "                line = re.sub(r'[\" \"]+', ' ', line) # remove continous space\n",
    "                chars = line.split() + ['¿'] # ¿ as <eos>\n",
    "                for char in chars:\n",
    "                    if char in self.dictionary.word2idx:\n",
    "                        ids[token] = self.dictionary.word2idx[char]\n",
    "                        token += 1\n",
    "                    else:\n",
    "                        ids[token] = self.dictionary.word2idx['⸘']\n",
    "                        token += 1\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, char_id, h):\n",
    "        # Embed word ids to vectors\n",
    "        x = self.embed(char_id)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, h = self.gru(x, h)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "embed_size = 300\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "seq_length = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Load dataset\n",
    "corpus = Corpus(max_size=4000)\n",
    "ids = corpus.get_data('icwb2-data/training/msr_training.utf8', batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = ids.size(1) // seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "model.train()\n",
    "params = list(model.parameters())\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# Truncated backpropagation\n",
    "def detach(state):\n",
    "    return state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-25 16:34:42.927360 Epoch [1/50], Step[0/8080], Loss: 8.3332, Perplexity: 4159.65\n",
      "2019-11-25 16:34:46.226363 Epoch [1/50], Step[1000/8080], Loss: 5.0667, Perplexity: 158.65\n",
      "2019-11-25 16:34:49.540944 Epoch [1/50], Step[2000/8080], Loss: 4.6069, Perplexity: 100.17\n",
      "2019-11-25 16:34:52.933879 Epoch [1/50], Step[3000/8080], Loss: 4.3897, Perplexity: 80.61\n",
      "2019-11-25 16:34:56.535907 Epoch [1/50], Step[4000/8080], Loss: 4.3132, Perplexity: 74.68\n",
      "2019-11-25 16:35:00.530098 Epoch [1/50], Step[5000/8080], Loss: 4.7531, Perplexity: 115.94\n",
      "2019-11-25 16:35:04.527642 Epoch [1/50], Step[6000/8080], Loss: 4.6090, Perplexity: 100.39\n",
      "2019-11-25 16:35:08.178163 Epoch [1/50], Step[7000/8080], Loss: 4.1619, Perplexity: 64.19\n",
      "2019-11-25 16:35:11.882611 Epoch [1/50], Step[8000/8080], Loss: 4.2702, Perplexity: 71.54\n",
      "2019-11-25 16:35:12.196362 Epoch [2/50], Step[0/8080], Loss: 4.3602, Perplexity: 78.27\n",
      "2019-11-25 16:35:15.548286 Epoch [2/50], Step[1000/8080], Loss: 4.2432, Perplexity: 69.63\n",
      "2019-11-25 16:35:18.615801 Epoch [2/50], Step[2000/8080], Loss: 4.0987, Perplexity: 60.26\n",
      "2019-11-25 16:35:21.678432 Epoch [2/50], Step[3000/8080], Loss: 4.0520, Perplexity: 57.51\n",
      "2019-11-25 16:35:24.665197 Epoch [2/50], Step[4000/8080], Loss: 3.9938, Perplexity: 54.26\n",
      "2019-11-25 16:35:27.634710 Epoch [2/50], Step[5000/8080], Loss: 4.5570, Perplexity: 95.29\n",
      "2019-11-25 16:35:30.615200 Epoch [2/50], Step[6000/8080], Loss: 4.3023, Perplexity: 73.87\n",
      "2019-11-25 16:35:33.594491 Epoch [2/50], Step[7000/8080], Loss: 3.9657, Perplexity: 52.76\n",
      "2019-11-25 16:35:36.492919 Epoch [2/50], Step[8000/8080], Loss: 4.1583, Perplexity: 63.96\n",
      "2019-11-25 16:35:36.724595 Epoch [3/50], Step[0/8080], Loss: 4.2104, Perplexity: 67.38\n",
      "2019-11-25 16:35:39.703392 Epoch [3/50], Step[1000/8080], Loss: 4.0689, Perplexity: 58.49\n",
      "2019-11-25 16:35:42.685870 Epoch [3/50], Step[2000/8080], Loss: 4.0319, Perplexity: 56.37\n",
      "2019-11-25 16:35:45.703095 Epoch [3/50], Step[3000/8080], Loss: 3.9191, Perplexity: 50.36\n",
      "2019-11-25 16:35:49.021185 Epoch [3/50], Step[4000/8080], Loss: 3.8896, Perplexity: 48.89\n",
      "2019-11-25 16:35:52.633317 Epoch [3/50], Step[5000/8080], Loss: 4.4918, Perplexity: 89.28\n",
      "2019-11-25 16:35:56.310167 Epoch [3/50], Step[6000/8080], Loss: 4.1848, Perplexity: 65.68\n",
      "2019-11-25 16:35:59.857577 Epoch [3/50], Step[7000/8080], Loss: 3.9185, Perplexity: 50.32\n",
      "2019-11-25 16:36:03.428792 Epoch [3/50], Step[8000/8080], Loss: 4.0495, Perplexity: 57.37\n",
      "2019-11-25 16:36:03.756490 Epoch [4/50], Step[0/8080], Loss: 4.1473, Perplexity: 63.26\n",
      "2019-11-25 16:36:07.342254 Epoch [4/50], Step[1000/8080], Loss: 4.0208, Perplexity: 55.75\n",
      "2019-11-25 16:36:11.078175 Epoch [4/50], Step[2000/8080], Loss: 4.0118, Perplexity: 55.25\n",
      "2019-11-25 16:36:14.760289 Epoch [4/50], Step[3000/8080], Loss: 3.8844, Perplexity: 48.64\n",
      "2019-11-25 16:36:18.441618 Epoch [4/50], Step[4000/8080], Loss: 3.8261, Perplexity: 45.88\n",
      "2019-11-25 16:36:22.183069 Epoch [4/50], Step[5000/8080], Loss: 4.4686, Perplexity: 87.23\n",
      "2019-11-25 16:36:25.857012 Epoch [4/50], Step[6000/8080], Loss: 4.1125, Perplexity: 61.10\n",
      "2019-11-25 16:36:29.582397 Epoch [4/50], Step[7000/8080], Loss: 3.8623, Perplexity: 47.58\n",
      "2019-11-25 16:36:33.283326 Epoch [4/50], Step[8000/8080], Loss: 4.0080, Perplexity: 55.04\n",
      "2019-11-25 16:36:33.574386 Epoch [5/50], Step[0/8080], Loss: 4.0792, Perplexity: 59.10\n",
      "2019-11-25 16:36:37.416527 Epoch [5/50], Step[1000/8080], Loss: 3.9920, Perplexity: 54.17\n",
      "2019-11-25 16:36:41.021050 Epoch [5/50], Step[2000/8080], Loss: 3.9961, Perplexity: 54.39\n",
      "2019-11-25 16:36:44.524663 Epoch [5/50], Step[3000/8080], Loss: 3.8407, Perplexity: 46.56\n",
      "2019-11-25 16:36:48.076768 Epoch [5/50], Step[4000/8080], Loss: 3.7924, Perplexity: 44.36\n",
      "2019-11-25 16:36:51.663923 Epoch [5/50], Step[5000/8080], Loss: 4.4201, Perplexity: 83.10\n",
      "2019-11-25 16:36:55.165321 Epoch [5/50], Step[6000/8080], Loss: 4.0833, Perplexity: 59.34\n",
      "2019-11-25 16:36:58.702186 Epoch [5/50], Step[7000/8080], Loss: 3.8326, Perplexity: 46.18\n",
      "2019-11-25 16:37:02.295505 Epoch [5/50], Step[8000/8080], Loss: 3.9812, Perplexity: 53.58\n",
      "2019-11-25 16:37:02.566709 Epoch [6/50], Step[0/8080], Loss: 4.0378, Perplexity: 56.70\n",
      "2019-11-25 16:37:06.170905 Epoch [6/50], Step[1000/8080], Loss: 3.9610, Perplexity: 52.51\n",
      "2019-11-25 16:37:09.636601 Epoch [6/50], Step[2000/8080], Loss: 3.9690, Perplexity: 52.93\n",
      "2019-11-25 16:37:13.373544 Epoch [6/50], Step[3000/8080], Loss: 3.8259, Perplexity: 45.87\n",
      "2019-11-25 16:37:16.987313 Epoch [6/50], Step[4000/8080], Loss: 3.7850, Perplexity: 44.03\n",
      "2019-11-25 16:37:20.576569 Epoch [6/50], Step[5000/8080], Loss: 4.3846, Perplexity: 80.20\n",
      "2019-11-25 16:37:24.216959 Epoch [6/50], Step[6000/8080], Loss: 4.0284, Perplexity: 56.17\n",
      "2019-11-25 16:37:27.962416 Epoch [6/50], Step[7000/8080], Loss: 3.8116, Perplexity: 45.22\n",
      "2019-11-25 16:37:31.639336 Epoch [6/50], Step[8000/8080], Loss: 3.9614, Perplexity: 52.53\n",
      "2019-11-25 16:37:31.907291 Epoch [7/50], Step[0/8080], Loss: 4.0057, Perplexity: 54.91\n",
      "2019-11-25 16:37:35.362114 Epoch [7/50], Step[1000/8080], Loss: 3.9418, Perplexity: 51.51\n",
      "2019-11-25 16:37:38.967897 Epoch [7/50], Step[2000/8080], Loss: 3.9598, Perplexity: 52.45\n",
      "2019-11-25 16:37:42.491104 Epoch [7/50], Step[3000/8080], Loss: 3.7916, Perplexity: 44.33\n",
      "2019-11-25 16:37:45.974727 Epoch [7/50], Step[4000/8080], Loss: 3.7746, Perplexity: 43.58\n",
      "2019-11-25 16:37:49.467492 Epoch [7/50], Step[5000/8080], Loss: 4.3567, Perplexity: 78.00\n",
      "2019-11-25 16:37:53.015931 Epoch [7/50], Step[6000/8080], Loss: 4.0137, Perplexity: 55.35\n",
      "2019-11-25 16:37:56.647309 Epoch [7/50], Step[7000/8080], Loss: 3.7970, Perplexity: 44.57\n",
      "2019-11-25 16:38:00.287922 Epoch [7/50], Step[8000/8080], Loss: 3.9119, Perplexity: 50.00\n",
      "2019-11-25 16:38:00.574649 Epoch [8/50], Step[0/8080], Loss: 3.9736, Perplexity: 53.17\n",
      "2019-11-25 16:38:04.184267 Epoch [8/50], Step[1000/8080], Loss: 3.9032, Perplexity: 49.56\n",
      "2019-11-25 16:38:07.682900 Epoch [8/50], Step[2000/8080], Loss: 3.9531, Perplexity: 52.10\n",
      "2019-11-25 16:38:11.342392 Epoch [8/50], Step[3000/8080], Loss: 3.7666, Perplexity: 43.23\n",
      "2019-11-25 16:38:14.996495 Epoch [8/50], Step[4000/8080], Loss: 3.7631, Perplexity: 43.08\n",
      "2019-11-25 16:38:18.592592 Epoch [8/50], Step[5000/8080], Loss: 4.3536, Perplexity: 77.75\n",
      "2019-11-25 16:38:22.202215 Epoch [8/50], Step[6000/8080], Loss: 3.9892, Perplexity: 54.01\n",
      "2019-11-25 16:38:25.863420 Epoch [8/50], Step[7000/8080], Loss: 3.7644, Perplexity: 43.14\n",
      "2019-11-25 16:38:29.339090 Epoch [8/50], Step[8000/8080], Loss: 3.9220, Perplexity: 50.50\n",
      "2019-11-25 16:38:29.620672 Epoch [9/50], Step[0/8080], Loss: 3.9628, Perplexity: 52.60\n",
      "2019-11-25 16:38:33.128490 Epoch [9/50], Step[1000/8080], Loss: 3.8698, Perplexity: 47.93\n",
      "2019-11-25 16:38:36.565664 Epoch [9/50], Step[2000/8080], Loss: 3.9303, Perplexity: 50.92\n",
      "2019-11-25 16:38:40.196889 Epoch [9/50], Step[3000/8080], Loss: 3.7561, Perplexity: 42.78\n",
      "2019-11-25 16:38:43.796947 Epoch [9/50], Step[4000/8080], Loss: 3.7358, Perplexity: 41.92\n",
      "2019-11-25 16:38:47.482782 Epoch [9/50], Step[5000/8080], Loss: 4.3472, Perplexity: 77.26\n",
      "2019-11-25 16:38:50.954974 Epoch [9/50], Step[6000/8080], Loss: 3.9642, Perplexity: 52.68\n",
      "2019-11-25 16:38:54.519900 Epoch [9/50], Step[7000/8080], Loss: 3.7342, Perplexity: 41.85\n",
      "2019-11-25 16:38:58.025904 Epoch [9/50], Step[8000/8080], Loss: 3.9230, Perplexity: 50.55\n",
      "2019-11-25 16:38:58.317078 Epoch [10/50], Step[0/8080], Loss: 3.9495, Perplexity: 51.91\n",
      "2019-11-25 16:39:01.868022 Epoch [10/50], Step[1000/8080], Loss: 3.8689, Perplexity: 47.89\n",
      "2019-11-25 16:39:05.323527 Epoch [10/50], Step[2000/8080], Loss: 3.9435, Perplexity: 51.60\n",
      "2019-11-25 16:39:08.944366 Epoch [10/50], Step[3000/8080], Loss: 3.7489, Perplexity: 42.48\n",
      "2019-11-25 16:39:12.484171 Epoch [10/50], Step[4000/8080], Loss: 3.7416, Perplexity: 42.17\n",
      "2019-11-25 16:39:15.952295 Epoch [10/50], Step[5000/8080], Loss: 4.3434, Perplexity: 76.97\n",
      "2019-11-25 16:39:19.359607 Epoch [10/50], Step[6000/8080], Loss: 3.9577, Perplexity: 52.34\n",
      "2019-11-25 16:39:22.966795 Epoch [10/50], Step[7000/8080], Loss: 3.7462, Perplexity: 42.36\n",
      "2019-11-25 16:39:26.345744 Epoch [10/50], Step[8000/8080], Loss: 3.9242, Perplexity: 50.61\n",
      "2019-11-25 16:39:26.616858 Epoch [11/50], Step[0/8080], Loss: 3.9420, Perplexity: 51.52\n",
      "2019-11-25 16:39:30.116459 Epoch [11/50], Step[1000/8080], Loss: 3.8695, Perplexity: 47.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-25 16:39:33.553033 Epoch [11/50], Step[2000/8080], Loss: 3.9155, Perplexity: 50.17\n",
      "2019-11-25 16:39:37.046685 Epoch [11/50], Step[3000/8080], Loss: 3.7420, Perplexity: 42.18\n",
      "2019-11-25 16:39:40.605762 Epoch [11/50], Step[4000/8080], Loss: 3.7413, Perplexity: 42.15\n",
      "2019-11-25 16:39:44.165428 Epoch [11/50], Step[5000/8080], Loss: 4.3696, Perplexity: 79.01\n",
      "2019-11-25 16:39:47.717851 Epoch [11/50], Step[6000/8080], Loss: 3.9294, Perplexity: 50.88\n",
      "2019-11-25 16:39:51.289860 Epoch [11/50], Step[7000/8080], Loss: 3.7481, Perplexity: 42.44\n",
      "2019-11-25 16:39:54.925217 Epoch [11/50], Step[8000/8080], Loss: 3.9216, Perplexity: 50.48\n",
      "2019-11-25 16:39:55.197532 Epoch [12/50], Step[0/8080], Loss: 3.9281, Perplexity: 50.81\n",
      "2019-11-25 16:39:58.678749 Epoch [12/50], Step[1000/8080], Loss: 3.8325, Perplexity: 46.18\n",
      "2019-11-25 16:40:02.152361 Epoch [12/50], Step[2000/8080], Loss: 3.8973, Perplexity: 49.27\n",
      "2019-11-25 16:40:05.674003 Epoch [12/50], Step[3000/8080], Loss: 3.7405, Perplexity: 42.12\n",
      "2019-11-25 16:40:09.386992 Epoch [12/50], Step[4000/8080], Loss: 3.7225, Perplexity: 41.37\n",
      "2019-11-25 16:40:12.927456 Epoch [12/50], Step[5000/8080], Loss: 4.3505, Perplexity: 77.51\n",
      "2019-11-25 16:40:16.475930 Epoch [12/50], Step[6000/8080], Loss: 3.9323, Perplexity: 51.02\n",
      "2019-11-25 16:40:20.393253 Epoch [12/50], Step[7000/8080], Loss: 3.7354, Perplexity: 41.90\n",
      "2019-11-25 16:40:24.054344 Epoch [12/50], Step[8000/8080], Loss: 3.9204, Perplexity: 50.42\n",
      "2019-11-25 16:40:24.351003 Epoch [13/50], Step[0/8080], Loss: 3.9282, Perplexity: 50.82\n",
      "2019-11-25 16:40:27.906301 Epoch [13/50], Step[1000/8080], Loss: 3.8297, Perplexity: 46.05\n",
      "2019-11-25 16:40:31.602789 Epoch [13/50], Step[2000/8080], Loss: 3.9181, Perplexity: 50.31\n",
      "2019-11-25 16:40:35.194376 Epoch [13/50], Step[3000/8080], Loss: 3.7334, Perplexity: 41.82\n",
      "2019-11-25 16:40:38.781867 Epoch [13/50], Step[4000/8080], Loss: 3.7093, Perplexity: 40.83\n",
      "2019-11-25 16:40:42.420446 Epoch [13/50], Step[5000/8080], Loss: 4.3343, Perplexity: 76.27\n",
      "2019-11-25 16:40:46.153665 Epoch [13/50], Step[6000/8080], Loss: 3.9159, Perplexity: 50.20\n",
      "2019-11-25 16:40:49.659186 Epoch [13/50], Step[7000/8080], Loss: 3.7120, Perplexity: 40.94\n",
      "2019-11-25 16:40:53.101433 Epoch [13/50], Step[8000/8080], Loss: 3.8916, Perplexity: 48.99\n",
      "2019-11-25 16:40:53.399763 Epoch [14/50], Step[0/8080], Loss: 3.9135, Perplexity: 50.07\n",
      "2019-11-25 16:40:56.741175 Epoch [14/50], Step[1000/8080], Loss: 3.8057, Perplexity: 44.95\n",
      "2019-11-25 16:41:00.144638 Epoch [14/50], Step[2000/8080], Loss: 3.8906, Perplexity: 48.94\n",
      "2019-11-25 16:41:03.567772 Epoch [14/50], Step[3000/8080], Loss: 3.7355, Perplexity: 41.91\n",
      "2019-11-25 16:41:06.968369 Epoch [14/50], Step[4000/8080], Loss: 3.7156, Perplexity: 41.08\n",
      "2019-11-25 16:41:10.482739 Epoch [14/50], Step[5000/8080], Loss: 4.3340, Perplexity: 76.25\n",
      "2019-11-25 16:41:13.976497 Epoch [14/50], Step[6000/8080], Loss: 3.9228, Perplexity: 50.54\n",
      "2019-11-25 16:41:17.588073 Epoch [14/50], Step[7000/8080], Loss: 3.7151, Perplexity: 41.06\n",
      "2019-11-25 16:41:21.070256 Epoch [14/50], Step[8000/8080], Loss: 3.9060, Perplexity: 49.70\n",
      "2019-11-25 16:41:21.327388 Epoch [15/50], Step[0/8080], Loss: 3.8975, Perplexity: 49.28\n",
      "2019-11-25 16:41:24.904596 Epoch [15/50], Step[1000/8080], Loss: 3.8078, Perplexity: 45.05\n",
      "2019-11-25 16:41:28.554375 Epoch [15/50], Step[2000/8080], Loss: 3.8927, Perplexity: 49.04\n",
      "2019-11-25 16:41:32.063547 Epoch [15/50], Step[3000/8080], Loss: 3.7370, Perplexity: 41.97\n",
      "2019-11-25 16:41:35.794694 Epoch [15/50], Step[4000/8080], Loss: 3.7096, Perplexity: 40.84\n",
      "2019-11-25 16:41:39.496414 Epoch [15/50], Step[5000/8080], Loss: 4.3314, Perplexity: 76.05\n",
      "2019-11-25 16:41:43.315728 Epoch [15/50], Step[6000/8080], Loss: 3.8926, Perplexity: 49.04\n",
      "2019-11-25 16:41:47.248464 Epoch [15/50], Step[7000/8080], Loss: 3.6766, Perplexity: 39.51\n",
      "2019-11-25 16:41:51.001325 Epoch [15/50], Step[8000/8080], Loss: 3.8880, Perplexity: 48.82\n",
      "2019-11-25 16:41:51.261333 Epoch [16/50], Step[0/8080], Loss: 3.8996, Perplexity: 49.38\n",
      "2019-11-25 16:41:54.911835 Epoch [16/50], Step[1000/8080], Loss: 3.8056, Perplexity: 44.95\n",
      "2019-11-25 16:41:58.543206 Epoch [16/50], Step[2000/8080], Loss: 3.8918, Perplexity: 49.00\n",
      "2019-11-25 16:42:02.118489 Epoch [16/50], Step[3000/8080], Loss: 3.7156, Perplexity: 41.08\n",
      "2019-11-25 16:42:05.887948 Epoch [16/50], Step[4000/8080], Loss: 3.7025, Perplexity: 40.55\n",
      "2019-11-25 16:42:09.508429 Epoch [16/50], Step[5000/8080], Loss: 4.3170, Perplexity: 74.97\n",
      "2019-11-25 16:42:13.127295 Epoch [16/50], Step[6000/8080], Loss: 3.9121, Perplexity: 50.01\n",
      "2019-11-25 16:42:16.829951 Epoch [16/50], Step[7000/8080], Loss: 3.6876, Perplexity: 39.95\n",
      "2019-11-25 16:42:20.679369 Epoch [16/50], Step[8000/8080], Loss: 3.8831, Perplexity: 48.58\n",
      "2019-11-25 16:42:21.012419 Epoch [17/50], Step[0/8080], Loss: 3.8830, Perplexity: 48.57\n",
      "2019-11-25 16:42:24.657460 Epoch [17/50], Step[1000/8080], Loss: 3.7686, Perplexity: 43.32\n",
      "2019-11-25 16:42:28.354694 Epoch [17/50], Step[2000/8080], Loss: 3.8900, Perplexity: 48.91\n",
      "2019-11-25 16:42:32.115817 Epoch [17/50], Step[3000/8080], Loss: 3.6880, Perplexity: 39.96\n",
      "2019-11-25 16:42:35.770896 Epoch [17/50], Step[4000/8080], Loss: 3.7101, Perplexity: 40.86\n",
      "2019-11-25 16:42:39.387639 Epoch [17/50], Step[5000/8080], Loss: 4.3013, Perplexity: 73.80\n",
      "2019-11-25 16:42:43.014305 Epoch [17/50], Step[6000/8080], Loss: 3.8919, Perplexity: 49.00\n",
      "2019-11-25 16:42:46.710412 Epoch [17/50], Step[7000/8080], Loss: 3.6818, Perplexity: 39.72\n",
      "2019-11-25 16:42:50.298935 Epoch [17/50], Step[8000/8080], Loss: 3.8664, Perplexity: 47.77\n",
      "2019-11-25 16:42:50.578572 Epoch [18/50], Step[0/8080], Loss: 3.8819, Perplexity: 48.52\n",
      "2019-11-25 16:42:54.193955 Epoch [18/50], Step[1000/8080], Loss: 3.7566, Perplexity: 42.80\n",
      "2019-11-25 16:42:57.676840 Epoch [18/50], Step[2000/8080], Loss: 3.8885, Perplexity: 48.84\n",
      "2019-11-25 16:43:01.347662 Epoch [18/50], Step[3000/8080], Loss: 3.7078, Perplexity: 40.76\n",
      "2019-11-25 16:43:05.015692 Epoch [18/50], Step[4000/8080], Loss: 3.7006, Perplexity: 40.47\n",
      "2019-11-25 16:43:08.642421 Epoch [18/50], Step[5000/8080], Loss: 4.3143, Perplexity: 74.76\n",
      "2019-11-25 16:43:12.312984 Epoch [18/50], Step[6000/8080], Loss: 3.8885, Perplexity: 48.84\n",
      "2019-11-25 16:43:16.067579 Epoch [18/50], Step[7000/8080], Loss: 3.6801, Perplexity: 39.65\n",
      "2019-11-25 16:43:19.806138 Epoch [18/50], Step[8000/8080], Loss: 3.8833, Perplexity: 48.59\n",
      "2019-11-25 16:43:20.096391 Epoch [19/50], Step[0/8080], Loss: 3.8727, Perplexity: 48.07\n",
      "2019-11-25 16:43:23.784630 Epoch [19/50], Step[1000/8080], Loss: 3.7527, Perplexity: 42.63\n",
      "2019-11-25 16:43:27.827753 Epoch [19/50], Step[2000/8080], Loss: 3.9042, Perplexity: 49.61\n",
      "2019-11-25 16:43:31.790998 Epoch [19/50], Step[3000/8080], Loss: 3.7071, Perplexity: 40.74\n",
      "2019-11-25 16:43:35.680805 Epoch [19/50], Step[4000/8080], Loss: 3.6735, Perplexity: 39.39\n",
      "2019-11-25 16:43:39.579676 Epoch [19/50], Step[5000/8080], Loss: 4.3068, Perplexity: 74.21\n",
      "2019-11-25 16:43:43.456942 Epoch [19/50], Step[6000/8080], Loss: 3.8804, Perplexity: 48.44\n",
      "2019-11-25 16:43:47.411627 Epoch [19/50], Step[7000/8080], Loss: 3.6809, Perplexity: 39.68\n",
      "2019-11-25 16:43:51.235078 Epoch [19/50], Step[8000/8080], Loss: 3.8684, Perplexity: 47.87\n",
      "2019-11-25 16:43:51.542314 Epoch [20/50], Step[0/8080], Loss: 3.8803, Perplexity: 48.44\n",
      "2019-11-25 16:43:55.560133 Epoch [20/50], Step[1000/8080], Loss: 3.7115, Perplexity: 40.91\n",
      "2019-11-25 16:43:59.433821 Epoch [20/50], Step[2000/8080], Loss: 3.9006, Perplexity: 49.43\n",
      "2019-11-25 16:44:03.316576 Epoch [20/50], Step[3000/8080], Loss: 3.6760, Perplexity: 39.49\n",
      "2019-11-25 16:44:07.490519 Epoch [20/50], Step[4000/8080], Loss: 3.6679, Perplexity: 39.17\n",
      "2019-11-25 16:44:11.293741 Epoch [20/50], Step[5000/8080], Loss: 4.3127, Perplexity: 74.64\n",
      "2019-11-25 16:44:15.299040 Epoch [20/50], Step[6000/8080], Loss: 3.8518, Perplexity: 47.08\n",
      "2019-11-25 16:44:19.255899 Epoch [20/50], Step[7000/8080], Loss: 3.6535, Perplexity: 38.61\n",
      "2019-11-25 16:44:23.329055 Epoch [20/50], Step[8000/8080], Loss: 3.8638, Perplexity: 47.65\n",
      "2019-11-25 16:44:23.623775 Epoch [21/50], Step[0/8080], Loss: 3.8888, Perplexity: 48.85\n",
      "2019-11-25 16:44:27.281440 Epoch [21/50], Step[1000/8080], Loss: 3.7361, Perplexity: 41.93\n",
      "2019-11-25 16:44:31.121726 Epoch [21/50], Step[2000/8080], Loss: 3.9033, Perplexity: 49.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-25 16:44:35.028521 Epoch [21/50], Step[3000/8080], Loss: 3.6895, Perplexity: 40.03\n",
      "2019-11-25 16:44:38.828042 Epoch [21/50], Step[4000/8080], Loss: 3.6738, Perplexity: 39.40\n",
      "2019-11-25 16:44:42.505853 Epoch [21/50], Step[5000/8080], Loss: 4.3070, Perplexity: 74.22\n",
      "2019-11-25 16:44:46.540201 Epoch [21/50], Step[6000/8080], Loss: 3.8736, Perplexity: 48.11\n",
      "2019-11-25 16:44:50.602572 Epoch [21/50], Step[7000/8080], Loss: 3.6539, Perplexity: 38.63\n",
      "2019-11-25 16:44:54.804658 Epoch [21/50], Step[8000/8080], Loss: 3.8664, Perplexity: 47.77\n",
      "2019-11-25 16:44:55.122196 Epoch [22/50], Step[0/8080], Loss: 3.8729, Perplexity: 48.08\n",
      "2019-11-25 16:44:59.216249 Epoch [22/50], Step[1000/8080], Loss: 3.7090, Perplexity: 40.81\n",
      "2019-11-25 16:45:03.125923 Epoch [22/50], Step[2000/8080], Loss: 3.9050, Perplexity: 49.65\n",
      "2019-11-25 16:45:06.893025 Epoch [22/50], Step[3000/8080], Loss: 3.6865, Perplexity: 39.90\n",
      "2019-11-25 16:45:10.855960 Epoch [22/50], Step[4000/8080], Loss: 3.6629, Perplexity: 38.97\n",
      "2019-11-25 16:45:14.941889 Epoch [22/50], Step[5000/8080], Loss: 4.3095, Perplexity: 74.40\n",
      "2019-11-25 16:45:18.941070 Epoch [22/50], Step[6000/8080], Loss: 3.8728, Perplexity: 48.08\n",
      "2019-11-25 16:45:22.741615 Epoch [22/50], Step[7000/8080], Loss: 3.6748, Perplexity: 39.44\n",
      "2019-11-25 16:45:26.702125 Epoch [22/50], Step[8000/8080], Loss: 3.8664, Perplexity: 47.77\n",
      "2019-11-25 16:45:27.011116 Epoch [23/50], Step[0/8080], Loss: 3.8577, Perplexity: 47.35\n",
      "2019-11-25 16:45:30.983196 Epoch [23/50], Step[1000/8080], Loss: 3.7202, Perplexity: 41.27\n",
      "2019-11-25 16:45:34.823092 Epoch [23/50], Step[2000/8080], Loss: 3.8708, Perplexity: 47.98\n",
      "2019-11-25 16:45:38.753026 Epoch [23/50], Step[3000/8080], Loss: 3.6617, Perplexity: 38.93\n",
      "2019-11-25 16:45:42.636843 Epoch [23/50], Step[4000/8080], Loss: 3.6962, Perplexity: 40.30\n",
      "2019-11-25 16:45:46.711434 Epoch [23/50], Step[5000/8080], Loss: 4.2720, Perplexity: 71.66\n",
      "2019-11-25 16:45:50.397642 Epoch [23/50], Step[6000/8080], Loss: 3.8639, Perplexity: 47.65\n",
      "2019-11-25 16:45:54.280223 Epoch [23/50], Step[7000/8080], Loss: 3.6574, Perplexity: 38.76\n",
      "2019-11-25 16:45:58.075920 Epoch [23/50], Step[8000/8080], Loss: 3.8542, Perplexity: 47.19\n",
      "2019-11-25 16:45:58.406971 Epoch [24/50], Step[0/8080], Loss: 3.8814, Perplexity: 48.49\n",
      "2019-11-25 16:46:02.251547 Epoch [24/50], Step[1000/8080], Loss: 3.6996, Perplexity: 40.43\n",
      "2019-11-25 16:46:06.294367 Epoch [24/50], Step[2000/8080], Loss: 3.8875, Perplexity: 48.79\n",
      "2019-11-25 16:46:10.238266 Epoch [24/50], Step[3000/8080], Loss: 3.6618, Perplexity: 38.93\n",
      "2019-11-25 16:46:14.216077 Epoch [24/50], Step[4000/8080], Loss: 3.6782, Perplexity: 39.57\n",
      "2019-11-25 16:46:18.281856 Epoch [24/50], Step[5000/8080], Loss: 4.2710, Perplexity: 71.59\n",
      "2019-11-25 16:46:22.347356 Epoch [24/50], Step[6000/8080], Loss: 3.8828, Perplexity: 48.56\n",
      "2019-11-25 16:46:26.383255 Epoch [24/50], Step[7000/8080], Loss: 3.6590, Perplexity: 38.82\n",
      "2019-11-25 16:46:30.184613 Epoch [24/50], Step[8000/8080], Loss: 3.8834, Perplexity: 48.59\n",
      "2019-11-25 16:46:30.491779 Epoch [25/50], Step[0/8080], Loss: 3.8521, Perplexity: 47.09\n",
      "2019-11-25 16:46:34.342110 Epoch [25/50], Step[1000/8080], Loss: 3.6930, Perplexity: 40.17\n",
      "2019-11-25 16:46:38.215336 Epoch [25/50], Step[2000/8080], Loss: 3.8880, Perplexity: 48.81\n",
      "2019-11-25 16:46:42.053073 Epoch [25/50], Step[3000/8080], Loss: 3.6717, Perplexity: 39.32\n",
      "2019-11-25 16:46:46.265031 Epoch [25/50], Step[4000/8080], Loss: 3.6460, Perplexity: 38.32\n",
      "2019-11-25 16:46:50.486240 Epoch [25/50], Step[5000/8080], Loss: 4.2865, Perplexity: 72.71\n",
      "2019-11-25 16:46:54.351311 Epoch [25/50], Step[6000/8080], Loss: 3.8752, Perplexity: 48.19\n",
      "2019-11-25 16:46:58.445982 Epoch [25/50], Step[7000/8080], Loss: 3.6413, Perplexity: 38.14\n",
      "2019-11-25 16:47:02.463717 Epoch [25/50], Step[8000/8080], Loss: 3.8674, Perplexity: 47.82\n",
      "2019-11-25 16:47:02.767619 Epoch [26/50], Step[0/8080], Loss: 3.8592, Perplexity: 47.43\n",
      "2019-11-25 16:47:06.642260 Epoch [26/50], Step[1000/8080], Loss: 3.6604, Perplexity: 38.88\n",
      "2019-11-25 16:47:10.517524 Epoch [26/50], Step[2000/8080], Loss: 3.8582, Perplexity: 47.38\n",
      "2019-11-25 16:47:14.331145 Epoch [26/50], Step[3000/8080], Loss: 3.6713, Perplexity: 39.30\n",
      "2019-11-25 16:47:18.369990 Epoch [26/50], Step[4000/8080], Loss: 3.6574, Perplexity: 38.76\n",
      "2019-11-25 16:47:22.239737 Epoch [26/50], Step[5000/8080], Loss: 4.2680, Perplexity: 71.38\n",
      "2019-11-25 16:47:26.263338 Epoch [26/50], Step[6000/8080], Loss: 3.8838, Perplexity: 48.61\n",
      "2019-11-25 16:47:30.128748 Epoch [26/50], Step[7000/8080], Loss: 3.6155, Perplexity: 37.17\n",
      "2019-11-25 16:47:34.070984 Epoch [26/50], Step[8000/8080], Loss: 3.8782, Perplexity: 48.34\n",
      "2019-11-25 16:47:34.348279 Epoch [27/50], Step[0/8080], Loss: 3.8707, Perplexity: 47.97\n",
      "2019-11-25 16:47:38.338939 Epoch [27/50], Step[1000/8080], Loss: 3.6691, Perplexity: 39.22\n",
      "2019-11-25 16:47:42.214650 Epoch [27/50], Step[2000/8080], Loss: 3.8705, Perplexity: 47.97\n",
      "2019-11-25 16:47:46.123796 Epoch [27/50], Step[3000/8080], Loss: 3.6638, Perplexity: 39.01\n",
      "2019-11-25 16:47:50.313334 Epoch [27/50], Step[4000/8080], Loss: 3.6780, Perplexity: 39.57\n",
      "2019-11-25 16:47:54.423962 Epoch [27/50], Step[5000/8080], Loss: 4.2679, Perplexity: 71.37\n",
      "2019-11-25 16:47:58.267138 Epoch [27/50], Step[6000/8080], Loss: 3.8831, Perplexity: 48.57\n",
      "2019-11-25 16:48:02.315357 Epoch [27/50], Step[7000/8080], Loss: 3.6276, Perplexity: 37.62\n",
      "2019-11-25 16:48:06.119394 Epoch [27/50], Step[8000/8080], Loss: 3.8497, Perplexity: 46.98\n",
      "2019-11-25 16:48:06.451676 Epoch [28/50], Step[0/8080], Loss: 3.8712, Perplexity: 48.00\n",
      "2019-11-25 16:48:10.434865 Epoch [28/50], Step[1000/8080], Loss: 3.6635, Perplexity: 39.00\n",
      "2019-11-25 16:48:14.523669 Epoch [28/50], Step[2000/8080], Loss: 3.8906, Perplexity: 48.94\n",
      "2019-11-25 16:48:18.416695 Epoch [28/50], Step[3000/8080], Loss: 3.6683, Perplexity: 39.18\n",
      "2019-11-25 16:48:22.520212 Epoch [28/50], Step[4000/8080], Loss: 3.6502, Perplexity: 38.48\n",
      "2019-11-25 16:48:26.487473 Epoch [28/50], Step[5000/8080], Loss: 4.2808, Perplexity: 72.30\n",
      "2019-11-25 16:48:30.406723 Epoch [28/50], Step[6000/8080], Loss: 3.8699, Perplexity: 47.94\n",
      "2019-11-25 16:48:34.340671 Epoch [28/50], Step[7000/8080], Loss: 3.6243, Perplexity: 37.50\n",
      "2019-11-25 16:48:38.110374 Epoch [28/50], Step[8000/8080], Loss: 3.8891, Perplexity: 48.87\n",
      "2019-11-25 16:48:38.396304 Epoch [29/50], Step[0/8080], Loss: 3.8569, Perplexity: 47.32\n",
      "2019-11-25 16:48:42.436539 Epoch [29/50], Step[1000/8080], Loss: 3.6502, Perplexity: 38.48\n",
      "2019-11-25 16:48:46.373788 Epoch [29/50], Step[2000/8080], Loss: 3.9018, Perplexity: 49.49\n",
      "2019-11-25 16:48:50.142061 Epoch [29/50], Step[3000/8080], Loss: 3.6893, Perplexity: 40.02\n",
      "2019-11-25 16:48:54.157484 Epoch [29/50], Step[4000/8080], Loss: 3.6614, Perplexity: 38.92\n",
      "2019-11-25 16:48:57.954733 Epoch [29/50], Step[5000/8080], Loss: 4.2645, Perplexity: 71.13\n",
      "2019-11-25 16:49:01.765557 Epoch [29/50], Step[6000/8080], Loss: 3.8655, Perplexity: 47.73\n",
      "2019-11-25 16:49:05.512843 Epoch [29/50], Step[7000/8080], Loss: 3.6221, Perplexity: 37.41\n",
      "2019-11-25 16:49:09.306172 Epoch [29/50], Step[8000/8080], Loss: 3.8557, Perplexity: 47.26\n",
      "2019-11-25 16:49:09.624889 Epoch [30/50], Step[0/8080], Loss: 3.8417, Perplexity: 46.61\n",
      "2019-11-25 16:49:13.627620 Epoch [30/50], Step[1000/8080], Loss: 3.6507, Perplexity: 38.50\n",
      "2019-11-25 16:49:17.573440 Epoch [30/50], Step[2000/8080], Loss: 3.8790, Perplexity: 48.37\n",
      "2019-11-25 16:49:21.575167 Epoch [30/50], Step[3000/8080], Loss: 3.6730, Perplexity: 39.37\n",
      "2019-11-25 16:49:25.422054 Epoch [30/50], Step[4000/8080], Loss: 3.6292, Perplexity: 37.68\n",
      "2019-11-25 16:49:29.131194 Epoch [30/50], Step[5000/8080], Loss: 4.2594, Perplexity: 70.77\n",
      "2019-11-25 16:49:33.010776 Epoch [30/50], Step[6000/8080], Loss: 3.8657, Perplexity: 47.74\n",
      "2019-11-25 16:49:36.781836 Epoch [30/50], Step[7000/8080], Loss: 3.6246, Perplexity: 37.51\n",
      "2019-11-25 16:49:40.595247 Epoch [30/50], Step[8000/8080], Loss: 3.8789, Perplexity: 48.37\n",
      "2019-11-25 16:49:40.873578 Epoch [31/50], Step[0/8080], Loss: 3.8723, Perplexity: 48.05\n",
      "2019-11-25 16:49:44.905956 Epoch [31/50], Step[1000/8080], Loss: 3.6568, Perplexity: 38.74\n",
      "2019-11-25 16:49:48.759928 Epoch [31/50], Step[2000/8080], Loss: 3.8506, Perplexity: 47.02\n",
      "2019-11-25 16:49:52.827917 Epoch [31/50], Step[3000/8080], Loss: 3.6725, Perplexity: 39.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-25 16:49:56.827747 Epoch [31/50], Step[4000/8080], Loss: 3.6275, Perplexity: 37.62\n",
      "2019-11-25 16:50:00.617289 Epoch [31/50], Step[5000/8080], Loss: 4.2477, Perplexity: 69.94\n",
      "2019-11-25 16:50:04.628475 Epoch [31/50], Step[6000/8080], Loss: 3.8688, Perplexity: 47.88\n",
      "2019-11-25 16:50:08.537782 Epoch [31/50], Step[7000/8080], Loss: 3.6273, Perplexity: 37.61\n",
      "2019-11-25 16:50:12.925974 Epoch [31/50], Step[8000/8080], Loss: 3.8978, Perplexity: 49.30\n",
      "2019-11-25 16:50:13.267956 Epoch [32/50], Step[0/8080], Loss: 3.8473, Perplexity: 46.86\n",
      "2019-11-25 16:50:17.467991 Epoch [32/50], Step[1000/8080], Loss: 3.6638, Perplexity: 39.01\n",
      "2019-11-25 16:50:21.800254 Epoch [32/50], Step[2000/8080], Loss: 3.9033, Perplexity: 49.57\n",
      "2019-11-25 16:50:25.536254 Epoch [32/50], Step[3000/8080], Loss: 3.6492, Perplexity: 38.45\n",
      "2019-11-25 16:50:29.509969 Epoch [32/50], Step[4000/8080], Loss: 3.6274, Perplexity: 37.61\n",
      "2019-11-25 16:50:33.284363 Epoch [32/50], Step[5000/8080], Loss: 4.2537, Perplexity: 70.36\n",
      "2019-11-25 16:50:37.164313 Epoch [32/50], Step[6000/8080], Loss: 3.8652, Perplexity: 47.71\n",
      "2019-11-25 16:50:41.118798 Epoch [32/50], Step[7000/8080], Loss: 3.6377, Perplexity: 38.00\n",
      "2019-11-25 16:50:45.035400 Epoch [32/50], Step[8000/8080], Loss: 3.8804, Perplexity: 48.44\n",
      "2019-11-25 16:50:45.374009 Epoch [33/50], Step[0/8080], Loss: 3.8499, Perplexity: 46.99\n",
      "2019-11-25 16:50:49.326396 Epoch [33/50], Step[1000/8080], Loss: 3.6322, Perplexity: 37.80\n",
      "2019-11-25 16:50:53.209466 Epoch [33/50], Step[2000/8080], Loss: 3.8654, Perplexity: 47.72\n",
      "2019-11-25 16:50:57.185545 Epoch [33/50], Step[3000/8080], Loss: 3.6579, Perplexity: 38.78\n",
      "2019-11-25 16:51:01.153515 Epoch [33/50], Step[4000/8080], Loss: 3.6387, Perplexity: 38.04\n",
      "2019-11-25 16:51:05.300562 Epoch [33/50], Step[5000/8080], Loss: 4.2592, Perplexity: 70.76\n",
      "2019-11-25 16:51:09.252367 Epoch [33/50], Step[6000/8080], Loss: 3.8843, Perplexity: 48.63\n",
      "2019-11-25 16:51:13.305914 Epoch [33/50], Step[7000/8080], Loss: 3.6444, Perplexity: 38.26\n",
      "2019-11-25 16:51:17.443183 Epoch [33/50], Step[8000/8080], Loss: 3.8342, Perplexity: 46.26\n",
      "2019-11-25 16:51:17.745133 Epoch [34/50], Step[0/8080], Loss: 3.8692, Perplexity: 47.91\n",
      "2019-11-25 16:51:21.758772 Epoch [34/50], Step[1000/8080], Loss: 3.6368, Perplexity: 37.97\n",
      "2019-11-25 16:51:25.879589 Epoch [34/50], Step[2000/8080], Loss: 3.8779, Perplexity: 48.32\n",
      "2019-11-25 16:51:29.791151 Epoch [34/50], Step[3000/8080], Loss: 3.6562, Perplexity: 38.71\n",
      "2019-11-25 16:51:33.957986 Epoch [34/50], Step[4000/8080], Loss: 3.6418, Perplexity: 38.16\n",
      "2019-11-25 16:51:37.831940 Epoch [34/50], Step[5000/8080], Loss: 4.2377, Perplexity: 69.25\n",
      "2019-11-25 16:51:41.760102 Epoch [34/50], Step[6000/8080], Loss: 3.8721, Perplexity: 48.05\n",
      "2019-11-25 16:51:45.810954 Epoch [34/50], Step[7000/8080], Loss: 3.6367, Perplexity: 37.97\n",
      "2019-11-25 16:51:49.628467 Epoch [34/50], Step[8000/8080], Loss: 3.8476, Perplexity: 46.88\n",
      "2019-11-25 16:51:49.938800 Epoch [35/50], Step[0/8080], Loss: 3.8557, Perplexity: 47.26\n",
      "2019-11-25 16:51:53.903118 Epoch [35/50], Step[1000/8080], Loss: 3.6460, Perplexity: 38.32\n",
      "2019-11-25 16:51:57.782129 Epoch [35/50], Step[2000/8080], Loss: 3.8766, Perplexity: 48.26\n",
      "2019-11-25 16:52:01.575089 Epoch [35/50], Step[3000/8080], Loss: 3.6227, Perplexity: 37.44\n",
      "2019-11-25 16:52:05.365551 Epoch [35/50], Step[4000/8080], Loss: 3.6354, Perplexity: 37.92\n",
      "2019-11-25 16:52:09.393103 Epoch [35/50], Step[5000/8080], Loss: 4.2482, Perplexity: 69.98\n",
      "2019-11-25 16:52:13.548881 Epoch [35/50], Step[6000/8080], Loss: 3.8491, Perplexity: 46.95\n",
      "2019-11-25 16:52:17.565139 Epoch [35/50], Step[7000/8080], Loss: 3.6238, Perplexity: 37.48\n",
      "2019-11-25 16:52:21.613687 Epoch [35/50], Step[8000/8080], Loss: 3.8555, Perplexity: 47.25\n",
      "2019-11-25 16:52:21.953255 Epoch [36/50], Step[0/8080], Loss: 3.8207, Perplexity: 45.64\n",
      "2019-11-25 16:52:25.980867 Epoch [36/50], Step[1000/8080], Loss: 3.6582, Perplexity: 38.79\n",
      "2019-11-25 16:52:30.217792 Epoch [36/50], Step[2000/8080], Loss: 3.8684, Perplexity: 47.87\n",
      "2019-11-25 16:52:34.447931 Epoch [36/50], Step[3000/8080], Loss: 3.6317, Perplexity: 37.78\n",
      "2019-11-25 16:52:38.242892 Epoch [36/50], Step[4000/8080], Loss: 3.6188, Perplexity: 37.29\n",
      "2019-11-25 16:52:42.469767 Epoch [36/50], Step[5000/8080], Loss: 4.2273, Perplexity: 68.53\n",
      "2019-11-25 16:52:46.509763 Epoch [36/50], Step[6000/8080], Loss: 3.8663, Perplexity: 47.77\n",
      "2019-11-25 16:52:50.630998 Epoch [36/50], Step[7000/8080], Loss: 3.6337, Perplexity: 37.85\n",
      "2019-11-25 16:52:54.440770 Epoch [36/50], Step[8000/8080], Loss: 3.8672, Perplexity: 47.81\n",
      "2019-11-25 16:52:54.811137 Epoch [37/50], Step[0/8080], Loss: 3.8397, Perplexity: 46.51\n",
      "2019-11-25 16:52:58.786577 Epoch [37/50], Step[1000/8080], Loss: 3.6429, Perplexity: 38.20\n",
      "2019-11-25 16:53:03.007093 Epoch [37/50], Step[2000/8080], Loss: 3.8718, Perplexity: 48.03\n",
      "2019-11-25 16:53:06.975951 Epoch [37/50], Step[3000/8080], Loss: 3.6302, Perplexity: 37.72\n",
      "2019-11-25 16:53:10.890810 Epoch [37/50], Step[4000/8080], Loss: 3.6470, Perplexity: 38.36\n",
      "2019-11-25 16:53:15.001100 Epoch [37/50], Step[5000/8080], Loss: 4.2335, Perplexity: 68.96\n",
      "2019-11-25 16:53:18.970269 Epoch [37/50], Step[6000/8080], Loss: 3.8438, Perplexity: 46.70\n",
      "2019-11-25 16:53:23.036021 Epoch [37/50], Step[7000/8080], Loss: 3.6119, Perplexity: 37.04\n",
      "2019-11-25 16:53:26.839573 Epoch [37/50], Step[8000/8080], Loss: 3.8328, Perplexity: 46.19\n",
      "2019-11-25 16:53:27.130543 Epoch [38/50], Step[0/8080], Loss: 3.8164, Perplexity: 45.44\n",
      "2019-11-25 16:53:31.099177 Epoch [38/50], Step[1000/8080], Loss: 3.6602, Perplexity: 38.87\n",
      "2019-11-25 16:53:35.194682 Epoch [38/50], Step[2000/8080], Loss: 3.8498, Perplexity: 46.98\n",
      "2019-11-25 16:53:39.235180 Epoch [38/50], Step[3000/8080], Loss: 3.6575, Perplexity: 38.76\n",
      "2019-11-25 16:53:43.200589 Epoch [38/50], Step[4000/8080], Loss: 3.6344, Perplexity: 37.88\n",
      "2019-11-25 16:53:47.097421 Epoch [38/50], Step[5000/8080], Loss: 4.2368, Perplexity: 69.19\n",
      "2019-11-25 16:53:50.943251 Epoch [38/50], Step[6000/8080], Loss: 3.8591, Perplexity: 47.42\n",
      "2019-11-25 16:53:55.091913 Epoch [38/50], Step[7000/8080], Loss: 3.6101, Perplexity: 36.97\n",
      "2019-11-25 16:53:59.144109 Epoch [38/50], Step[8000/8080], Loss: 3.8367, Perplexity: 46.37\n",
      "2019-11-25 16:53:59.468232 Epoch [39/50], Step[0/8080], Loss: 3.8463, Perplexity: 46.82\n",
      "2019-11-25 16:54:03.379991 Epoch [39/50], Step[1000/8080], Loss: 3.6183, Perplexity: 37.27\n",
      "2019-11-25 16:54:07.338849 Epoch [39/50], Step[2000/8080], Loss: 3.8680, Perplexity: 47.85\n",
      "2019-11-25 16:54:11.380136 Epoch [39/50], Step[3000/8080], Loss: 3.6648, Perplexity: 39.05\n",
      "2019-11-25 16:54:15.441459 Epoch [39/50], Step[4000/8080], Loss: 3.6474, Perplexity: 38.37\n",
      "2019-11-25 16:54:19.696857 Epoch [39/50], Step[5000/8080], Loss: 4.2445, Perplexity: 69.72\n",
      "2019-11-25 16:54:23.495311 Epoch [39/50], Step[6000/8080], Loss: 3.8783, Perplexity: 48.34\n",
      "2019-11-25 16:54:27.440461 Epoch [39/50], Step[7000/8080], Loss: 3.5809, Perplexity: 35.91\n",
      "2019-11-25 16:54:31.335849 Epoch [39/50], Step[8000/8080], Loss: 3.8301, Perplexity: 46.07\n",
      "2019-11-25 16:54:31.677669 Epoch [40/50], Step[0/8080], Loss: 3.8378, Perplexity: 46.42\n",
      "2019-11-25 16:54:35.803739 Epoch [40/50], Step[1000/8080], Loss: 3.6521, Perplexity: 38.55\n",
      "2019-11-25 16:54:40.003090 Epoch [40/50], Step[2000/8080], Loss: 3.8610, Perplexity: 47.51\n",
      "2019-11-25 16:54:43.926888 Epoch [40/50], Step[3000/8080], Loss: 3.6510, Perplexity: 38.51\n",
      "2019-11-25 16:54:47.914259 Epoch [40/50], Step[4000/8080], Loss: 3.6693, Perplexity: 39.22\n",
      "2019-11-25 16:54:51.901772 Epoch [40/50], Step[5000/8080], Loss: 4.2443, Perplexity: 69.71\n",
      "2019-11-25 16:54:55.923780 Epoch [40/50], Step[6000/8080], Loss: 3.8590, Perplexity: 47.42\n",
      "2019-11-25 16:55:00.074191 Epoch [40/50], Step[7000/8080], Loss: 3.6012, Perplexity: 36.64\n",
      "2019-11-25 16:55:03.983895 Epoch [40/50], Step[8000/8080], Loss: 3.8485, Perplexity: 46.92\n",
      "2019-11-25 16:55:04.306429 Epoch [41/50], Step[0/8080], Loss: 3.8355, Perplexity: 46.31\n",
      "2019-11-25 16:55:08.404416 Epoch [41/50], Step[1000/8080], Loss: 3.6505, Perplexity: 38.49\n",
      "2019-11-25 16:55:12.270126 Epoch [41/50], Step[2000/8080], Loss: 3.8948, Perplexity: 49.15\n",
      "2019-11-25 16:55:15.999362 Epoch [41/50], Step[3000/8080], Loss: 3.6521, Perplexity: 38.55\n",
      "2019-11-25 16:55:20.075999 Epoch [41/50], Step[4000/8080], Loss: 3.6377, Perplexity: 38.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-25 16:55:24.200865 Epoch [41/50], Step[5000/8080], Loss: 4.2617, Perplexity: 70.93\n",
      "2019-11-25 16:55:28.077689 Epoch [41/50], Step[6000/8080], Loss: 3.8557, Perplexity: 47.26\n",
      "2019-11-25 16:55:31.821122 Epoch [41/50], Step[7000/8080], Loss: 3.6191, Perplexity: 37.30\n",
      "2019-11-25 16:55:35.792220 Epoch [41/50], Step[8000/8080], Loss: 3.8641, Perplexity: 47.66\n",
      "2019-11-25 16:55:36.111000 Epoch [42/50], Step[0/8080], Loss: 3.8405, Perplexity: 46.55\n",
      "2019-11-25 16:55:39.999243 Epoch [42/50], Step[1000/8080], Loss: 3.6239, Perplexity: 37.48\n",
      "2019-11-25 16:55:43.843061 Epoch [42/50], Step[2000/8080], Loss: 3.8773, Perplexity: 48.29\n",
      "2019-11-25 16:55:47.792781 Epoch [42/50], Step[3000/8080], Loss: 3.6432, Perplexity: 38.21\n",
      "2019-11-25 16:55:51.524966 Epoch [42/50], Step[4000/8080], Loss: 3.6464, Perplexity: 38.34\n",
      "2019-11-25 16:55:55.259601 Epoch [42/50], Step[5000/8080], Loss: 4.2188, Perplexity: 67.95\n",
      "2019-11-25 16:55:59.103004 Epoch [42/50], Step[6000/8080], Loss: 3.8921, Perplexity: 49.01\n",
      "2019-11-25 16:56:03.046968 Epoch [42/50], Step[7000/8080], Loss: 3.6084, Perplexity: 36.91\n",
      "2019-11-25 16:56:07.128482 Epoch [42/50], Step[8000/8080], Loss: 3.8467, Perplexity: 46.84\n",
      "2019-11-25 16:56:07.443834 Epoch [43/50], Step[0/8080], Loss: 3.8230, Perplexity: 45.74\n",
      "2019-11-25 16:56:11.504937 Epoch [43/50], Step[1000/8080], Loss: 3.6375, Perplexity: 38.00\n",
      "2019-11-25 16:56:15.479288 Epoch [43/50], Step[2000/8080], Loss: 3.8841, Perplexity: 48.62\n",
      "2019-11-25 16:56:19.360548 Epoch [43/50], Step[3000/8080], Loss: 3.6142, Perplexity: 37.12\n",
      "2019-11-25 16:56:23.172735 Epoch [43/50], Step[4000/8080], Loss: 3.6321, Perplexity: 37.79\n",
      "2019-11-25 16:56:27.283025 Epoch [43/50], Step[5000/8080], Loss: 4.2446, Perplexity: 69.73\n",
      "2019-11-25 16:56:31.395669 Epoch [43/50], Step[6000/8080], Loss: 3.8497, Perplexity: 46.98\n",
      "2019-11-25 16:56:35.514548 Epoch [43/50], Step[7000/8080], Loss: 3.5757, Perplexity: 35.72\n",
      "2019-11-25 16:56:39.426832 Epoch [43/50], Step[8000/8080], Loss: 3.8582, Perplexity: 47.38\n",
      "2019-11-25 16:56:39.750748 Epoch [44/50], Step[0/8080], Loss: 3.8196, Perplexity: 45.59\n",
      "2019-11-25 16:56:43.545533 Epoch [44/50], Step[1000/8080], Loss: 3.6544, Perplexity: 38.64\n",
      "2019-11-25 16:56:47.646560 Epoch [44/50], Step[2000/8080], Loss: 3.8686, Perplexity: 47.88\n",
      "2019-11-25 16:56:51.553396 Epoch [44/50], Step[3000/8080], Loss: 3.6314, Perplexity: 37.77\n",
      "2019-11-25 16:56:55.339392 Epoch [44/50], Step[4000/8080], Loss: 3.6596, Perplexity: 38.84\n",
      "2019-11-25 16:56:59.336292 Epoch [44/50], Step[5000/8080], Loss: 4.2395, Perplexity: 69.37\n",
      "2019-11-25 16:57:03.572398 Epoch [44/50], Step[6000/8080], Loss: 3.8441, Perplexity: 46.71\n",
      "2019-11-25 16:57:07.651231 Epoch [44/50], Step[7000/8080], Loss: 3.6032, Perplexity: 36.71\n",
      "2019-11-25 16:57:11.680883 Epoch [44/50], Step[8000/8080], Loss: 3.8332, Perplexity: 46.21\n",
      "2019-11-25 16:57:11.973850 Epoch [45/50], Step[0/8080], Loss: 3.8273, Perplexity: 45.94\n",
      "2019-11-25 16:57:15.869106 Epoch [45/50], Step[1000/8080], Loss: 3.6502, Perplexity: 38.48\n",
      "2019-11-25 16:57:19.888197 Epoch [45/50], Step[2000/8080], Loss: 3.9047, Perplexity: 49.63\n",
      "2019-11-25 16:57:23.958951 Epoch [45/50], Step[3000/8080], Loss: 3.6266, Perplexity: 37.58\n",
      "2019-11-25 16:57:27.882424 Epoch [45/50], Step[4000/8080], Loss: 3.6442, Perplexity: 38.25\n",
      "2019-11-25 16:57:31.756532 Epoch [45/50], Step[5000/8080], Loss: 4.2110, Perplexity: 67.43\n",
      "2019-11-25 16:57:35.544364 Epoch [45/50], Step[6000/8080], Loss: 3.8306, Perplexity: 46.09\n",
      "2019-11-25 16:57:39.566648 Epoch [45/50], Step[7000/8080], Loss: 3.6009, Perplexity: 36.63\n",
      "2019-11-25 16:57:43.484139 Epoch [45/50], Step[8000/8080], Loss: 3.8594, Perplexity: 47.44\n",
      "2019-11-25 16:57:43.826692 Epoch [46/50], Step[0/8080], Loss: 3.7881, Perplexity: 44.17\n",
      "2019-11-25 16:57:47.914648 Epoch [46/50], Step[1000/8080], Loss: 3.6251, Perplexity: 37.53\n",
      "2019-11-25 16:57:51.731387 Epoch [46/50], Step[2000/8080], Loss: 3.8845, Perplexity: 48.64\n",
      "2019-11-25 16:57:55.654081 Epoch [46/50], Step[3000/8080], Loss: 3.6463, Perplexity: 38.33\n",
      "2019-11-25 16:57:59.575923 Epoch [46/50], Step[4000/8080], Loss: 3.6352, Perplexity: 37.91\n",
      "2019-11-25 16:58:03.771950 Epoch [46/50], Step[5000/8080], Loss: 4.2182, Perplexity: 67.91\n",
      "2019-11-25 16:58:07.855476 Epoch [46/50], Step[6000/8080], Loss: 3.8347, Perplexity: 46.28\n",
      "2019-11-25 16:58:11.762803 Epoch [46/50], Step[7000/8080], Loss: 3.6199, Perplexity: 37.33\n",
      "2019-11-25 16:58:15.705138 Epoch [46/50], Step[8000/8080], Loss: 3.8525, Perplexity: 47.11\n",
      "2019-11-25 16:58:16.001308 Epoch [47/50], Step[0/8080], Loss: 3.8419, Perplexity: 46.61\n",
      "2019-11-25 16:58:19.944391 Epoch [47/50], Step[1000/8080], Loss: 3.6411, Perplexity: 38.13\n",
      "2019-11-25 16:58:24.073287 Epoch [47/50], Step[2000/8080], Loss: 3.8488, Perplexity: 46.93\n",
      "2019-11-25 16:58:27.940036 Epoch [47/50], Step[3000/8080], Loss: 3.6226, Perplexity: 37.43\n",
      "2019-11-25 16:58:31.859894 Epoch [47/50], Step[4000/8080], Loss: 3.6379, Perplexity: 38.01\n",
      "2019-11-25 16:58:35.714217 Epoch [47/50], Step[5000/8080], Loss: 4.2129, Perplexity: 67.55\n",
      "2019-11-25 16:58:39.635414 Epoch [47/50], Step[6000/8080], Loss: 3.8185, Perplexity: 45.53\n",
      "2019-11-25 16:58:43.507175 Epoch [47/50], Step[7000/8080], Loss: 3.5827, Perplexity: 35.97\n",
      "2019-11-25 16:58:47.493489 Epoch [47/50], Step[8000/8080], Loss: 3.8460, Perplexity: 46.81\n",
      "2019-11-25 16:58:47.802556 Epoch [48/50], Step[0/8080], Loss: 3.8332, Perplexity: 46.21\n",
      "2019-11-25 16:58:51.834399 Epoch [48/50], Step[1000/8080], Loss: 3.6138, Perplexity: 37.11\n",
      "2019-11-25 16:58:55.712997 Epoch [48/50], Step[2000/8080], Loss: 3.8847, Perplexity: 48.65\n",
      "2019-11-25 16:58:59.676320 Epoch [48/50], Step[3000/8080], Loss: 3.6598, Perplexity: 38.85\n",
      "2019-11-25 16:59:03.681991 Epoch [48/50], Step[4000/8080], Loss: 3.6128, Perplexity: 37.07\n",
      "2019-11-25 16:59:07.917021 Epoch [48/50], Step[5000/8080], Loss: 4.2137, Perplexity: 67.60\n",
      "2019-11-25 16:59:12.237102 Epoch [48/50], Step[6000/8080], Loss: 3.8249, Perplexity: 45.83\n",
      "2019-11-25 16:59:16.312219 Epoch [48/50], Step[7000/8080], Loss: 3.5989, Perplexity: 36.56\n",
      "2019-11-25 16:59:20.298151 Epoch [48/50], Step[8000/8080], Loss: 3.8299, Perplexity: 46.06\n",
      "2019-11-25 16:59:20.626273 Epoch [49/50], Step[0/8080], Loss: 3.8227, Perplexity: 45.73\n",
      "2019-11-25 16:59:24.632106 Epoch [49/50], Step[1000/8080], Loss: 3.5863, Perplexity: 36.10\n",
      "2019-11-25 16:59:28.548177 Epoch [49/50], Step[2000/8080], Loss: 3.8573, Perplexity: 47.34\n",
      "2019-11-25 16:59:32.538352 Epoch [49/50], Step[3000/8080], Loss: 3.6410, Perplexity: 38.13\n",
      "2019-11-25 16:59:36.606444 Epoch [49/50], Step[4000/8080], Loss: 3.6018, Perplexity: 36.66\n",
      "2019-11-25 16:59:40.489642 Epoch [49/50], Step[5000/8080], Loss: 4.2498, Perplexity: 70.09\n",
      "2019-11-25 16:59:44.352255 Epoch [49/50], Step[6000/8080], Loss: 3.8215, Perplexity: 45.67\n",
      "2019-11-25 16:59:48.157529 Epoch [49/50], Step[7000/8080], Loss: 3.6397, Perplexity: 38.08\n",
      "2019-11-25 16:59:52.253314 Epoch [49/50], Step[8000/8080], Loss: 3.8265, Perplexity: 45.90\n",
      "2019-11-25 16:59:52.592889 Epoch [50/50], Step[0/8080], Loss: 3.8221, Perplexity: 45.70\n",
      "2019-11-25 16:59:56.504259 Epoch [50/50], Step[1000/8080], Loss: 3.6243, Perplexity: 37.50\n",
      "2019-11-25 17:00:00.325839 Epoch [50/50], Step[2000/8080], Loss: 3.8657, Perplexity: 47.73\n",
      "2019-11-25 17:00:04.169098 Epoch [50/50], Step[3000/8080], Loss: 3.6165, Perplexity: 37.21\n",
      "2019-11-25 17:00:08.106999 Epoch [50/50], Step[4000/8080], Loss: 3.6533, Perplexity: 38.60\n",
      "2019-11-25 17:00:11.913798 Epoch [50/50], Step[5000/8080], Loss: 4.2214, Perplexity: 68.13\n",
      "2019-11-25 17:00:15.984250 Epoch [50/50], Step[6000/8080], Loss: 3.8411, Perplexity: 46.58\n",
      "2019-11-25 17:00:19.839183 Epoch [50/50], Step[7000/8080], Loss: 3.5918, Perplexity: 36.30\n",
      "2019-11-25 17:00:23.646850 Epoch [50/50], Step[8000/8080], Loss: 3.8060, Perplexity: 44.97\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Set initial hidden and cell states\n",
    "    state = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # Get mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length].to(device)\n",
    "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "               \n",
    "        # Forward pass\n",
    "        state = detach(state)\n",
    "        outputs, state = model(inputs, state)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(params, 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = (i+1) // seq_length\n",
    "        if step % 1000 == 0:\n",
    "            print ('{} Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "                   .format(datetime.now(), epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Perplexity: 43.86830965343008\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = ids[:, i:i+seq_length].to(device)\n",
    "    targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(inputs, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "    \n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Train Perplexity: {perplexity / num_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = corpus.get_data('icwb2-data/testing/msr_test.utf8', batch_size)\n",
    "# filter out unknown character\n",
    "test_ids = test_ids.view(-1)\n",
    "mask = test_ids < vocab_size\n",
    "test_ids = test_ids[mask]\n",
    "num_batches = test_ids.size(0) // batch_size\n",
    "test_ids = test_ids[:num_batches*batch_size]\n",
    "test_ids = test_ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 58.28757771350248\n"
     ]
    }
   ],
   "source": [
    "perplexity = .0\n",
    "num_step = 0\n",
    "for i in range(0, test_ids.size(1) - seq_length, seq_length):\n",
    "    # Get mini-batch inputs and targets\n",
    "    inputs = test_ids[:, i:i+seq_length].to(device)\n",
    "    targets = test_ids[:, (i+1):(i+1)+seq_length].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    state = detach(state)\n",
    "    outputs, state = model(inputs, state)\n",
    "    loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "    perplexity += np.exp(loss.item())\n",
    "\n",
    "    num_step += 1\n",
    "    \n",
    "print(f\"Test Perplexity: {perplexity / num_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch1.2)",
   "language": "python",
   "name": "pytorch1.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
